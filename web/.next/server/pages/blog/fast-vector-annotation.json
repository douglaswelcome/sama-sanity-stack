{"pageProps":{"config":{"footerNav":{"items":[{"_key":"f255606f8f25","_type":"navDropdownMenu","items":[{"_key":"76389ad94cbb","_type":"navItem","title":"Autonomous Transportation","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"training-data-for-autonomous-driving"}}}},{"_key":"5f64a8d6a69d","_type":"navItem","title":"E-Commerce","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"training-data-for-ecommerce"}}}},{"_key":"f10e54ae04d0","_type":"navItem","title":"AR/VR","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"training-data-for-ar-vr"}}}},{"_key":"fd729b522a77","_type":"navItem","title":"Data Quality","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"data-quality"}}}}],"title":"Guides","url":null},{"_key":"681ef7d8763a","_type":"navDropdownMenu","items":[{"_key":"6238a422b667","_type":"navItem","title":"Our Story","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"our-story"}}}},{"_key":"258985d6d46b","_type":"navItem","title":"Our Team","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"our-team"}}}},{"_key":"e0a76077324a","_type":"navItem","title":"Our Mission","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"mission-vision-values"}}}},{"_key":"239e49661b0d","_type":"navItem","title":"Careers","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"careers"}}}},{"_key":"e005a740cd80","_type":"navItem","title":"Contact","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"company-contact"}}}}],"title":"Company","url":null}]},"logo":{"asset":{"_createdAt":"2021-12-09T21:42:35Z","_id":"image-4f4e2f86a8fad952c02dffffd7008aa35f83c850-2380x636-svg","_rev":"7Z7VDk3xHzg51hvomGzc99","_type":"sanity.imageAsset","_updatedAt":"2021-12-09T21:42:35Z","assetId":"4f4e2f86a8fad952c02dffffd7008aa35f83c850","extension":"svg","metadata":{"_type":"sanity.imageMetadata","blurHash":"D009jvfQfQfQfQfQfQfQfQfQ","dimensions":{"_type":"sanity.imageDimensions","aspectRatio":3.742138364779874,"height":636,"width":2380},"hasAlpha":true,"isOpaque":false,"lqip":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAFCAYAAABFA8wzAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAr0lEQVQYlU3QyUoDQBAE0HeIEjeixAVyESRI3BNFJSZxAU/+/wdJQSEehu6Zqa6uKnjBGz7xgTuc4gAj7LWe4LD9cc8Y+9jBUed84QGrEt/gEleYY9Ylr3jGU/tV77fFLirId0nWBYYgoACi+r3D6YPN0vwFm4VxmNlgfkK4qcLUZdVMcdHB+75FzWPfzzCpxXNcFxuOP2uxnhNLASaX5LjbnJJf6jYG2PpXh812/AvSEQ+GGZqgYgAAAABJRU5ErkJggg==","palette":{"_type":"sanity.imagePalette","darkMuted":{"_type":"sanity.imagePaletteSwatch","background":"#040404","foreground":"#fff","population":100.29,"title":"#fff"},"darkVibrant":{"_type":"sanity.imagePaletteSwatch","background":"#424242","foreground":"#fff","population":0,"title":"#fff"},"dominant":{"_type":"sanity.imagePaletteSwatch","background":"#040404","foreground":"#fff","population":100.29,"title":"#fff"},"lightMuted":{"_type":"sanity.imagePaletteSwatch","background":"#4c4c4c","foreground":"#fff","population":0,"title":"#fff"},"lightVibrant":{"_type":"sanity.imagePaletteSwatch","background":"#bcbcbc","foreground":"#000","population":0,"title":"#fff"},"muted":{"_type":"sanity.imagePaletteSwatch","background":"#4c4c4c","foreground":"#fff","population":0,"title":"#fff"},"vibrant":{"_type":"sanity.imagePaletteSwatch","background":"#7f7f7f","foreground":"#fff","population":0,"title":"#fff"}}},"mimeType":"image/svg+xml","originalFilename":"e20f8cc53e5f74df10ae9a822edb7ec2c4d00f02-2380x636.svg","path":"images/76e3r62u/production/4f4e2f86a8fad952c02dffffd7008aa35f83c850-2380x636.svg","sha1hash":"4f4e2f86a8fad952c02dffffd7008aa35f83c850","size":2009,"uploadId":"jTUF9DIFqAwpLJ0GcI9bRqb17D69QQlN","url":"https://cdn.sanity.io/images/76e3r62u/production/4f4e2f86a8fad952c02dffffd7008aa35f83c850-2380x636.svg"}},"mainNav":{"items":[{"_key":"58c18e9aa9ea","_type":"navDropdownMenu","items":[{"_key":"b5b5b8bee78b","_type":"navCat","items":[{"_key":"0e80156a2f1a","_type":"navItem","title":"How it Works","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"how-it-works"}}}},{"_key":"40bacee029b4","_type":"navItem","title":"Video Annotation","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"video-annotation"}}}},{"_key":"32650ef07503","_type":"navItem","title":"Image Annotation","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"image-annotation"}}}},{"_key":"fe9137cd0167","_type":"navItem","title":"3D & LiDAR Annotation","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"3d-lidar"}}}},{"_key":"d9a1316d400a","_type":"navItem","title":"Natural Language Processing","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"natural-language-processing"}}}},{"_key":"ac12c7c5d70a","_type":"navItem","title":"Data Curation (Beta)","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"data-curation"}}}}],"title":"Platform","url":null},{"_key":"37ff4fa913bd","_type":"navCat","items":[{"_key":"6026b1a9314e","_type":"navItem","title":"Semantic Segmentation","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"semantic-segmentation"}}}},{"_key":"f4611b19b406","_type":"navItem","title":"Polygons","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"polygons"}}}},{"_key":"5155d874d6c8","_type":"navItem","title":"Bounding Boxes","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"bounding-boxes"}}}},{"_key":"9ef3c1e21e74","_type":"navItem","title":"Key Points","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"key-points"}}}},{"_key":"314d4c00d351","_type":"navItem","title":"Cuboids","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"cuboids"}}}},{"_key":"8e17a6388d74","_type":"navItem","title":"Lines & Arrows","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"lines-and-arrows"}}}}],"title":"Shapes","url":null}],"title":"Platform","url":null},{"_key":"112867ca4d03","_type":"navDropdownMenu","items":[{"_key":"22699c7e06cb","_type":"navItem","items":null,"title":"Transportation & Navigation","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"transportation-navigation"}}}},{"_key":"122ae5928d6d","_type":"navItem","items":null,"title":"Retail & E-Commerce","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"retail-ecommerce"}}}},{"_key":"7bb234b69fb0","_type":"navItem","items":null,"title":"Consumer & Media","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"consumer-media"}}}},{"_key":"33e6a886b39d","_type":"navItem","items":null,"title":"Biotech & Medtech","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"biotech-medtech"}}}},{"_key":"d095b2619c4e","_type":"navItem","items":null,"title":"Robotics & Manufacturing","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"robotics-and-manufacturing"}}}},{"_key":"2c4b82a94d79","_type":"navItem","items":null,"title":"Food & Agriculture","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"training-data-food-agriculture"}}}}],"title":"Industries","url":null},{"_key":"c47e8763a906","_type":"navDropdownMenu","items":[{"_key":"1d563df30b3f","_type":"navItem","items":null,"title":"Quality","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"quality-training-data"}}}},{"_key":"041725f35d96","_type":"navItem","items":null,"title":"Security","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"security-and-trust"}}}},{"_key":"fd64ede25798","_type":"navItem","items":null,"title":"Ethical AI","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"our-impact"}}}},{"_key":"398dcbb1c95d","_type":"navItem","items":null,"title":"Compare","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"compare"}}}},{"_key":"93bdfdd87879","_type":"navItem","items":null,"title":"Partners","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"partners"}}}}],"title":"Why Sama","url":null},{"_key":"1d38bf63df54","_type":"navDropdownMenu","items":[{"_key":"be81659b38a5","_type":"navItem","items":null,"title":"API Documentation","url":{"_type":"link","externalUrl":"https://docs.sama.com/reference/overview","internalLink":null}},{"_key":"2cec80e94962","_type":"navItem","items":null,"title":"Blog","url":{"_type":"link","internalLink":null,"internalLink_custom":"/blog"}},{"_key":"09e284fcb1d3","_type":"navItem","items":null,"title":"Events","url":{"_type":"link","internalLink":null,"internalLink_custom":"/events"}}],"title":"Resources","url":null},{"_key":"dbee93713c19","_type":"navDropdownMenu","items":[{"_key":"12d594a568bf","_type":"navItem","items":null,"title":"Our Story","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"our-story"}}}},{"_key":"ce36540a102d","_type":"navItem","items":null,"title":"Our Team","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"our-team"}}}},{"_key":"34fc328e8022","_type":"navItem","items":null,"title":"Careers","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"careers"}}}},{"_key":"c1fe2961020a","_type":"navItem","items":null,"title":"Contact","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"company-contact"}}}},{"_key":"ebd81873e538","_type":"navItem","items":null,"title":"Press","url":{"_type":"link","internalLink":null,"internalLink_custom":"/press"}}],"title":"Company","url":null}],"nav_cta":{"_type":"button","link":{"_type":"link","internalLink":{"_ref":"136788cb-06a6-4f27-b75b-07faf403bfa6","_type":"reference"}},"title":"Request a Demo","type":"secondary"}}},"data":{"post":{"_createdAt":"2020-12-15T21:43:21Z","author":{"_id":"a3099d34-9595-4978-b517-e508196414c1","avatar":{"_type":"image","asset":{"_ref":"image-6643136d6c33f77b8e49366c166642ca5dafba8d-500x500-webp","_type":"reference"}},"bio":"Frédéric is a researcher and team leader with over 15 years of R&D experience in machine learning, AI, NLP, speech recognition, and computer vision. Currently Head of AI at Sama, he has worked on building ML-based products in multiple industries from Healthcare to Retail, in large companies and startups. He cares about the impact of technology, and outside of work, you can often see him on a bicycle or on skis.","name":"Frederic Ratle","slug":{"_type":"slug","current":"frederic-ratle"}},"body":[{"_key":"eebc5c4b61c8","_type":"block","children":[{"_key":"63071f4b9b3b","_type":"span","marks":[],"text":"At Sama, Vector Annotation of objects using polygons is a task that our expert annotators spend a great deal of time on. This is especially true for projects involving autonomous vehicles, where it is typical to apply instance segmentation to label scenes comprising hundreds of frames, each with multiple objects (vehicles, pedestrians, traffic signs, etc.) like you see in Figure 1."}],"markDefs":[],"style":"normal"},{"_key":"e4803d20524c","_type":"image","asset":{"_ref":"image-3a4b80bf2e9cb5cebab901de01ab7f5b4fb8c5c9-1475x758-png","_type":"reference"}},{"_key":"b354b6fba58c","_type":"block","children":[{"_key":"42ffa496c529","_type":"span","marks":["strong"],"text":"Figure 1"},{"_key":"40b0f9d11a28","_type":"span","marks":[],"text":". Example of Polygonal Annotation in Sama."}],"markDefs":[],"style":"h6"},{"_key":"cbe433c0d8ad","_type":"block","children":[{"_key":"419d4d3c71bb0","_type":"span","marks":[],"text":""}],"markDefs":[],"style":"normal"},{"_key":"31a034e0f206","_type":"block","children":[{"_key":"7497db35e5e10","_type":"span","marks":[],"text":"In this post, we summarize an approach that we have developed to speed up polygonal instance segmentation using machine learning. This approach was presented earlier this year at the "},{"_key":"68bf56458f0e","_type":"span","marks":["594f0769bd80"],"text":"CVPR Workshop on Scalability in Autonomous Driving"},{"_key":"0a349de379b0","_type":"span","marks":[],"text":", and the "},{"_key":"cf700d61e8da","_type":"span","marks":["a94fd912ecfb"],"text":"ICML Workshop on Human-in-the-Loop Learning"},{"_key":"7497db35e5e11","_type":"span","marks":["em"],"text":"."}],"markDefs":[{"_key":"594f0769bd80","_type":"button_link","externalUrl":"http://cvpr2020.wad.vision/"},{"_key":"a94fd912ecfb","_type":"button_link","externalUrl":"https://icml.cc/"}],"style":"normal"},{"_key":"179a41949f08","_type":"block","children":[{"_key":"00b877c28ca30","_type":"span","marks":[],"text":"Few-Click Annotation"}],"markDefs":[],"style":"h4"},{"_key":"cecc97391bc6","_type":"block","children":[{"_key":"e4a08f9c20d80","_type":"span","marks":[],"text":"Building instance segmentation Deep Learning (DL) models for autonomous vehicles requires a significant amount of labeled data. The use of Machine Learning (ML) for producing pre-annotations to be reviewed by human annotators, whether in an interactive setting or as a pre-processing procedure, is a very popular approach for scaling up labeling while controlling the costs."}],"markDefs":[],"style":"normal"},{"_key":"4b3a24ca8cba","_type":"block","children":[{"_key":"539f877a007d0","_type":"span","marks":[],"text":"Multiple approaches have been suggested for machine-assisted instance segmentation. These typically consist of a DL-based segmentation of the object(s) integrated into a human-in-the-loop system. The human can interact with the system by correcting the model output, initializing the model with one or several clicks, or a combination of those steps. Examples of such systems include Polygon-RNN++ [1], DELSE [7], DEXTR [3], and CurveGCN [2]. Those systems all present good results, but some open questions remain:"}],"markDefs":[],"style":"normal"},{"_key":"c92f55769104","_type":"block","children":[{"_key":"5f69d213a5bd0","_type":"span","marks":[],"text":"Do these methods perform well when a production-level accuracy is required, as when working for a customer project?"}],"level":1,"listItem":"bullet","markDefs":[],"style":"normal"},{"_key":"36decf65e948","_type":"block","children":[{"_key":"ff0f0393404c0","_type":"span","marks":[],"text":"Does the choice of annotation tool influence the results? The gains to be made by using ML depend on how difficult it is for humans to draw polygons in the provided UI. Here we used our optimized drawing tool for polygons, which is part of our labeling platform."}],"level":1,"listItem":"bullet","markDefs":[],"style":"normal"},{"_key":"16106c03b772","_type":"block","children":[{"_key":"e181c4c8f4bc0","_type":"span","marks":[],"text":"ML integration is not usually approached from a human-centric perspective. Beyond the optimization of traditional metrics like IoU, what interactions are most desirable and how should we present the output of the model to annotators?"}],"level":1,"listItem":"bullet","markDefs":[],"style":"normal"},{"_key":"d64fd39a61f4","_type":"block","children":[{"_key":"01ee6afc1a0d0","_type":"span","marks":[],"text":"Our method relies on combining the well-known DEXTR [3] approach with a raster-to-polygon algorithm, to make the result more easily editable. This is not unlike what other tools (such as CVAT) have implemented, though we have optimized this approach for our specific use cases using A/B testing."}],"markDefs":[],"style":"normal"},{"_key":"3ac01570dffd","_type":"block","children":[{"_key":"59eaa44a4c260","_type":"span","marks":[],"text":"The Model"}],"markDefs":[],"style":"h4"},{"_key":"54d3bffc72f6","_type":"block","children":[{"_key":"1462d2b372cd0","_type":"span","marks":[],"text":"Our instance segmentation model is based on the well-known Deep Extreme Cut (DEXTR) approach [3], along with a raster-to-polygon conversion algorithm that yields high quality polygons whose vertices are sampled in a way that reproduces human drawing patterns. The model uses the few clicks provided by human annotators at inference time. The steps are described in Figure 2."}],"markDefs":[],"style":"normal"},{"_key":"f1a9600d373f","_type":"image","asset":{"_ref":"image-2385ed2f2128573a154324bb5fbbded2e611e9fa-1424x638-png","_type":"reference"}},{"_key":"e911991f8b70","_type":"block","children":[{"_key":"4bf3633cbab3","_type":"span","marks":["strong"],"text":"Figure 2"},{"_key":"17324ee29aa9","_type":"span","marks":[],"text":". An overview of the approach."}],"markDefs":[],"style":"h6"},{"_key":"ae73a6896af9","_type":"block","children":[{"_key":"cb111cf90433","_type":"span","marks":[],"text":""}],"markDefs":[],"style":"normal"},{"_key":"8c802d2aaab5","_type":"block","children":[{"_key":"5f359a2eba5a0","_type":"span","marks":[],"text":"Regarding the model itself, we adopted a custom version of the UNet [5] along with an EfficientNet backbone [6] (instead of the ResNet backbone used in the paper)."}],"markDefs":[],"style":"normal"},{"_key":"19f0856e9d21","_type":"block","children":[{"_key":"2a36d4c4c0180","_type":"span","marks":[],"text":"In our experience, for human annotators to produce good instance segmentation masks efficiently, a polygon annotation tool should be used. As such, we needed to convert the raster masks produced by our model to high quality polygons. To add to the challenge, humans tend to produce sparse polygons, adding vertices only when necessary. We therefore adopted a raster-to-polygon procedure that minimizes the number of output vertices."}],"markDefs":[],"style":"normal"},{"_key":"3303f1c4fc82","_type":"block","children":[{"_key":"8838a493180a0","_type":"span","marks":[],"text":"A/B Testing the Approach"}],"markDefs":[],"style":"h4"},{"_key":"44a37c3a6108","_type":"block","children":[{"_key":"0da12b52688d0","_type":"span","marks":[],"text":"At Sama, we use A/B testing as much as possible to systematically refine and improve our new features. To this end, we have developed a flexible testing infrastructure that can ingest and aggregate data from multiple internal processes and is made available to anyone within the organization."}],"markDefs":[],"style":"normal"},{"_key":"17d662e807d5","_type":"block","children":[{"_key":"69b1f49269f20","_type":"span","marks":[],"text":"This framework measures the statistical impact of proposed changes on our efficiency metrics (such as drawing time or shape adjustment time). The significance of observed differences on a given efficiency metric is evaluated using statistical tests."}],"markDefs":[],"style":"normal"},{"_key":"1180ca13ba2f","_type":"block","children":[{"_key":"ea083eb414500","_type":"span","marks":[],"text":"Toy A/B Tests"}],"markDefs":[],"style":"h5"},{"_key":"23882d62cd08","_type":"block","children":[{"_key":"0acaf35e0ba60","_type":"span","marks":[],"text":"We conducted an A/B test of the method using a synthetic automotive dataset called SYNTHIA-AL [8]. The dataset's images and corresponding annotations were generated from video streams at 25 frames per second (FPS). Figure 3 shows SYNTHIA image examples, along with their segmentation (done manually and with the Few-Click tool)."}],"markDefs":[],"style":"normal"},{"_key":"f1a4316ae458","_type":"image","asset":{"_ref":"image-ad6b4a09778cefac7979292c68c15ba0dcfa5ddd-512x231-png","_type":"reference"}},{"_key":"808d376bf982","_type":"block","children":[{"_key":"3a1413f967fe","_type":"span","marks":["strong"],"text":"Figure 3"},{"_key":"8601872f2fc4","_type":"span","marks":[],"text":". SYNTHIA example images, along with their manual and semi-automated annotations."}],"markDefs":[],"style":"h6"},{"_key":"526d134806e9","_type":"block","children":[{"_key":"ca49d87a7f95","_type":"span","marks":[],"text":""}],"markDefs":[],"style":"normal"},{"_key":"bae3a97f7e6f","_type":"block","children":[{"_key":"d9ee99bb3525","_type":"span","marks":[],"text":"The test, applied only to motor vehicles, reproduced realistic annotation guidelines, namely:"}],"markDefs":[],"style":"normal"},{"_key":"ecf8e8ea82b1","_type":"block","children":[{"_key":"49e2d5ac21660","_type":"span","marks":[],"text":"The drawn polygon needs to be within 2 pixels of the edge of the vehicle."}],"level":1,"listItem":"bullet","markDefs":[],"style":"normal"},{"_key":"dab9087621d0","_type":"block","children":[{"_key":"ba3c3b0ac1c40","_type":"span","marks":[],"text":"All vehicles down to 10 pixels (height or width) need to be annotated."}],"level":1,"listItem":"bullet","markDefs":[],"style":"normal"},{"_key":"f9b86647ddeb","_type":"block","children":[{"_key":"5cbfbf2c7af60","_type":"span","marks":[],"text":"Following this test, we found a nearly 3-fold reduction in annotation time. On the other hand, we also found that on some of the more complex shapes, annotators were spending quite some time manually adjusting the ML output. DEXTR's authors originally showed that the segmentation can be improved with additional clicks beyond the four initial ones. We therefore extended our few-click tool to allow online refinement of the polygons by considering modifications to their vertices as extra clicks. At train time we simulated the corrective clicks by considering the point of greatest deviation between predicted mask and ground truth as illustrated in Figure 4."}],"markDefs":[],"style":"normal"},{"_key":"71c13a4d8416","_type":"block","children":[{"_key":"057816f9b3890","_type":"span","marks":["strong"],"text":"Problem"},{"_key":"057816f9b3891","_type":"span","marks":[],"text":": DEXTR’s 4 extreme clicks are not always sufficient."}],"level":1,"listItem":"bullet","markDefs":[],"style":"normal"},{"_key":"68de997cd6fe","_type":"block","children":[{"_key":"9d1bcff9fd4c0","_type":"span","marks":["strong"],"text":"Observation"},{"_key":"9d1bcff9fd4c1","_type":"span","marks":[],"text":": DEXTR trained on 4 clicks benefits from additional clicks."}],"level":1,"listItem":"bullet","markDefs":[],"style":"normal"},{"_key":"1b1455f60e96","_type":"block","children":[{"_key":"cc55125cc4340","_type":"span","marks":["strong"],"text":"Solution"},{"_key":"cc55125cc4341","_type":"span","marks":[],"text":": Fine-tune DEXTR model with additional clicks for hard samples as established by IoU at train time."}],"level":1,"listItem":"bullet","markDefs":[],"style":"normal"},{"_key":"8e6a368385b1","_type":"image","asset":{"_ref":"image-9761c7350f514786633b12b6f17058b6b67d6677-627x427-png","_type":"reference"}},{"_key":"0802009112d6","_type":"block","children":[{"_key":"7b5b2d61f4d8","_type":"span","marks":["strong"],"text":"Figure 4"},{"_key":"2ed23d83d06b","_type":"span","marks":[],"text":". Integrating additional clicks in the training process.\n"}],"markDefs":[],"style":"h6"},{"_key":"361f9f2ea9ff","_type":"block","children":[{"_key":"35e7aac5405c0","_type":"span","marks":[],"text":"\nUsing this method, annotators are able to re-trigger the model inference with an additional click, instead of manually adjusting the output. We proceeded to a second toy A/B test, and results showed that we could obtain a theoretical efficiency gain of up to 3.5x on vehicles using the improved method.\n\n"},{"_key":"8aaf238574a9","_type":"span","marks":["4c5ab5c85862"],"text":"Download our paper on Human-Centric Efficiency Improvements in Image Annotation for Autonomous Driving here"},{"_key":"45b4ebd5c155","_type":"span","marks":[],"text":" and stay tuned to hear more about our latest advances!"}],"markDefs":[{"_key":"4c5ab5c85862","_type":"button_link","externalUrl":"https://www.sama.com/hubfs/Research%20Papers/Human-Centric%20Efficiency%20Improvements%20in%20Image%20Annotation%20for%20Autonomous%20Driving.pdf"}],"style":"normal"},{"_key":"5781b73520e3","_type":"block","children":[{"_key":"81c33d4ca3f7","_type":"span","marks":[],"text":""}],"markDefs":[],"style":"normal"},{"_key":"b94fb50cbcb5","_type":"block","children":[{"_key":"ef9bee866628","_type":"span","marks":["strong"],"text":"References"}],"markDefs":[],"style":"normal"},{"_key":"e03bf7bdcf2c","_type":"block","children":[{"_key":"8554d7938d6a0","_type":"span","marks":[],"text":"Acuna, D., Ling, H., Kar, A., and Fidler, S. Efficient annotation of segmentation datasets with polygon-rnn++. In CVPR, 2018."}],"level":1,"listItem":"number","markDefs":[],"style":"normal"},{"_key":"68d19051c1d7","_type":"block","children":[{"_key":"8114c10f973d0","_type":"span","marks":[],"text":"Ling, H., Gao, J., Kar, A., Chen, W., and Fidler, S. Fast interactive object annotation with curve-gcn. In CVPR, 2019."}],"level":1,"listItem":"number","markDefs":[],"style":"normal"},{"_key":"d319ee43a89e","_type":"block","children":[{"_key":"52a5f1d1ca4b0","_type":"span","marks":[],"text":"Maninis, K.-K., Caelles, S., Pont-Tuset, J., and Van Gool, L. Deep extreme cut: From extreme points to object segmentation. In Computer Vision and Pattern Recognition (CVPR), 2018."}],"level":1,"listItem":"number","markDefs":[],"style":"normal"},{"_key":"b75b2ef61c86","_type":"block","children":[{"_key":"2654c54524690","_type":"span","marks":[],"text":"Papadopoulos, D., Uijlings, J., Keller, F., and Ferrari, V. Extreme clicking for efficient object annotation. In ICCV, 2017."}],"level":1,"listItem":"number","markDefs":[],"style":"normal"},{"_key":"7e687f6aaa7a","_type":"block","children":[{"_key":"6d23551b69520","_type":"span","marks":[],"text":"Ronneberger, O., Fischer, P., and Brox, T. U-net: Convolutional networks for biomedical image segmentation. CoRR, abs/1505.04597, 2015. URL http://arxiv. org/abs/1505.04597."}],"level":1,"listItem":"number","markDefs":[],"style":"normal"},{"_key":"8b32ad5b452b","_type":"block","children":[{"_key":"52f98de763ee0","_type":"span","marks":[],"text":"Tan, M. and Le, Q. V. Efficientnet: Rethinking model scaling for convolutional neural networks. CoRR, abs/1905.11946, 2019. URL http://arxiv.org/ abs/1905.11946."}],"level":1,"listItem":"number","markDefs":[],"style":"normal"},{"_key":"fc2fc1479556","_type":"block","children":[{"_key":"165a0bce0b9f0","_type":"span","marks":[],"text":"Wang, Z., Acuna, D., Ling, H., Kar, A., and Fidler, S. Object instance annotation with deep extreme level set evolution. In CVPR, 2019."}],"level":1,"listItem":"number","markDefs":[],"style":"normal"},{"_key":"818e48f068d7","_type":"block","children":[{"_key":"4b5e7645a5b40","_type":"span","marks":[],"text":"Zolfaghari Bengar, J., Gonzalez-Garcia, A., Villalonga, G., Raducanu, B., Aghdam, H. H., Mozerov, M., Lopez, A. M., and van de Weijer, J. Temporal coherence for active learning in videos. arXiv preprint arXiv:1908.11757, 2019."}],"level":1,"listItem":"number","markDefs":[],"style":"normal"},{"_key":"1af2aede9358","_type":"block","children":[{"_key":"94aa7f1782210","_type":"span","marks":[],"text":"This post was written by Frederic Ratle and Martine Bertrand."}],"markDefs":[],"style":"normal"}],"estimatedReadingTime":8,"featured_image":{"_type":"image","asset":{"_ref":"image-6abd2f846119ba50a8325787f99ec3aeec68ffff-1076x605-png","_type":"reference"}},"meta_description":"In this article we summarize an approach that we have developed to speed up polygonal instance segmentation using machine learning.","openGraphImage":null,"plaintextBody":"At Sama, Vector Annotation of objects using polygons is a task that our expert annotators spend a great deal of time on. This is especially true for projects involving autonomous vehicles, where it is typical to apply instance segmentation to label scenes comprising hundreds of frames, each with multiple objects (vehicles, pedestrians, traffic signs, etc.) like you see in Figure 1.\n\nFigure 1. Example of Polygonal Annotation in Sama.\n\n\n\nIn this post, we summarize an approach that we have developed to speed up polygonal instance segmentation using machine learning. This approach was presented earlier this year at the CVPR Workshop on Scalability in Autonomous Driving, and the ICML Workshop on Human-in-the-Loop Learning.\n\nFew-Click Annotation\n\nBuilding instance segmentation Deep Learning (DL) models for autonomous vehicles requires a significant amount of labeled data. The use of Machine Learning (ML) for producing pre-annotations to be reviewed by human annotators, whether in an interactive setting or as a pre-processing procedure, is a very popular approach for scaling up labeling while controlling the costs.\n\nMultiple approaches have been suggested for machine-assisted instance segmentation. These typically consist of a DL-based segmentation of the object(s) integrated into a human-in-the-loop system. The human can interact with the system by correcting the model output, initializing the model with one or several clicks, or a combination of those steps. Examples of such systems include Polygon-RNN++ [1], DELSE [7], DEXTR [3], and CurveGCN [2]. Those systems all present good results, but some open questions remain:\n\nDo these methods perform well when a production-level accuracy is required, as when working for a customer project?\n\nDoes the choice of annotation tool influence the results? The gains to be made by using ML depend on how difficult it is for humans to draw polygons in the provided UI. Here we used our optimized drawing tool for polygons, which is part of our labeling platform.\n\nML integration is not usually approached from a human-centric perspective. Beyond the optimization of traditional metrics like IoU, what interactions are most desirable and how should we present the output of the model to annotators?\n\nOur method relies on combining the well-known DEXTR [3] approach with a raster-to-polygon algorithm, to make the result more easily editable. This is not unlike what other tools (such as CVAT) have implemented, though we have optimized this approach for our specific use cases using A/B testing.\n\nThe Model\n\nOur instance segmentation model is based on the well-known Deep Extreme Cut (DEXTR) approach [3], along with a raster-to-polygon conversion algorithm that yields high quality polygons whose vertices are sampled in a way that reproduces human drawing patterns. The model uses the few clicks provided by human annotators at inference time. The steps are described in Figure 2.\n\nFigure 2. An overview of the approach.\n\n\n\nRegarding the model itself, we adopted a custom version of the UNet [5] along with an EfficientNet backbone [6] (instead of the ResNet backbone used in the paper).\n\nIn our experience, for human annotators to produce good instance segmentation masks efficiently, a polygon annotation tool should be used. As such, we needed to convert the raster masks produced by our model to high quality polygons. To add to the challenge, humans tend to produce sparse polygons, adding vertices only when necessary. We therefore adopted a raster-to-polygon procedure that minimizes the number of output vertices.\n\nA/B Testing the Approach\n\nAt Sama, we use A/B testing as much as possible to systematically refine and improve our new features. To this end, we have developed a flexible testing infrastructure that can ingest and aggregate data from multiple internal processes and is made available to anyone within the organization.\n\nThis framework measures the statistical impact of proposed changes on our efficiency metrics (such as drawing time or shape adjustment time). The significance of observed differences on a given efficiency metric is evaluated using statistical tests.\n\nToy A/B Tests\n\nWe conducted an A/B test of the method using a synthetic automotive dataset called SYNTHIA-AL [8]. The dataset's images and corresponding annotations were generated from video streams at 25 frames per second (FPS). Figure 3 shows SYNTHIA image examples, along with their segmentation (done manually and with the Few-Click tool).\n\nFigure 3. SYNTHIA example images, along with their manual and semi-automated annotations.\n\n\n\nThe test, applied only to motor vehicles, reproduced realistic annotation guidelines, namely:\n\nThe drawn polygon needs to be within 2 pixels of the edge of the vehicle.\n\nAll vehicles down to 10 pixels (height or width) need to be annotated.\n\nFollowing this test, we found a nearly 3-fold reduction in annotation time. On the other hand, we also found that on some of the more complex shapes, annotators were spending quite some time manually adjusting the ML output. DEXTR's authors originally showed that the segmentation can be improved with additional clicks beyond the four initial ones. We therefore extended our few-click tool to allow online refinement of the polygons by considering modifications to their vertices as extra clicks. At train time we simulated the corrective clicks by considering the point of greatest deviation between predicted mask and ground truth as illustrated in Figure 4.\n\nProblem: DEXTR’s 4 extreme clicks are not always sufficient.\n\nObservation: DEXTR trained on 4 clicks benefits from additional clicks.\n\nSolution: Fine-tune DEXTR model with additional clicks for hard samples as established by IoU at train time.\n\nFigure 4. Integrating additional clicks in the training process.\n\n\n\nUsing this method, annotators are able to re-trigger the model inference with an additional click, instead of manually adjusting the output. We proceeded to a second toy A/B test, and results showed that we could obtain a theoretical efficiency gain of up to 3.5x on vehicles using the improved method.\n\nDownload our paper on Human-Centric Efficiency Improvements in Image Annotation for Autonomous Driving here and stay tuned to hear more about our latest advances!\n\n\n\nReferences\n\nAcuna, D., Ling, H., Kar, A., and Fidler, S. Efficient annotation of segmentation datasets with polygon-rnn++. In CVPR, 2018.\n\nLing, H., Gao, J., Kar, A., Chen, W., and Fidler, S. Fast interactive object annotation with curve-gcn. In CVPR, 2019.\n\nManinis, K.-K., Caelles, S., Pont-Tuset, J., and Van Gool, L. Deep extreme cut: From extreme points to object segmentation. In Computer Vision and Pattern Recognition (CVPR), 2018.\n\nPapadopoulos, D., Uijlings, J., Keller, F., and Ferrari, V. Extreme clicking for efficient object annotation. In ICCV, 2017.\n\nRonneberger, O., Fischer, P., and Brox, T. U-net: Convolutional networks for biomedical image segmentation. CoRR, abs/1505.04597, 2015. URL http://arxiv. org/abs/1505.04597.\n\nTan, M. and Le, Q. V. Efficientnet: Rethinking model scaling for convolutional neural networks. CoRR, abs/1905.11946, 2019. URL http://arxiv.org/ abs/1905.11946.\n\nWang, Z., Acuna, D., Ling, H., Kar, A., and Fidler, S. Object instance annotation with deep extreme level set evolution. In CVPR, 2019.\n\nZolfaghari Bengar, J., Gonzalez-Garcia, A., Villalonga, G., Raducanu, B., Aghdam, H. H., Mozerov, M., Lopez, A. M., and van de Weijer, J. Temporal coherence for active learning in videos. arXiv preprint arXiv:1908.11757, 2019.\n\nThis post was written by Frederic Ratle and Martine Bertrand.","relatedPosts":[{"_id":"edd6abc4-87b4-42db-a32a-15900a353dbf","featured_image":{"_type":"image","asset":{"_ref":"image-30d0eaccd1e57322b31a4e16b84576ef1f8db57e-1920x960-png","_type":"reference"}},"slug":{"_type":"slug","current":"zerog-aircraft-turnaround"},"tags":[{"label":"Case Studies","value":"Case Studies"}],"title":"High-Quality Labeled Data Fuels zeroG’s Mission to Optimize Aircraft Turnaround Management"},{"_id":"4d9c5816-3584-4622-8ea6-77869ca8dbf0","featured_image":{"_type":"image","asset":{"_ref":"image-f80b5e83a9927bafc61285d9e7a16b07070f53c1-1200x630-png","_type":"reference"}},"slug":{"_type":"slug","current":"sama-mila-partnership"},"tags":[{"_key":"ZLub2KFj","label":"Company News","value":"Company News"}],"title":"Sama Partners with Mila to Solve Key Problems in AI Development"},{"_id":"728400d2-d453-42f7-b20c-47f753bc4583","featured_image":{"_type":"image","asset":{"_ref":"image-1502043edcf701cb4267cb2f6d7c6edf8e0e0cad-2500x1313-png","_type":"reference"}},"slug":{"_type":"slug","current":"podcast-episode-facebook-manohar-paluri"},"tags":[{"_key":"6BgUw5oN","label":"Podcast","value":"Podcast"}],"title":"New Podcast Episode: Facebook's Manohar Paluri Makes Machines See"}],"seo_title":"Fast Vector Annotation with Machine Learning Assisted Annotation","slug":{"_type":"slug","current":"fast-vector-annotation"},"tags":[{"_key":"fJGSmFCx","label":"Vector Annotation","value":"Vector Annotation"},{"_key":"21OOfbwx","label":"Polygons","value":"Polygons"},{"_key":"SPABaoXN","label":"Sama Engineering","value":"Sama Engineering"},{"_key":"O5nmmFbm","label":"Featured","value":"Featured"}],"title":"Fast Vector Annotation with Machine Learning Assisted Annotation"},"config":{"title":"Fast Vector Annotation with Machine Learning Assisted Annotation","description":"In this article we summarize an approach that we have developed to speed up polygonal instance segmentation using machine learning.","openGraphImage":""}}},"__N_SSG":true}