<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"/><meta name="viewport" content="initial-scale=1.0, width=device-width, viewport-fit=cover"/><title>Sama Blog | Training Data, AI and Impact Sourcing Insights</title><link rel="canonical" href="https://sama.com/blog/tag/sama-engineering"/><meta name="description" content="From machine learning to training data strategy, the Sama blog covers research, news and other AI trends from thought leaders across the globe."/><meta property="og:type" content="website"/><meta property="og:description" content="From machine learning to training data strategy, the Sama blog covers research, news and other AI trends from thought leaders across the globe."/><meta property="og:title" content="Sama Blog | Training Data, AI and Impact Sourcing Insights"/><meta property="og:url" content="https://sama.com/blog/tag/sama-engineering"/><meta name="twitter:card" content="summary"/><meta property="twitter:description" content="From machine learning to training data strategy, the Sama blog covers research, news and other AI trends from thought leaders across the globe."/><meta property="twitter:title" content="Sama Blog | Training Data, AI and Impact Sourcing Insights"/><meta name="msapplication-TileColor" content="#28282a"/><meta name="theme-color" content="#ffffff"/><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"/><link rel="shortcut icon" href="/favicon.ico"/><meta name="next-head-count" content="17"/><link rel="preload" href="/_next/static/css/bd60e2be2420db639f1f.css" as="style"/><link rel="stylesheet" href="/_next/static/css/bd60e2be2420db639f1f.css" data-n-g=""/><link rel="preload" href="/_next/static/css/bfc9e7bd13d5ad59bf17.css" as="style"/><link rel="stylesheet" href="/_next/static/css/bfc9e7bd13d5ad59bf17.css" data-n-p=""/><link rel="preload" href="/_next/static/css/0d4bd6b9e4f2c8d7d433.css" as="style"/><link rel="stylesheet" href="/_next/static/css/0d4bd6b9e4f2c8d7d433.css"/><link rel="preload" href="/_next/static/css/3b03d00d40d80e105549.css" as="style"/><link rel="stylesheet" href="/_next/static/css/3b03d00d40d80e105549.css"/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-a40ef1678bae11e696dba45124eadd70.js"></script><script defer="" src="/_next/static/chunks/1776.90811ba0e7bd8502bb5d.js"></script><script defer="" src="/_next/static/chunks/4934.37651fffe4e244d39030.js"></script><script defer="" src="/_next/static/chunks/1952.c48d8556ec1dc7f6b94b.js"></script><script defer="" src="/_next/static/chunks/3551.489d89a23d509b64ee09.js"></script><script src="/_next/static/chunks/webpack-34ffe2c262fa1e2c00d4.js" defer=""></script><script src="/_next/static/chunks/framework-bdc1b4e5e48979e16d36.js" defer=""></script><script src="/_next/static/chunks/main-6409a04df91a58e5134b.js" defer=""></script><script src="/_next/static/chunks/pages/_app-8b37b83a9f5e9f7f90ae.js" defer=""></script><script src="/_next/static/chunks/commons-3318f1705e53c96f3f49.js" defer=""></script><script src="/_next/static/chunks/pages/blog/tag/%5Bslug%5D-03b8b977e5f158c8d5d1.js" defer=""></script><script src="/_next/static/dMo_2owL0njcN1tTH_wi3/_buildManifest.js" defer=""></script><script src="/_next/static/dMo_2owL0njcN1tTH_wi3/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="container"><header class="header_outer__yu9q7 "><nav class="umoja-l-grid--12 header_wrapper__3Ghzm"><a class="header_logo__eiLSq" href="/"></a><button class="header_hamburger__1ZcbZ" type="button"><span class="header_hamburger_box__RZ7CY"><span class="header_hamburger_box_inner__1PmWZ"></span></span></button><ul class="header_navBar__37eSJ"><li class="header_navItem__1HiGN"><p class="header_navItem_label__fN3Cy">Platform</p><div class="header_dropDown__6NxUb"><div class="header_dropDown_group__2BZXC"><p class="header_dropdown_group_label__tmND8">Platform</p><a class="header_navLink__1ARu5" href="/how-it-works">How it Works</a><a class="header_navLink__1ARu5" href="/video-annotation">Video Annotation</a><a class="header_navLink__1ARu5" href="/image-annotation">Image Annotation</a><a class="header_navLink__1ARu5" href="/3d-lidar">3D &amp; LiDAR Annotation</a><a class="header_navLink__1ARu5" href="/natural-language-processing">Natural Language Processing</a><a class="header_navLink__1ARu5" href="/data-curation">Data Curation (Beta)</a></div><div class="header_dropDown_group__2BZXC"><p class="header_dropdown_group_label__tmND8">Shapes</p><a class="header_navLink__1ARu5" href="/semantic-segmentation">Semantic Segmentation</a><a class="header_navLink__1ARu5" href="/polygons">Polygons</a><a class="header_navLink__1ARu5" href="/bounding-boxes">Bounding Boxes</a><a class="header_navLink__1ARu5" href="/key-points">Key Points</a><a class="header_navLink__1ARu5" href="/cuboids">Cuboids</a><a class="header_navLink__1ARu5" href="/lines-and-arrows">Lines &amp; Arrows</a></div></div></li><li class="header_navItem__1HiGN"><p class="header_navItem_label__fN3Cy">Industries</p><div class="header_dropDown__6NxUb"><a class="header_navLink__1ARu5" href="/transportation-navigation">Transportation &amp; Navigation</a><a class="header_navLink__1ARu5" href="/retail-ecommerce">Retail &amp; E-Commerce</a><a class="header_navLink__1ARu5" href="/consumer-media">Consumer &amp; Media</a><a class="header_navLink__1ARu5" href="/biotech-medtech">Biotech &amp; Medtech</a><a class="header_navLink__1ARu5" href="/robotics-and-manufacturing">Robotics &amp; Manufacturing</a><a class="header_navLink__1ARu5" href="/training-data-food-agriculture">Food &amp; Agriculture</a></div></li><li class="header_navItem__1HiGN"><p class="header_navItem_label__fN3Cy">Why Sama</p><div class="header_dropDown__6NxUb"><a class="header_navLink__1ARu5" href="/quality-training-data">Quality</a><a class="header_navLink__1ARu5" href="/security-and-trust">Security</a><a class="header_navLink__1ARu5" href="/our-impact">Ethical AI</a><a class="header_navLink__1ARu5" href="/compare">Compare</a><a class="header_navLink__1ARu5" href="/partners">Partners</a></div></li><li class="header_navItem__1HiGN"><p class="header_navItem_label__fN3Cy">Resources</p><div class="header_dropDown__6NxUb"><a href="https://docs.sama.com/reference/overview" class="header_navLink__1ARu5" target="_blank">API Documentation</a><a class="header_navLink__1ARu5" href="/blog">Blog</a><a class="header_navLink__1ARu5" href="/events">Events</a></div></li><li class="header_navItem__1HiGN"><p class="header_navItem_label__fN3Cy">Company</p><div class="header_dropDown__6NxUb"><a class="header_navLink__1ARu5" href="/our-story">Our Story</a><a class="header_navLink__1ARu5" href="/our-team">Our Team</a><a class="header_navLink__1ARu5" href="/careers">Careers</a><a class="header_navLink__1ARu5" href="/company-contact">Contact</a><a class="header_navLink__1ARu5" href="/press">Press</a></div></li></ul><div class="header_cta__3J8I7"><a class="button_wrapper__3lRbv button__secondary__1pZ5q button__small__2kIwW" href="/[object%20Object]"><button class="button_btn__1qxP1"><h3 class="button_text__3_sCS">Request a Demo</h3></button></a></div></nav></header><main class="content"><section class="umoja-l-grid-section umoja-u-bg--white"><div class="umoja-l-grid--12"><div class="blog-tag_name__-rmiA"><h1>Sama Engineering</h1></div></div></section><section class="umoja-l-grid-section umoja-u-bg--white"><div class="umoja-l-grid--12 umoja-l-grid-gap--row-1"><div class="blog-post_postCard__i0O-O"><a class="blog-post_postCard_image__3k8eA" href="/[object%20Object]"><div style="display:block;overflow:hidden;position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;margin:0"><img alt="" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="fill" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover"/><noscript><img alt="" sizes="100vw" srcSet="https://cdn.sanity.io/images/76e3r62u/production/bbf0e002b88fd9c3b2fcbd032f60f114bc93eed0-1980x990.png?w=640&amp;q=75&amp;fit=clip&amp;auto=format 640w, https://cdn.sanity.io/images/76e3r62u/production/bbf0e002b88fd9c3b2fcbd032f60f114bc93eed0-1980x990.png?w=750&amp;q=75&amp;fit=clip&amp;auto=format 750w, https://cdn.sanity.io/images/76e3r62u/production/bbf0e002b88fd9c3b2fcbd032f60f114bc93eed0-1980x990.png?w=828&amp;q=75&amp;fit=clip&amp;auto=format 828w, https://cdn.sanity.io/images/76e3r62u/production/bbf0e002b88fd9c3b2fcbd032f60f114bc93eed0-1980x990.png?w=1080&amp;q=75&amp;fit=clip&amp;auto=format 1080w, https://cdn.sanity.io/images/76e3r62u/production/bbf0e002b88fd9c3b2fcbd032f60f114bc93eed0-1980x990.png?w=1200&amp;q=75&amp;fit=clip&amp;auto=format 1200w, https://cdn.sanity.io/images/76e3r62u/production/bbf0e002b88fd9c3b2fcbd032f60f114bc93eed0-1980x990.png?w=1920&amp;q=75&amp;fit=clip&amp;auto=format 1920w, https://cdn.sanity.io/images/76e3r62u/production/bbf0e002b88fd9c3b2fcbd032f60f114bc93eed0-1980x990.png?w=2048&amp;q=75&amp;fit=clip&amp;auto=format 2048w, https://cdn.sanity.io/images/76e3r62u/production/bbf0e002b88fd9c3b2fcbd032f60f114bc93eed0-1980x990.png?w=3840&amp;q=75&amp;fit=clip&amp;auto=format 3840w" src="https://cdn.sanity.io/images/76e3r62u/production/bbf0e002b88fd9c3b2fcbd032f60f114bc93eed0-1980x990.png?w=3840&amp;q=75&amp;fit=clip&amp;auto=format" decoding="async" data-nimg="fill" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover" loading="lazy"/></noscript></div></a><div class="blog-post_postCard_data__2vfn3"><a href="/[object%20Object]"><h3>Part 3: A/B Testing with Python</h3></a><a class="blog-post_postCard_author__Y7RjO" href="/[object%20Object]">Rafael Alfaro &amp; Juan Esquivel</a><p class="blog-post_postCard_date__hrDMA ">March 16, 2021<!-- --> | 6 Min Read</p></div></div><div class="blog-post_postCard__i0O-O"><a class="blog-post_postCard_image__3k8eA" href="/[object%20Object]"><div style="display:block;overflow:hidden;position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;margin:0"><img alt="" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="fill" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover"/><noscript><img alt="" sizes="100vw" srcSet="https://cdn.sanity.io/images/76e3r62u/production/bbf0e002b88fd9c3b2fcbd032f60f114bc93eed0-1980x990.png?w=640&amp;q=75&amp;fit=clip&amp;auto=format 640w, https://cdn.sanity.io/images/76e3r62u/production/bbf0e002b88fd9c3b2fcbd032f60f114bc93eed0-1980x990.png?w=750&amp;q=75&amp;fit=clip&amp;auto=format 750w, https://cdn.sanity.io/images/76e3r62u/production/bbf0e002b88fd9c3b2fcbd032f60f114bc93eed0-1980x990.png?w=828&amp;q=75&amp;fit=clip&amp;auto=format 828w, https://cdn.sanity.io/images/76e3r62u/production/bbf0e002b88fd9c3b2fcbd032f60f114bc93eed0-1980x990.png?w=1080&amp;q=75&amp;fit=clip&amp;auto=format 1080w, https://cdn.sanity.io/images/76e3r62u/production/bbf0e002b88fd9c3b2fcbd032f60f114bc93eed0-1980x990.png?w=1200&amp;q=75&amp;fit=clip&amp;auto=format 1200w, https://cdn.sanity.io/images/76e3r62u/production/bbf0e002b88fd9c3b2fcbd032f60f114bc93eed0-1980x990.png?w=1920&amp;q=75&amp;fit=clip&amp;auto=format 1920w, https://cdn.sanity.io/images/76e3r62u/production/bbf0e002b88fd9c3b2fcbd032f60f114bc93eed0-1980x990.png?w=2048&amp;q=75&amp;fit=clip&amp;auto=format 2048w, https://cdn.sanity.io/images/76e3r62u/production/bbf0e002b88fd9c3b2fcbd032f60f114bc93eed0-1980x990.png?w=3840&amp;q=75&amp;fit=clip&amp;auto=format 3840w" src="https://cdn.sanity.io/images/76e3r62u/production/bbf0e002b88fd9c3b2fcbd032f60f114bc93eed0-1980x990.png?w=3840&amp;q=75&amp;fit=clip&amp;auto=format" decoding="async" data-nimg="fill" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover" loading="lazy"/></noscript></div></a><div class="blog-post_postCard_data__2vfn3"><a href="/[object%20Object]"><h3>Part 2: A/B Testing</h3></a><a class="blog-post_postCard_author__Y7RjO" href="/[object%20Object]">Rafael Alfaro &amp; Juan Esquivel</a><p class="blog-post_postCard_date__hrDMA ">March 8, 2021<!-- --> | 4 Min Read</p></div></div><div class="blog-post_postCard__i0O-O"><a class="blog-post_postCard_image__3k8eA" href="/[object%20Object]"><div style="display:block;overflow:hidden;position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;margin:0"><img alt="" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="fill" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover"/><noscript><img alt="" sizes="100vw" srcSet="https://cdn.sanity.io/images/76e3r62u/production/bbf0e002b88fd9c3b2fcbd032f60f114bc93eed0-1980x990.png?w=640&amp;q=75&amp;fit=clip&amp;auto=format 640w, https://cdn.sanity.io/images/76e3r62u/production/bbf0e002b88fd9c3b2fcbd032f60f114bc93eed0-1980x990.png?w=750&amp;q=75&amp;fit=clip&amp;auto=format 750w, https://cdn.sanity.io/images/76e3r62u/production/bbf0e002b88fd9c3b2fcbd032f60f114bc93eed0-1980x990.png?w=828&amp;q=75&amp;fit=clip&amp;auto=format 828w, https://cdn.sanity.io/images/76e3r62u/production/bbf0e002b88fd9c3b2fcbd032f60f114bc93eed0-1980x990.png?w=1080&amp;q=75&amp;fit=clip&amp;auto=format 1080w, https://cdn.sanity.io/images/76e3r62u/production/bbf0e002b88fd9c3b2fcbd032f60f114bc93eed0-1980x990.png?w=1200&amp;q=75&amp;fit=clip&amp;auto=format 1200w, https://cdn.sanity.io/images/76e3r62u/production/bbf0e002b88fd9c3b2fcbd032f60f114bc93eed0-1980x990.png?w=1920&amp;q=75&amp;fit=clip&amp;auto=format 1920w, https://cdn.sanity.io/images/76e3r62u/production/bbf0e002b88fd9c3b2fcbd032f60f114bc93eed0-1980x990.png?w=2048&amp;q=75&amp;fit=clip&amp;auto=format 2048w, https://cdn.sanity.io/images/76e3r62u/production/bbf0e002b88fd9c3b2fcbd032f60f114bc93eed0-1980x990.png?w=3840&amp;q=75&amp;fit=clip&amp;auto=format 3840w" src="https://cdn.sanity.io/images/76e3r62u/production/bbf0e002b88fd9c3b2fcbd032f60f114bc93eed0-1980x990.png?w=3840&amp;q=75&amp;fit=clip&amp;auto=format" decoding="async" data-nimg="fill" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover" loading="lazy"/></noscript></div></a><div class="blog-post_postCard_data__2vfn3"><a href="/[object%20Object]"><h3>Part 1: Experiment Driven Development</h3></a><a class="blog-post_postCard_author__Y7RjO" href="/[object%20Object]">Rafael Alfaro &amp; Juan Esquivel</a><p class="blog-post_postCard_date__hrDMA ">March 1, 2021<!-- --> | 3 Min Read</p></div></div><div class="blog-post_postCard__i0O-O"><a class="blog-post_postCard_image__3k8eA" href="/[object%20Object]"><div style="display:block;overflow:hidden;position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;margin:0"><img alt="" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="fill" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover"/><noscript><img alt="" sizes="100vw" srcSet="https://cdn.sanity.io/images/76e3r62u/production/19ef3f125b8ee47727e9f94dc3aff7ed08a4d78c-720x360.png?w=640&amp;q=75&amp;fit=clip&amp;auto=format 640w, https://cdn.sanity.io/images/76e3r62u/production/19ef3f125b8ee47727e9f94dc3aff7ed08a4d78c-720x360.png?w=750&amp;q=75&amp;fit=clip&amp;auto=format 750w, https://cdn.sanity.io/images/76e3r62u/production/19ef3f125b8ee47727e9f94dc3aff7ed08a4d78c-720x360.png?w=828&amp;q=75&amp;fit=clip&amp;auto=format 828w, https://cdn.sanity.io/images/76e3r62u/production/19ef3f125b8ee47727e9f94dc3aff7ed08a4d78c-720x360.png?w=1080&amp;q=75&amp;fit=clip&amp;auto=format 1080w, https://cdn.sanity.io/images/76e3r62u/production/19ef3f125b8ee47727e9f94dc3aff7ed08a4d78c-720x360.png?w=1200&amp;q=75&amp;fit=clip&amp;auto=format 1200w, https://cdn.sanity.io/images/76e3r62u/production/19ef3f125b8ee47727e9f94dc3aff7ed08a4d78c-720x360.png?w=1920&amp;q=75&amp;fit=clip&amp;auto=format 1920w, https://cdn.sanity.io/images/76e3r62u/production/19ef3f125b8ee47727e9f94dc3aff7ed08a4d78c-720x360.png?w=2048&amp;q=75&amp;fit=clip&amp;auto=format 2048w, https://cdn.sanity.io/images/76e3r62u/production/19ef3f125b8ee47727e9f94dc3aff7ed08a4d78c-720x360.png?w=3840&amp;q=75&amp;fit=clip&amp;auto=format 3840w" src="https://cdn.sanity.io/images/76e3r62u/production/19ef3f125b8ee47727e9f94dc3aff7ed08a4d78c-720x360.png?w=3840&amp;q=75&amp;fit=clip&amp;auto=format" decoding="async" data-nimg="fill" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover" loading="lazy"/></noscript></div></a><div class="blog-post_postCard_data__2vfn3"><a href="/[object%20Object]"><h3>Factotum: Containerizing DevOps Tools for Cloud Native Engineering and CI/CD</h3></a><a class="blog-post_postCard_author__Y7RjO" href="/[object%20Object]">Mathieu Frenette</a><p class="blog-post_postCard_date__hrDMA ">February 1, 2021<!-- --> | 4 Min Read</p></div></div><div class="blog-post_postCard__i0O-O"><a class="blog-post_postCard_image__3k8eA" href="/[object%20Object]"><div style="display:block;overflow:hidden;position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;margin:0"><img alt="" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="fill" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover"/><noscript><img alt="" sizes="100vw" srcSet="https://cdn.sanity.io/images/76e3r62u/production/04641081798c41caebebefa44685828d80b7434f-1200x675.png?w=640&amp;q=75&amp;fit=clip&amp;auto=format 640w, https://cdn.sanity.io/images/76e3r62u/production/04641081798c41caebebefa44685828d80b7434f-1200x675.png?w=750&amp;q=75&amp;fit=clip&amp;auto=format 750w, https://cdn.sanity.io/images/76e3r62u/production/04641081798c41caebebefa44685828d80b7434f-1200x675.png?w=828&amp;q=75&amp;fit=clip&amp;auto=format 828w, https://cdn.sanity.io/images/76e3r62u/production/04641081798c41caebebefa44685828d80b7434f-1200x675.png?w=1080&amp;q=75&amp;fit=clip&amp;auto=format 1080w, https://cdn.sanity.io/images/76e3r62u/production/04641081798c41caebebefa44685828d80b7434f-1200x675.png?w=1200&amp;q=75&amp;fit=clip&amp;auto=format 1200w, https://cdn.sanity.io/images/76e3r62u/production/04641081798c41caebebefa44685828d80b7434f-1200x675.png?w=1920&amp;q=75&amp;fit=clip&amp;auto=format 1920w, https://cdn.sanity.io/images/76e3r62u/production/04641081798c41caebebefa44685828d80b7434f-1200x675.png?w=2048&amp;q=75&amp;fit=clip&amp;auto=format 2048w, https://cdn.sanity.io/images/76e3r62u/production/04641081798c41caebebefa44685828d80b7434f-1200x675.png?w=3840&amp;q=75&amp;fit=clip&amp;auto=format 3840w" src="https://cdn.sanity.io/images/76e3r62u/production/04641081798c41caebebefa44685828d80b7434f-1200x675.png?w=3840&amp;q=75&amp;fit=clip&amp;auto=format" decoding="async" data-nimg="fill" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover" loading="lazy"/></noscript></div></a><div class="blog-post_postCard_data__2vfn3"><a href="/[object%20Object]"><h3>The Sama MLOps Pipeline: Automating Model Training on the Cloud</h3></a><a class="blog-post_postCard_author__Y7RjO" href="/[object%20Object]">Dimitri Gallos</a><p class="blog-post_postCard_date__hrDMA ">January 19, 2021<!-- --> | 3 Min Read</p></div></div><div class="blog-post_postCard__i0O-O"><a class="blog-post_postCard_image__3k8eA" href="/[object%20Object]"><div style="display:block;overflow:hidden;position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;margin:0"><img alt="" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="fill" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover"/><noscript><img alt="" sizes="100vw" srcSet="https://cdn.sanity.io/images/76e3r62u/production/6abd2f846119ba50a8325787f99ec3aeec68ffff-1076x605.png?w=640&amp;q=75&amp;fit=clip&amp;auto=format 640w, https://cdn.sanity.io/images/76e3r62u/production/6abd2f846119ba50a8325787f99ec3aeec68ffff-1076x605.png?w=750&amp;q=75&amp;fit=clip&amp;auto=format 750w, https://cdn.sanity.io/images/76e3r62u/production/6abd2f846119ba50a8325787f99ec3aeec68ffff-1076x605.png?w=828&amp;q=75&amp;fit=clip&amp;auto=format 828w, https://cdn.sanity.io/images/76e3r62u/production/6abd2f846119ba50a8325787f99ec3aeec68ffff-1076x605.png?w=1080&amp;q=75&amp;fit=clip&amp;auto=format 1080w, https://cdn.sanity.io/images/76e3r62u/production/6abd2f846119ba50a8325787f99ec3aeec68ffff-1076x605.png?w=1200&amp;q=75&amp;fit=clip&amp;auto=format 1200w, https://cdn.sanity.io/images/76e3r62u/production/6abd2f846119ba50a8325787f99ec3aeec68ffff-1076x605.png?w=1920&amp;q=75&amp;fit=clip&amp;auto=format 1920w, https://cdn.sanity.io/images/76e3r62u/production/6abd2f846119ba50a8325787f99ec3aeec68ffff-1076x605.png?w=2048&amp;q=75&amp;fit=clip&amp;auto=format 2048w, https://cdn.sanity.io/images/76e3r62u/production/6abd2f846119ba50a8325787f99ec3aeec68ffff-1076x605.png?w=3840&amp;q=75&amp;fit=clip&amp;auto=format 3840w" src="https://cdn.sanity.io/images/76e3r62u/production/6abd2f846119ba50a8325787f99ec3aeec68ffff-1076x605.png?w=3840&amp;q=75&amp;fit=clip&amp;auto=format" decoding="async" data-nimg="fill" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover" loading="lazy"/></noscript></div></a><div class="blog-post_postCard_data__2vfn3"><a href="/[object%20Object]"><h3>Fast Vector Annotation with Machine Learning Assisted Annotation</h3></a><a class="blog-post_postCard_author__Y7RjO" href="/[object%20Object]">Frederic Ratle</a><p class="blog-post_postCard_date__hrDMA ">December 15, 2020<!-- --> | 8 Min Read</p></div></div><div class="blog-post_postCard__i0O-O"><a class="blog-post_postCard_image__3k8eA" href="/[object%20Object]"><div style="display:block;overflow:hidden;position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;margin:0"><img alt="" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="fill" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover"/><noscript><img alt="" sizes="100vw" srcSet="https://cdn.sanity.io/images/76e3r62u/production/8b71517603a306e0f80ea070e3d0d532f0039105-1024x512.png?w=640&amp;q=75&amp;fit=clip&amp;auto=format 640w, https://cdn.sanity.io/images/76e3r62u/production/8b71517603a306e0f80ea070e3d0d532f0039105-1024x512.png?w=750&amp;q=75&amp;fit=clip&amp;auto=format 750w, https://cdn.sanity.io/images/76e3r62u/production/8b71517603a306e0f80ea070e3d0d532f0039105-1024x512.png?w=828&amp;q=75&amp;fit=clip&amp;auto=format 828w, https://cdn.sanity.io/images/76e3r62u/production/8b71517603a306e0f80ea070e3d0d532f0039105-1024x512.png?w=1080&amp;q=75&amp;fit=clip&amp;auto=format 1080w, https://cdn.sanity.io/images/76e3r62u/production/8b71517603a306e0f80ea070e3d0d532f0039105-1024x512.png?w=1200&amp;q=75&amp;fit=clip&amp;auto=format 1200w, https://cdn.sanity.io/images/76e3r62u/production/8b71517603a306e0f80ea070e3d0d532f0039105-1024x512.png?w=1920&amp;q=75&amp;fit=clip&amp;auto=format 1920w, https://cdn.sanity.io/images/76e3r62u/production/8b71517603a306e0f80ea070e3d0d532f0039105-1024x512.png?w=2048&amp;q=75&amp;fit=clip&amp;auto=format 2048w, https://cdn.sanity.io/images/76e3r62u/production/8b71517603a306e0f80ea070e3d0d532f0039105-1024x512.png?w=3840&amp;q=75&amp;fit=clip&amp;auto=format 3840w" src="https://cdn.sanity.io/images/76e3r62u/production/8b71517603a306e0f80ea070e3d0d532f0039105-1024x512.png?w=3840&amp;q=75&amp;fit=clip&amp;auto=format" decoding="async" data-nimg="fill" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover" loading="lazy"/></noscript></div></a><div class="blog-post_postCard_data__2vfn3"><a href="/[object%20Object]"><h3>Code.Jam(2020)-McGill Hackathon: and the winner is A Virtual Fitting Room</h3></a><a class="blog-post_postCard_author__Y7RjO" href="/[object%20Object]">Loic Juillard</a><p class="blog-post_postCard_date__hrDMA ">November 16, 2020<!-- --> | 3 Min Read</p></div></div><div class="blog-post_postCard__i0O-O"><a class="blog-post_postCard_image__3k8eA" href="/[object%20Object]"><div style="display:block;overflow:hidden;position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;margin:0"><img alt="" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="fill" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover"/><noscript><img alt="" sizes="100vw" srcSet="https://cdn.sanity.io/images/76e3r62u/production/ade4117797bfbff2e90f48d6ad558ab992aca273-3423x2283.jpg?w=640&amp;q=75&amp;fit=clip&amp;auto=format 640w, https://cdn.sanity.io/images/76e3r62u/production/ade4117797bfbff2e90f48d6ad558ab992aca273-3423x2283.jpg?w=750&amp;q=75&amp;fit=clip&amp;auto=format 750w, https://cdn.sanity.io/images/76e3r62u/production/ade4117797bfbff2e90f48d6ad558ab992aca273-3423x2283.jpg?w=828&amp;q=75&amp;fit=clip&amp;auto=format 828w, https://cdn.sanity.io/images/76e3r62u/production/ade4117797bfbff2e90f48d6ad558ab992aca273-3423x2283.jpg?w=1080&amp;q=75&amp;fit=clip&amp;auto=format 1080w, https://cdn.sanity.io/images/76e3r62u/production/ade4117797bfbff2e90f48d6ad558ab992aca273-3423x2283.jpg?w=1200&amp;q=75&amp;fit=clip&amp;auto=format 1200w, https://cdn.sanity.io/images/76e3r62u/production/ade4117797bfbff2e90f48d6ad558ab992aca273-3423x2283.jpg?w=1920&amp;q=75&amp;fit=clip&amp;auto=format 1920w, https://cdn.sanity.io/images/76e3r62u/production/ade4117797bfbff2e90f48d6ad558ab992aca273-3423x2283.jpg?w=2048&amp;q=75&amp;fit=clip&amp;auto=format 2048w, https://cdn.sanity.io/images/76e3r62u/production/ade4117797bfbff2e90f48d6ad558ab992aca273-3423x2283.jpg?w=3840&amp;q=75&amp;fit=clip&amp;auto=format 3840w" src="https://cdn.sanity.io/images/76e3r62u/production/ade4117797bfbff2e90f48d6ad558ab992aca273-3423x2283.jpg?w=3840&amp;q=75&amp;fit=clip&amp;auto=format" decoding="async" data-nimg="fill" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover" loading="lazy"/></noscript></div></a><div class="blog-post_postCard_data__2vfn3"><a href="/[object%20Object]"><h3>8 Answers to Your Questions About AI and Machine Learning</h3></a><a class="blog-post_postCard_author__Y7RjO" href="/[object%20Object]">Sharon L. Hadden</a><p class="blog-post_postCard_date__hrDMA ">December 19, 2019<!-- --> | 18 Min Read</p></div></div><div class="blog-post_postCard__i0O-O"><a class="blog-post_postCard_image__3k8eA" href="/[object%20Object]"><div style="display:block;overflow:hidden;position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;margin:0"><img alt="" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="fill" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover"/><noscript><img alt="" sizes="100vw" srcSet="https://cdn.sanity.io/images/76e3r62u/production/980fdc38382a1cc5d9c462b19141b595b5d9ea31-974x460.png?w=640&amp;q=75&amp;fit=clip&amp;auto=format 640w, https://cdn.sanity.io/images/76e3r62u/production/980fdc38382a1cc5d9c462b19141b595b5d9ea31-974x460.png?w=750&amp;q=75&amp;fit=clip&amp;auto=format 750w, https://cdn.sanity.io/images/76e3r62u/production/980fdc38382a1cc5d9c462b19141b595b5d9ea31-974x460.png?w=828&amp;q=75&amp;fit=clip&amp;auto=format 828w, https://cdn.sanity.io/images/76e3r62u/production/980fdc38382a1cc5d9c462b19141b595b5d9ea31-974x460.png?w=1080&amp;q=75&amp;fit=clip&amp;auto=format 1080w, https://cdn.sanity.io/images/76e3r62u/production/980fdc38382a1cc5d9c462b19141b595b5d9ea31-974x460.png?w=1200&amp;q=75&amp;fit=clip&amp;auto=format 1200w, https://cdn.sanity.io/images/76e3r62u/production/980fdc38382a1cc5d9c462b19141b595b5d9ea31-974x460.png?w=1920&amp;q=75&amp;fit=clip&amp;auto=format 1920w, https://cdn.sanity.io/images/76e3r62u/production/980fdc38382a1cc5d9c462b19141b595b5d9ea31-974x460.png?w=2048&amp;q=75&amp;fit=clip&amp;auto=format 2048w, https://cdn.sanity.io/images/76e3r62u/production/980fdc38382a1cc5d9c462b19141b595b5d9ea31-974x460.png?w=3840&amp;q=75&amp;fit=clip&amp;auto=format 3840w" src="https://cdn.sanity.io/images/76e3r62u/production/980fdc38382a1cc5d9c462b19141b595b5d9ea31-974x460.png?w=3840&amp;q=75&amp;fit=clip&amp;auto=format" decoding="async" data-nimg="fill" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover" loading="lazy"/></noscript></div></a><div class="blog-post_postCard_data__2vfn3"><a href="/[object%20Object]"><h3>Highlights from McGill CodeJam 2019</h3></a><a class="blog-post_postCard_author__Y7RjO" href="/[object%20Object]">Taylor Rouleau</a><p class="blog-post_postCard_date__hrDMA ">December 2, 2019<!-- --> | 2 Min Read</p></div></div></div></section></main><footer class="footer_wrapper__2VAfJ"><div class="umoja-l-grid--12"><div class="footer_upper__2a6XG"><div><h4>Newsletter</h4><p>Subscribe today and be the first to receive the latest from Sama.</p></div><div class="footer_upper_right__cpliC"><div><p class="footer_nav_head__1keQK">Guides</p><a class="footer_nav_link__X1RNI" href="/training-data-for-autonomous-driving">Autonomous Transportation</a><a class="footer_nav_link__X1RNI" href="/training-data-for-ecommerce">E-Commerce</a><a class="footer_nav_link__X1RNI" href="/training-data-for-ar-vr">AR/VR</a><a class="footer_nav_link__X1RNI" href="/data-quality">Data Quality</a></div><div><p class="footer_nav_head__1keQK">Company</p><a class="footer_nav_link__X1RNI" href="/our-story">Our Story</a><a class="footer_nav_link__X1RNI" href="/our-team">Our Team</a><a class="footer_nav_link__X1RNI" href="/mission-vision-values">Our Mission</a><a class="footer_nav_link__X1RNI" href="/careers">Careers</a><a class="footer_nav_link__X1RNI" href="/company-contact">Contact</a></div></div></div><div class="footer_middle__iiTSJ"><div class="footer_middle_left__3ff78"><a href="/"></a></div><div class="footer_middle_right__2b-lC"><div class="footer_social__1NFfV"><a href="https://www.facebook.com/samaartificialintelligence" class="footer_social_icon__wI2OK" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 35.1 64.89"><title>facebook</title><g id="fb3209c8-23d4-4288-984d-a2b8f32b7f0c" data-name="Layer 2"><g id="ba1a815d-13f1-45f9-9321-18e9a3cfa5db" data-name="Layer 1"><path d="M35.1,11.26V1.36A1.35,1.35,0,0,0,33.76,0H25.35A15.34,15.34,0,0,0,14,4.35C11.24,7.2,9.78,11.22,9.78,16v7.34H1.34A1.34,1.34,0,0,0,0,24.66V35.32a1.35,1.35,0,0,0,1.34,1.35H9.78V63.55a1.34,1.34,0,0,0,1.34,1.34h11a1.34,1.34,0,0,0,1.34-1.34V36.67h9.87a1.35,1.35,0,0,0,1.34-1.35V24.66a1.37,1.37,0,0,0-.7-1.18,1.47,1.47,0,0,0-.67-.16H23.49V17.1c0-1.72.25-2.69.84-3.37s1.88-1.13,3.77-1.13h5.66A1.34,1.34,0,0,0,35.1,11.26Z"></path></g></g></svg></a><a href="https://www.instagram.com/sama_ai_" class="footer_social_icon__wI2OK" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 57 57"><title>insta</title><g id="ef02a3ef-c0d3-4be7-9d4e-2c42263777a3" data-name="Layer 2"><g id="a0a92778-6c5c-4e06-a08f-641546580dd0" data-name="Layer 1"><circle cx="28.5" cy="28.5" r="9.24"></circle><path d="M41.57,0H15.43A15.45,15.45,0,0,0,0,15.43V41.57A15.45,15.45,0,0,0,15.43,57H41.57A15.45,15.45,0,0,0,57,41.57V15.43A15.45,15.45,0,0,0,41.57,0ZM28.5,42.74A14.24,14.24,0,1,1,42.74,28.5,14.26,14.26,0,0,1,28.5,42.74ZM44.46,17a5,5,0,1,1,5-5A5,5,0,0,1,44.46,17Z"></path></g></g></svg></a><a href="https://twitter.com/SamaAI" class="footer_social_icon__wI2OK" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 62 51.19"><title>twitter</title><g id="e7743e55-1863-47ad-a995-4b7f1f924f07" data-name="Layer 2"><g id="ec8c6fd9-2f52-4507-9a24-804bc60dbc33" data-name="Layer 1"><path d="M23.59,51.19c-10.35,0-18.53-1.81-22.44-5l-.07-.06L1,46.1a3.19,3.19,0,0,1-.84-3.35l0-.1a3.24,3.24,0,0,1,3-2,26.57,26.57,0,0,0,7.06-1,13.45,13.45,0,0,1-7.07-8.16,2.92,2.92,0,0,1,1-3.38,3.06,3.06,0,0,1,.88-.45,19.52,19.52,0,0,1-4-7.18l0-.08,0-.09a3,3,0,0,1,1.4-3.23,3,3,0,0,1,1.43-.4,15.15,15.15,0,0,1-1.14-3.49A14.59,14.59,0,0,1,4.24,3.47l.38-.77a2.15,2.15,0,0,1,3.44-.56l.7.7c5.53,5.81,10.49,8.56,19.06,10.44a15.17,15.17,0,0,1,4.1-8.75A14.39,14.39,0,0,1,42.19,0h0c2.84,0,6.36,1.62,8.49,2.77,1.83-.6,4-1.53,6.32-2.51a2.88,2.88,0,0,1,3.22.57,2.85,2.85,0,0,1,.62,3.11c-.17.47-.36.92-.57,1.36a3.07,3.07,0,0,1,.84.58,3.13,3.13,0,0,1,.78,2.92l0,.1a11.92,11.92,0,0,1-4.78,6.56C56.73,35.23,41.84,51.19,23.59,51.19Z"></path></g></g></svg></a><a href="https://www.linkedin.com/company/sama-ai/" class="footer_social_icon__wI2OK" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 59.71 60.79"><title>linkedin</title><g id="bd073dff-b6ea-4cf0-bfeb-c3ce392ee6a5" data-name="Layer 2"><g id="f6b6eb77-7c10-4e27-a3d0-6f8b14e97f2e" data-name="Layer 1"><path d="M59.65,60.79l-12.35,0,0-19.36c0-4.62-.07-10.56-6.41-10.58s-7.44,5-7.45,10.21l0,19.7-12.36,0,.09-40.95,11.87,0v6.57h.16c1.66-3.13,5.7-6.42,11.73-6.41,12.51,0,14.81,8.28,14.79,19l-.05,21.85Z"></path><path d="M7.17,14.35a7.18,7.18,0,1,1,7.18-7.18A7.17,7.17,0,0,1,7.17,14.35Z"></path><rect x="0.98" y="19.8" width="12.39" height="40.95"></rect></g></g></svg></a><a href="https://www.youtube.com/c/SamaAI" class="footer_social_icon__wI2OK" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 65 44.63"><title>youtube</title><g id="b03ac260-8029-40d4-832d-1f2d757db2d5" data-name="Layer 2"><g id="eca1bfe4-f0ec-4c24-9169-be6d2f8b24b5" data-name="Layer 1"><path d="M55,0H10A10,10,0,0,0,0,10V34.63a10,10,0,0,0,10,10H55a10,10,0,0,0,10-10V10A10,10,0,0,0,55,0ZM40.89,24.41,28.3,31.18a2.31,2.31,0,0,1-3.41-2V15.48a2.3,2.3,0,0,1,3.42-2l12.6,6.89a2.31,2.31,0,0,1,0,4.06Z"></path></g></g></svg></a></div></div></div><div class="footer_lower__1z3Av"><div class="footer_lower_left__141hE"><a class="footer_nav_link__X1RNI" href="/terms-of-service">Terms</a><a class="footer_nav_link__X1RNI" href="/privacy-policy">Privacy</a><a class="footer_nav_link__X1RNI" href="/quality-and-information-policy">Quality &amp; Information</a></div><div class="footer_lower_right__22vMw"><h6>Copyright © <!-- -->0<!-- --> Sama Inc.</h6><h6>All rights reserved.</h6></div></div></div></footer></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"config":{"footerNav":{"items":[{"_key":"f255606f8f25","_type":"navDropdownMenu","items":[{"_key":"76389ad94cbb","_type":"navItem","title":"Autonomous Transportation","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"training-data-for-autonomous-driving"}}}},{"_key":"5f64a8d6a69d","_type":"navItem","title":"E-Commerce","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"training-data-for-ecommerce"}}}},{"_key":"f10e54ae04d0","_type":"navItem","title":"AR/VR","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"training-data-for-ar-vr"}}}},{"_key":"fd729b522a77","_type":"navItem","title":"Data Quality","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"data-quality"}}}}],"title":"Guides","url":null},{"_key":"681ef7d8763a","_type":"navDropdownMenu","items":[{"_key":"6238a422b667","_type":"navItem","title":"Our Story","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"our-story"}}}},{"_key":"258985d6d46b","_type":"navItem","title":"Our Team","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"our-team"}}}},{"_key":"e0a76077324a","_type":"navItem","title":"Our Mission","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"mission-vision-values"}}}},{"_key":"239e49661b0d","_type":"navItem","title":"Careers","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"careers"}}}},{"_key":"e005a740cd80","_type":"navItem","title":"Contact","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"company-contact"}}}}],"title":"Company","url":null}]},"logo":{"asset":{"_createdAt":"2021-12-09T21:42:35Z","_id":"image-4f4e2f86a8fad952c02dffffd7008aa35f83c850-2380x636-svg","_rev":"7Z7VDk3xHzg51hvomGzc99","_type":"sanity.imageAsset","_updatedAt":"2021-12-09T21:42:35Z","assetId":"4f4e2f86a8fad952c02dffffd7008aa35f83c850","extension":"svg","metadata":{"_type":"sanity.imageMetadata","blurHash":"D009jvfQfQfQfQfQfQfQfQfQ","dimensions":{"_type":"sanity.imageDimensions","aspectRatio":3.742138364779874,"height":636,"width":2380},"hasAlpha":true,"isOpaque":false,"lqip":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAFCAYAAABFA8wzAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAr0lEQVQYlU3QyUoDQBAE0HeIEjeixAVyESRI3BNFJSZxAU/+/wdJQSEehu6Zqa6uKnjBGz7xgTuc4gAj7LWe4LD9cc8Y+9jBUed84QGrEt/gEleYY9Ylr3jGU/tV77fFLirId0nWBYYgoACi+r3D6YPN0vwFm4VxmNlgfkK4qcLUZdVMcdHB+75FzWPfzzCpxXNcFxuOP2uxnhNLASaX5LjbnJJf6jYG2PpXh812/AvSEQ+GGZqgYgAAAABJRU5ErkJggg==","palette":{"_type":"sanity.imagePalette","darkMuted":{"_type":"sanity.imagePaletteSwatch","background":"#040404","foreground":"#fff","population":100.29,"title":"#fff"},"darkVibrant":{"_type":"sanity.imagePaletteSwatch","background":"#424242","foreground":"#fff","population":0,"title":"#fff"},"dominant":{"_type":"sanity.imagePaletteSwatch","background":"#040404","foreground":"#fff","population":100.29,"title":"#fff"},"lightMuted":{"_type":"sanity.imagePaletteSwatch","background":"#4c4c4c","foreground":"#fff","population":0,"title":"#fff"},"lightVibrant":{"_type":"sanity.imagePaletteSwatch","background":"#bcbcbc","foreground":"#000","population":0,"title":"#fff"},"muted":{"_type":"sanity.imagePaletteSwatch","background":"#4c4c4c","foreground":"#fff","population":0,"title":"#fff"},"vibrant":{"_type":"sanity.imagePaletteSwatch","background":"#7f7f7f","foreground":"#fff","population":0,"title":"#fff"}}},"mimeType":"image/svg+xml","originalFilename":"e20f8cc53e5f74df10ae9a822edb7ec2c4d00f02-2380x636.svg","path":"images/76e3r62u/production/4f4e2f86a8fad952c02dffffd7008aa35f83c850-2380x636.svg","sha1hash":"4f4e2f86a8fad952c02dffffd7008aa35f83c850","size":2009,"uploadId":"jTUF9DIFqAwpLJ0GcI9bRqb17D69QQlN","url":"https://cdn.sanity.io/images/76e3r62u/production/4f4e2f86a8fad952c02dffffd7008aa35f83c850-2380x636.svg"}},"mainNav":{"items":[{"_key":"58c18e9aa9ea","_type":"navDropdownMenu","items":[{"_key":"b5b5b8bee78b","_type":"navCat","items":[{"_key":"0e80156a2f1a","_type":"navItem","title":"How it Works","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"how-it-works"}}}},{"_key":"40bacee029b4","_type":"navItem","title":"Video Annotation","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"video-annotation"}}}},{"_key":"32650ef07503","_type":"navItem","title":"Image Annotation","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"image-annotation"}}}},{"_key":"fe9137cd0167","_type":"navItem","title":"3D \u0026 LiDAR Annotation","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"3d-lidar"}}}},{"_key":"d9a1316d400a","_type":"navItem","title":"Natural Language Processing","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"natural-language-processing"}}}},{"_key":"ac12c7c5d70a","_type":"navItem","title":"Data Curation (Beta)","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"data-curation"}}}}],"title":"Platform","url":null},{"_key":"37ff4fa913bd","_type":"navCat","items":[{"_key":"6026b1a9314e","_type":"navItem","title":"Semantic Segmentation","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"semantic-segmentation"}}}},{"_key":"f4611b19b406","_type":"navItem","title":"Polygons","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"polygons"}}}},{"_key":"5155d874d6c8","_type":"navItem","title":"Bounding Boxes","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"bounding-boxes"}}}},{"_key":"9ef3c1e21e74","_type":"navItem","title":"Key Points","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"key-points"}}}},{"_key":"314d4c00d351","_type":"navItem","title":"Cuboids","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"cuboids"}}}},{"_key":"8e17a6388d74","_type":"navItem","title":"Lines \u0026 Arrows","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"lines-and-arrows"}}}}],"title":"Shapes","url":null}],"title":"Platform","url":null},{"_key":"112867ca4d03","_type":"navDropdownMenu","items":[{"_key":"22699c7e06cb","_type":"navItem","items":null,"title":"Transportation \u0026 Navigation","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"transportation-navigation"}}}},{"_key":"122ae5928d6d","_type":"navItem","items":null,"title":"Retail \u0026 E-Commerce","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"retail-ecommerce"}}}},{"_key":"7bb234b69fb0","_type":"navItem","items":null,"title":"Consumer \u0026 Media","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"consumer-media"}}}},{"_key":"33e6a886b39d","_type":"navItem","items":null,"title":"Biotech \u0026 Medtech","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"biotech-medtech"}}}},{"_key":"d095b2619c4e","_type":"navItem","items":null,"title":"Robotics \u0026 Manufacturing","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"robotics-and-manufacturing"}}}},{"_key":"2c4b82a94d79","_type":"navItem","items":null,"title":"Food \u0026 Agriculture","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"training-data-food-agriculture"}}}}],"title":"Industries","url":null},{"_key":"c47e8763a906","_type":"navDropdownMenu","items":[{"_key":"1d563df30b3f","_type":"navItem","items":null,"title":"Quality","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"quality-training-data"}}}},{"_key":"041725f35d96","_type":"navItem","items":null,"title":"Security","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"security-and-trust"}}}},{"_key":"fd64ede25798","_type":"navItem","items":null,"title":"Ethical AI","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"our-impact"}}}},{"_key":"398dcbb1c95d","_type":"navItem","items":null,"title":"Compare","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"compare"}}}},{"_key":"93bdfdd87879","_type":"navItem","items":null,"title":"Partners","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"partners"}}}}],"title":"Why Sama","url":null},{"_key":"1d38bf63df54","_type":"navDropdownMenu","items":[{"_key":"be81659b38a5","_type":"navItem","items":null,"title":"API Documentation","url":{"_type":"link","externalUrl":"https://docs.sama.com/reference/overview","internalLink":null}},{"_key":"2cec80e94962","_type":"navItem","items":null,"title":"Blog","url":{"_type":"link","internalLink":null,"internalLink_custom":"/blog"}},{"_key":"09e284fcb1d3","_type":"navItem","items":null,"title":"Events","url":{"_type":"link","internalLink":null,"internalLink_custom":"/events"}}],"title":"Resources","url":null},{"_key":"dbee93713c19","_type":"navDropdownMenu","items":[{"_key":"12d594a568bf","_type":"navItem","items":null,"title":"Our Story","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"our-story"}}}},{"_key":"ce36540a102d","_type":"navItem","items":null,"title":"Our Team","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"our-team"}}}},{"_key":"34fc328e8022","_type":"navItem","items":null,"title":"Careers","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"careers"}}}},{"_key":"c1fe2961020a","_type":"navItem","items":null,"title":"Contact","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"company-contact"}}}},{"_key":"ebd81873e538","_type":"navItem","items":null,"title":"Press","url":{"_type":"link","internalLink":null,"internalLink_custom":"/press"}}],"title":"Company","url":null}],"nav_cta":{"_type":"button","link":{"_type":"link","internalLink":{"_ref":"136788cb-06a6-4f27-b75b-07faf403bfa6","_type":"reference"}},"title":"Request a Demo","type":"secondary"}}},"data":{"firstLoad":[{"_createdAt":"2021-03-17T03:58:33Z","author":{"_id":"40a70383-c941-405c-ae5b-f0aba421ee53","avatar":{"_type":"image","asset":{"_ref":"image-f1fd7fbcc4633299cdbedddba22cb44e24f17317-518x518-svg","_type":"reference"}},"bio":"Rafael and Juan are both senior members of Sama R\u0026D where they focus on Data Engineering and Data Science initiatives that help boost operational efficiency and achieve the highest quality.","name":"Rafael Alfaro \u0026 Juan Esquivel","slug":{"_type":"slug","current":"rafael-alfaro-and-juan-esquivel"}},"config":{"description":"In this series of three we’ll go into Experiment Driven Development and A/B Testing. EDD is fact-based development: based on evidence, not intuition.","openGraphImage":null,"title":"Part 3: A/B Testing with Python"},"estimatedReadingTime":6,"featured_image":{"_type":"image","asset":{"_ref":"image-bbf0e002b88fd9c3b2fcbd032f60f114bc93eed0-1980x990-png","_type":"reference"}},"plaintextBody":"We've previously explored the foundations of Experiment Driven Design and A/B Testing. Today we'll dig into A/B Testing with Python because analysis can be easily automated with existing open source python libraries. In this post we will explore their usage with an example. To orient the reader, we will state a few definitions to anchor the example:\n\nControl Group: current user interface.\nExperiment Group: rearranged point annotation button.\n\nH0: The mean of the annotation time for the control group is the same as the mean of the annotation time for the experiment group; there is no effect from rearranging the point annotation button.\n\nH1: The mean of the annotation time for the control group is different from the mean of the annotation time for the experiment group; there is an observable effect from rearranging the point annotation button.\n\nLet us assume that we ran an A/B Test feature experiment for two weeks. The UI modifications consisted of rearranging a button used in the process of drawing polygons around objects Let us assume these were recorded annotation times per image in minutes, for users of each variant (it can be represented as a python list):\n\nA. Control group (original arrangement):\n\nvariant_a = [150, 195, 120, 160, 97, 20, 100, 121, 250, 300, 80, 75, 100, 196, 147, 120, 100, 190, 57, 100, 157, 186, 91, 190, 210, 222, 192, 243, 99, 151]\n\n\n\nB. Experiment group (rearranged button):\n\nvariant_b = [120, 110, 96, 99, 87, 55, 43, 83, 200, 100, 125, 140, 75, 91, 141, 121, 250, 35, 94, 65, 85, 67, 93, 161, 35, 34, 111, 124, 85, 103]\n\n\n\n1. Run the t-test from the scipy.stats module of scipy (a mathematical, scientific and engineering library).\n\n\nimport scipy.stats as stats\n\nt, p = stats.ttest_ind(variant_a, variant_b, equal_var=False)\n\n\n\n2. Calculate the degrees of freedom according to Welch’s t-test definition which is the one implemented in stats.ttest_ind\n\n# For illustrative details see Wikipedia\n\n\ns1 = np.std(variant_a)\ns2 = np.std(variant_b)\nn1 = len(variant_a)\nn2 = len(variant_b)\n\ndf = np.floor(((((s1 ** 2) / n1) + ((s2 ** 2) / n2)) ** 2) /\n(((s1 ** 4) / ((n1 ** 2) * (n1 - 1))) + ((s2 ** 4) / ((n2 ** 2) * (n2 - 1)))))\n\n\n\n3. Now, using the same scipy.stats library, get the t-critical value for 95% or an alpha of 0.05 (1 - confidence level) from the t distribution’s ppf (percent point function) function and evaluate the t statistic from the previous step. If it falls in the range [-t-critical, t-critical] then H0 cannot be rejected, if it is outside, then we can reject H0 in favor of H1:\n\nalpha = 0.05\nt_critical_value = stats.t.ppf(1 - (alpha/2), df)\nnull_hypothesis = bool(t_critical_value \u003e= t_value \u003e= -t_critical_value)\n\n\n\n4. The confidence interval of variant_b (experiment) will help us visualize the difference between the two variants. If the mean of the control group doesn’t fall inside of this interval then the means of the two groups are significantly apart from each other, which suggests that the results are statistically significant.\n\ns = np.std(variant_b)\nx = np.mean(variant_b)\nn = len(variant_b)\nrho = (t_critical_value * s) / np.sqrt(n)\nconf_int = x - rho, x + rho\n\n\n\n\n\n5. Statistical power is the probability that the test correctly rejects the null hypothesis, in other words, the probability of a true positive result. This is only useful when the null hypothesis is rejected. A low value of power could be an indication that the sample size is not big enough yet to validate the results. To calculate the statistical power we use the class TTestIndPower from the module statsmodels.stats.power (https://www.statsmodels.org/stable/stats.html?highlight=power#module-statsmodels.stats.power) of the statsmodel (https://www.statsmodels.org/) library.\n\n\nfrom statsmodels.stats.power import TTestIndPower\n\n# Effect size based on Cohen’s d formula: https://en.wikipedia.org/wiki/Effect_size#Cohen's_d (https://en.wikipedia.org/wiki/Effect_size#Cohen's_d)\n\nx1 = np.mean(variant_a)\nx2 = np.mean(variant_b)\ns1 = np.std(variant_a)\ns2 = np.std(variant_b)\nn1 = len(variant_a)\nn2 = len(variant_b)\n\ns = np.sqrt((((n1 - 1) * (s1 ** 2)) + ((n2 - 1) * (s2 ** 2))) / (n1 + n2 - 2))\neffect = np.abs((x1 - x2) / s)\n\npower = TTestIndPower().power(effect, nobs1=n1, ratio=n2 / n1, df=(n1 + n2 - 2), alpha=alpha)\n\n\n\n6. Plot the sample distributions with confidence intervals as a visual aid using matplotlib library.\n\n\nimport matplotlib.pyplot as plt\n\n# Control\nfig, ax = plt.subplots(figsize=(12,6))\nxA = np.linspace(40, x1 + 3*s1, 100)\nyA = stats.norm(loc=x1, scale=s1).pdf(xA)\nax.plot(xA, yA, c='red', label='Variant A Distribution')\nax.axvline(x=x1, c='red', alpha=0.5, linestyle='--', label='Variant A')\n\n# Experimental\nxB = np.linspace(40, x2 + 3*s2, 100)\nyB = stats.norm(loc=x2, scale=s2).pdf(xB)\nax.plot(xB, yB, c='blue', label='Variant B Distribution')\nax.axvline(x=x2, c='blue', alpha=0.5, linestyle='--', label='Variant B')\n\n# Confidence interval\nax.axvline(conf_int[0], c='green', linestyle='--', alpha=0.5)\nax.axvline(conf_int[1], c='green', linestyle='--', alpha=0.5, label='Confidence Interval')\n\nplt.xlabel('Annotation Time')\nplt.ylabel('Percent of Tasks per Annotation Time')\nplt.title('Annotation Time Distributions')\nplt.legend()\nplt.show()\n\n\n\n","slug":{"_type":"slug","current":"experiment-driven-development-part-3"},"tags":[{"_key":"2iLUnerV","label":"Sama Engineering","value":"Sama Engineering"}],"title":"Part 3: A/B Testing with Python"},{"_createdAt":"2021-03-08T21:41:38Z","author":{"_id":"40a70383-c941-405c-ae5b-f0aba421ee53","avatar":{"_type":"image","asset":{"_ref":"image-f1fd7fbcc4633299cdbedddba22cb44e24f17317-518x518-svg","_type":"reference"}},"bio":"Rafael and Juan are both senior members of Sama R\u0026D where they focus on Data Engineering and Data Science initiatives that help boost operational efficiency and achieve the highest quality.","name":"Rafael Alfaro \u0026 Juan Esquivel","slug":{"_type":"slug","current":"rafael-alfaro-and-juan-esquivel"}},"config":{"description":"In this series of three we’ll go into Experiment Driven Development and A/B Testing. EDD is fact-based development: based on evidence, not intuition.","openGraphImage":null,"title":"Part 2: A/B Testing"},"estimatedReadingTime":4,"featured_image":{"_type":"image","asset":{"_ref":"image-bbf0e002b88fd9c3b2fcbd032f60f114bc93eed0-1980x990-png","_type":"reference"}},"plaintextBody":"After last week's intro into Experiment Driven Development at Sama, we'll go further into A/B Testing today. A/B Testing is a randomized experiment method to compare how two populations behave in a controlled environment and determine whether the variation of some target metrics defined are significant or not, to determine that the experiment yields better results than the alternative.\n\nWe say that a baseline sample (variation A), which normally refers to some existing system, is compared against an experimental treatment (variation B). In software development, samples drawn from the two populations will be used to analyze the metrics associated with the usage of features. We generally want to make sure that any new feature has a positive impact: improved usability, lower duration to finish a process, etc. We should aim to have data to back up our claims that a feature has benefits, otherwise it would be fair to question why we want to deploy a new feature.\n\nOne way of evaluating an A/B experiment is through the use of a t-test which works well when we expect the distribution to be normal and it also allows us to not worry about the unknown standard deviation of our data. Note, however, that population distributions are not always expected to be normal, of course, and the t-test can be replaced by some other appropriate hypothesis test, depending on the distribution of the data, e.g. Kolmogovor-Smirnov. In our case, we want to determine if the means of the two data samples are significantly different from each other, with a given confidence level (90%, 95% and 99% are commonly used values).\n\nThe framework of the experiment revolves around the definition of a hypothesis for an A/B Test as follows:\n\nH0: The mean of the baseline metric is the same as the mean of the experiment metric; there is no effect from the treatment (variation) of the experiment, thus the two means belong to the same population.\nH1: The mean of the baseline metric is different from the mean of the experiment metric; there is an observable effect from the treatment (variation) of the experiment metric and the two means belong to different populations.\n\nWe then define the timeline for the feature/process experiment, run the experiment and collect the observations from the samples of the two variants. We should have a notion in terms of how long we want to run this to collect enough information (we will treat this as out of scope on this piece, however).\n\nIn order to perform the t-test evaluation, we need per sample sizes (N), means (X) and the standard deviations (s) to calculate the statistic t and the degrees of freedom (v). This is calculated as follows (using Welch’s t-test for independent variables with unequal variances and unequal sample sizes):\n\nOnce the statistic and degrees of freedom are calculated, along with an (1-confidence level), we can evaluate our hypothesis against the t-distribution table to determine whether or not the populations are different.\n\nIf our t statistic is less than the value from the table, given the degrees of freedom and the significance level, we can reject the null hypothesis as there is enough evidence to determine that sample means are from different populations. That would mean our new feature is really different from the control and we can release it, if the direction of the variation is in line with our objectives (e.g. lower means when we want to lower durations is good).\n\nThere are other more nuanced circumstances that we can address with a similar framework. For instance, we may want to test two variants of a new feature side by side. We may also want to use a slightly different statistical tool than a t-test, depending on what our interests are. What is important is the test-driven culture that should be fostered within organizations to have a data-drive approach to justify the release of new features.\n\nNext up: A/B Testing with Python.","slug":{"_type":"slug","current":"experiment-driven-development-part-2"},"tags":[{"_key":"FSfjH4D2","label":"Sama Engineering","value":"Sama Engineering"}],"title":"Part 2: A/B Testing"},{"_createdAt":"2021-03-02T05:05:17Z","author":{"_id":"40a70383-c941-405c-ae5b-f0aba421ee53","avatar":{"_type":"image","asset":{"_ref":"image-f1fd7fbcc4633299cdbedddba22cb44e24f17317-518x518-svg","_type":"reference"}},"bio":"Rafael and Juan are both senior members of Sama R\u0026D where they focus on Data Engineering and Data Science initiatives that help boost operational efficiency and achieve the highest quality.","name":"Rafael Alfaro \u0026 Juan Esquivel","slug":{"_type":"slug","current":"rafael-alfaro-and-juan-esquivel"}},"config":{"description":"In this series of three we’ll go into Experiment Driven Development. EDD is fact-based development: based on evidence, not intuition.","openGraphImage":null,"title":"Part 1: Experiment Driven Development"},"estimatedReadingTime":3,"featured_image":{"_type":"image","asset":{"_ref":"image-bbf0e002b88fd9c3b2fcbd032f60f114bc93eed0-1980x990-png","_type":"reference"}},"plaintextBody":"In this series of three we’ll go into Experiment Driven Development and A/B testing. The opposite of developing features based on anecdotes heard in stories from the CEO’s next-door neighbor, EDD seeks proof and is iterative. EDD can be defined as fact-based development: development based on evidence gathered from the field, not intuition.\n\nIn EDD every new feature or process implemented is validated through a formal experiment design process, which looks to test a hypothesis that describes the status quo of the feature. Example hypotheses could range from \"Making a button bigger does not impact clicks\" all the way to \"Making a web app responsive does not increase visitation\" compared. In statistical terms, the base statement is referred to as the null hypothesis (H0), the status quo, and then an alternative hypothesis(H1) is proposed. The null hypothesis will usually state that the change introduced by the experiment will not affect the current behavior while the alternative supports that there is in fact a change.\n\nThe alternative hypothesis is a prediction of what is expected to happen before running the experiment. It can be a bold statement, not an open question and it should have three parts:\n\nThe variable (if we add/change/remove...): the change that the experiment will measure against the current state of the feature/process.\n\nThe desired result (then we expected to see...): what we expect to see after the change is introduced, a qualitative difference between the current state and new state.\n\nThe rationale behind the prediction (because we have seen that...): prior knowledge that has led you to come up with the current hypothesis (from prior observation).\n\nFor example, one can define the pair of hypotheses for a new registration form in a website as:\n\nH0: Changing the registration form from multiple to single page will not impact the current user registration rate.\n\nH1: Changing the registration from multiple to single page will increase the current user registration rate by 5% because we have previously seen that there is a 5% abandon rate on the multi page form format.\n\n\n\nEDD is based on A/B Testing, which is a randomized experiment method to compare two variants of a single variable. In this case, a baseline metric is compared thanks to the definition control (status quo) and treatment (new feature) groups in order to determine if the variation has a significant impact or not. Ideally, most decisions to release a feature would be based on the results given by A/B Tests. At Sama, we want to find viable ideas or fail fast. Instead of developing a monolithic solution and pushing a release, we iterate through experiments, evaluating how features perform and, most importantly, if and how customers use them.\n\nNext up: A/B Testing and A/B Testing with Python.","slug":{"_type":"slug","current":"experiment-driven-development-part-1"},"tags":[{"_key":"sZmqu6EC","label":"Sama Engineering","value":"Sama Engineering"}],"title":"Part 1: Experiment Driven Development"},{"_createdAt":"2021-02-01T20:53:26Z","author":{"_id":"e9e6679d-3ed9-4cd2-a5a5-cfbff31e1057","avatar":{"_type":"image","asset":{"_ref":"image-e33a218a0ab19114b971463e566b1d1cbdb2e6cd-512x512-webp","_type":"reference"}},"bio":"Mathieu is a Senior Software Engineer at Sama. He has been developing software for more than two decades, with a deep passion for DevOps, CI/CD, Infrastructure as Code, Kubernetes and everything Cloud Native. A joyful father of four, Mathieu loves working with people, learning from them, sharing his love of software engineering, coaching and bringing the best out of everyone, including a smile!","name":"Mathieu Frenette","slug":{"_type":"slug","current":"mathieu-frenette"}},"config":{"description":"Introducing Factotum: an MIT-licensed, open source, kubernetes-oriented, general purpose docker container for devs/devops and custom CI/CD pipelines.","openGraphImage":null,"title":"Factotum: Containerizing DevOps Tools for Cloud Native Engineering and CI/CD"},"estimatedReadingTime":4,"featured_image":{"_type":"image","asset":{"_ref":"image-19ef3f125b8ee47727e9f94dc3aff7ed08a4d78c-720x360-png","_type":"reference"}},"plaintextBody":"Our Cloud Journey\n\nLike most companies making the transition to the cloud today, one of the biggest challenges we face in empowering our engineering teams to adopt cloud-native technologies, such as Kubernetes and CI/CD, is the high barrier to entry of simply setting up their local machines with a unified set of tools and configs to interact with those cloud environments. As DevOps, we would ideally like all our colleagues to easily have access to the same tools and environments, and to even extend that to our CI/CD pipelines, which have no reason to run different versions of those tools from our development machines.\n\nContainers are a natural go-to for the job. However, while they're great at encapsulating many tools with specific versions and repeatable, predictable outcomes, containers are designed with remote services and jobs in mind, rather than day-to-day use as a local work environment.\n\nWhen you start to use containers as a daily working tool, you quickly stumble on many roadblocks. You need to master Docker CLI commands and flags to launch your container in various conditions, environment variables, volume mounts and port mappings. The syntax is different depending if you already started your container in the past, or if it's already running and you just want to open a few extra shells into it. What if you need to access multiple kubernetes clusters in parallel, each with its own container? And don't forget that if you modify any config files within your container, those will be discarded the next time you rebuild and launch a fresh copy of your image! Finally, if you want to share the same tools and configs with your colleagues and (gasp!) even your CI/CD pipeline, you're in for an extra ride!\n\nNaively patching up some Dockerfile with your desired tools is not the end of the road. For us, it was just the beginning.\n\n\nThe Process\n\nWe have long tinkered with the idea of packaging up a neat multi-purpose Docker container that would handle most use cases and make it seamless to setup and interact with different Kubernetes clusters. Starting with a few prototypes and then some inspiration from Cloud Posse's Geodesic container—which does a great job of streamlining the installation and launching of the container— our experiments finally evolved into a tool generic and mature enough to be shared with the community!\n\nOur goal has been to share such a tool with the DevOps community and, thanks to the folks at Sama, this vision has become a reality. Let me introduce you to an MIT-licensed, open source, kubernetes-oriented, general purpose docker container for devs/devops and custom CI/CD pipelines, that we affectionately called \"Factotum\" (from Latin, basically meaning an employee who does all kinds of work).\n\nCheck out the source on GitHub\n\nWhat to Know Before You Begin\n\nEven if you can try the vanilla build of Factotum as is from GitHub and Docker Hub, please understand that Factotum is really intended to be customized and made your own in order to leverage its full potential. If you settle to try it, it is worth forking the repo, customizing the Dockerfile and following the instructions in README.md. While that process of setting up your own customized build of Factotum can be rather involved, be assured that using, maintaining, upgrading and sharing it with your teammates afterwards is intended to be as straightforward as possible—it just takes that little initial effort! And, if you encounter any issues or can't figure how to set it up correctly, don't hesitate to file an issue in the GitHub repo and give us feedback.\n\nHappy factotum'ing.","slug":{"_type":"slug","current":"devops-tools-for-cloud-native-engineering"},"tags":[{"_key":"8AQmCtzZ","label":"Sama Engineering","value":"Sama Engineering"},{"_key":"Gl5fCrPJ","label":"Featured","value":"Featured"}],"title":"Factotum: Containerizing DevOps Tools for Cloud Native Engineering and CI/CD"},{"_createdAt":"2021-01-19T17:04:40Z","author":{"_id":"7aaa2439-b7e5-45f9-8aee-1fbf3c9b5fb8","avatar":{"_type":"image","asset":{"_ref":"image-0a1ace27b14b286beed42f8a189b8d4c9f8d5e71-512x512-webp","_type":"reference"}},"bio":"Dimitri is a software developer specializing in computer vision. Currently a machine learning developer at Sama, Dimitri has developed machine learning based products for the entertainment and image processing industries. He holds a Masters degree in Electrical Engineering from McGill University, where he performed research on active object recognition. He is passionate about the use of technology in order to combat income inequality and climate change. Outside of work, he can be found playing beach volleyball or table tennis depending on the time of year.","name":"Dimitri Gallos","slug":{"_type":"slug","current":"dimitri-gallos"}},"config":{"description":"The Sama MLOps Pipeline: At Sama, we decided to build our own automated training pipeline in order to limit costs, and to avoid tying ourselves to a particular cloud provider.","openGraphImage":null,"title":"The Sama MLOps Pipeline: Automating Model Training on the Cloud"},"estimatedReadingTime":3,"featured_image":{"_type":"image","asset":{"_ref":"image-04641081798c41caebebefa44685828d80b7434f-1200x675-png","_type":"reference"}},"plaintextBody":"Training computer vision models are notoriously computationally intensive, often requiring multiple GPUs. It's therefore usually not performed locally. One of the challenges when it comes to launching training jobs on the cloud or private GPU clusters is dealing with all the required manual steps. For example, when using AWS, ML engineers need to spin-up an EC2 instance manually to launch a training job, and then manually decommission it once the training job is completed. \n\nAlthough commercial tools do exist to automate this process (for example, SageMaker or DataBricks), at Sama we decided to build our own automated training pipeline in order to limit costs, and to avoid tying ourselves to a particular cloud provider.\n\nOur pipeline allows our ML Engineers and Scientists to launch a training job on the cloud by simply pushing the code they developed locally to a predefined git branch. This is very simple to achieve using a modern CI/CD platform like Codefresh. A Codefresh “trigger” can be set to track commits to specific git branches of a repository. Once a commit is pushed to a target branch, a Codefresh pipeline is triggered. The pipeline is just a workflow defined in a yaml file that executes the following steps :\n\nClone the tracked git repository, as well as any other repos required (in our case we have a separate repo for all our ML related tools).\n\nBuild a Docker image with all the project specific dependencies.\n\nDeploy the training job on the GPU cluster.\n\nSend a slack message to a channel that tracks all the training jobs whenever the results are ready or if there is an error.\n\nThe specific implementation of step 3 above depends on the particular frameworks and libraries that are used to train the models and track their performance, as well as the cloud provider. In our team, we use Kubernetes to orchestrate the creation and decommission of the instances. We also use mlflow to manage the ML lifecycle, which has built-in Kubernetes deployment support. So for us, this step simply reduces to doing some cloud provider specific configuration and running an mlflow experiment with Kubernetes as the backend.\n\n\n\nSummary of the Sama automated training pipeline: from pushing the code to Github to running the training job on AWS.\n\n\n\nAside from the obvious time savings, the advantage of this setup is that it is fully configurable and can be made to work with any cloud provider, or even a private GPU cluster. As an added bonus, it enforces experiments to be separated in different git commits, which we find is good practice.","slug":{"_type":"slug","current":"part-1-automating-model-training-on-the-cloud"},"tags":[{"_key":"OdrqBpR4","label":"Sama Engineering","value":"Sama Engineering"},{"_key":"Yw0P8Jjj","label":"Training Data","value":"Training Data"},{"_key":"08tVZdgm","label":"MLOps","value":"MLOps"},{"_key":"eMmu9s8R","label":"Featured","value":"Featured"}],"title":"The Sama MLOps Pipeline: Automating Model Training on the Cloud"},{"_createdAt":"2020-12-15T21:43:21Z","author":{"_id":"a3099d34-9595-4978-b517-e508196414c1","avatar":{"_type":"image","asset":{"_ref":"image-6643136d6c33f77b8e49366c166642ca5dafba8d-500x500-webp","_type":"reference"}},"bio":"Frédéric is a researcher and team leader with over 15 years of R\u0026D experience in machine learning, AI, NLP, speech recognition, and computer vision. Currently Head of AI at Sama, he has worked on building ML-based products in multiple industries from Healthcare to Retail, in large companies and startups. He cares about the impact of technology, and outside of work, you can often see him on a bicycle or on skis.","name":"Frederic Ratle","slug":{"_type":"slug","current":"frederic-ratle"}},"config":{"description":"In this article we summarize an approach that we have developed to speed up polygonal instance segmentation using machine learning.","openGraphImage":null,"title":"Fast Vector Annotation with Machine Learning Assisted Annotation"},"estimatedReadingTime":8,"featured_image":{"_type":"image","asset":{"_ref":"image-6abd2f846119ba50a8325787f99ec3aeec68ffff-1076x605-png","_type":"reference"}},"plaintextBody":"At Sama, Vector Annotation of objects using polygons is a task that our expert annotators spend a great deal of time on. This is especially true for projects involving autonomous vehicles, where it is typical to apply instance segmentation to label scenes comprising hundreds of frames, each with multiple objects (vehicles, pedestrians, traffic signs, etc.) like you see in Figure 1.\n\nFigure 1. Example of Polygonal Annotation in Sama.\n\n\n\nIn this post, we summarize an approach that we have developed to speed up polygonal instance segmentation using machine learning. This approach was presented earlier this year at the CVPR Workshop on Scalability in Autonomous Driving, and the ICML Workshop on Human-in-the-Loop Learning.\n\nFew-Click Annotation\n\nBuilding instance segmentation Deep Learning (DL) models for autonomous vehicles requires a significant amount of labeled data. The use of Machine Learning (ML) for producing pre-annotations to be reviewed by human annotators, whether in an interactive setting or as a pre-processing procedure, is a very popular approach for scaling up labeling while controlling the costs.\n\nMultiple approaches have been suggested for machine-assisted instance segmentation. These typically consist of a DL-based segmentation of the object(s) integrated into a human-in-the-loop system. The human can interact with the system by correcting the model output, initializing the model with one or several clicks, or a combination of those steps. Examples of such systems include Polygon-RNN++ [1], DELSE [7], DEXTR [3], and CurveGCN [2]. Those systems all present good results, but some open questions remain:\n\nDo these methods perform well when a production-level accuracy is required, as when working for a customer project?\n\nDoes the choice of annotation tool influence the results? The gains to be made by using ML depend on how difficult it is for humans to draw polygons in the provided UI. Here we used our optimized drawing tool for polygons, which is part of our labeling platform.\n\nML integration is not usually approached from a human-centric perspective. Beyond the optimization of traditional metrics like IoU, what interactions are most desirable and how should we present the output of the model to annotators?\n\nOur method relies on combining the well-known DEXTR [3] approach with a raster-to-polygon algorithm, to make the result more easily editable. This is not unlike what other tools (such as CVAT) have implemented, though we have optimized this approach for our specific use cases using A/B testing.\n\nThe Model\n\nOur instance segmentation model is based on the well-known Deep Extreme Cut (DEXTR) approach [3], along with a raster-to-polygon conversion algorithm that yields high quality polygons whose vertices are sampled in a way that reproduces human drawing patterns. The model uses the few clicks provided by human annotators at inference time. The steps are described in Figure 2.\n\nFigure 2. An overview of the approach.\n\n\n\nRegarding the model itself, we adopted a custom version of the UNet [5] along with an EfficientNet backbone [6] (instead of the ResNet backbone used in the paper).\n\nIn our experience, for human annotators to produce good instance segmentation masks efficiently, a polygon annotation tool should be used. As such, we needed to convert the raster masks produced by our model to high quality polygons. To add to the challenge, humans tend to produce sparse polygons, adding vertices only when necessary. We therefore adopted a raster-to-polygon procedure that minimizes the number of output vertices.\n\nA/B Testing the Approach\n\nAt Sama, we use A/B testing as much as possible to systematically refine and improve our new features. To this end, we have developed a flexible testing infrastructure that can ingest and aggregate data from multiple internal processes and is made available to anyone within the organization.\n\nThis framework measures the statistical impact of proposed changes on our efficiency metrics (such as drawing time or shape adjustment time). The significance of observed differences on a given efficiency metric is evaluated using statistical tests.\n\nToy A/B Tests\n\nWe conducted an A/B test of the method using a synthetic automotive dataset called SYNTHIA-AL [8]. The dataset's images and corresponding annotations were generated from video streams at 25 frames per second (FPS). Figure 3 shows SYNTHIA image examples, along with their segmentation (done manually and with the Few-Click tool).\n\nFigure 3. SYNTHIA example images, along with their manual and semi-automated annotations.\n\n\n\nThe test, applied only to motor vehicles, reproduced realistic annotation guidelines, namely:\n\nThe drawn polygon needs to be within 2 pixels of the edge of the vehicle.\n\nAll vehicles down to 10 pixels (height or width) need to be annotated.\n\nFollowing this test, we found a nearly 3-fold reduction in annotation time. On the other hand, we also found that on some of the more complex shapes, annotators were spending quite some time manually adjusting the ML output. DEXTR's authors originally showed that the segmentation can be improved with additional clicks beyond the four initial ones. We therefore extended our few-click tool to allow online refinement of the polygons by considering modifications to their vertices as extra clicks. At train time we simulated the corrective clicks by considering the point of greatest deviation between predicted mask and ground truth as illustrated in Figure 4.\n\nProblem: DEXTR’s 4 extreme clicks are not always sufficient.\n\nObservation: DEXTR trained on 4 clicks benefits from additional clicks.\n\nSolution: Fine-tune DEXTR model with additional clicks for hard samples as established by IoU at train time.\n\nFigure 4. Integrating additional clicks in the training process.\n\n\n\nUsing this method, annotators are able to re-trigger the model inference with an additional click, instead of manually adjusting the output. We proceeded to a second toy A/B test, and results showed that we could obtain a theoretical efficiency gain of up to 3.5x on vehicles using the improved method.\n\nDownload our paper on Human-Centric Efficiency Improvements in Image Annotation for Autonomous Driving here and stay tuned to hear more about our latest advances!\n\n\n\nReferences\n\nAcuna, D., Ling, H., Kar, A., and Fidler, S. Efficient annotation of segmentation datasets with polygon-rnn++. In CVPR, 2018.\n\nLing, H., Gao, J., Kar, A., Chen, W., and Fidler, S. Fast interactive object annotation with curve-gcn. In CVPR, 2019.\n\nManinis, K.-K., Caelles, S., Pont-Tuset, J., and Van Gool, L. Deep extreme cut: From extreme points to object segmentation. In Computer Vision and Pattern Recognition (CVPR), 2018.\n\nPapadopoulos, D., Uijlings, J., Keller, F., and Ferrari, V. Extreme clicking for efficient object annotation. In ICCV, 2017.\n\nRonneberger, O., Fischer, P., and Brox, T. U-net: Convolutional networks for biomedical image segmentation. CoRR, abs/1505.04597, 2015. URL http://arxiv. org/abs/1505.04597.\n\nTan, M. and Le, Q. V. Efficientnet: Rethinking model scaling for convolutional neural networks. CoRR, abs/1905.11946, 2019. URL http://arxiv.org/ abs/1905.11946.\n\nWang, Z., Acuna, D., Ling, H., Kar, A., and Fidler, S. Object instance annotation with deep extreme level set evolution. In CVPR, 2019.\n\nZolfaghari Bengar, J., Gonzalez-Garcia, A., Villalonga, G., Raducanu, B., Aghdam, H. H., Mozerov, M., Lopez, A. M., and van de Weijer, J. Temporal coherence for active learning in videos. arXiv preprint arXiv:1908.11757, 2019.\n\nThis post was written by Frederic Ratle and Martine Bertrand.","slug":{"_type":"slug","current":"fast-vector-annotation"},"tags":[{"_key":"fJGSmFCx","label":"Vector Annotation","value":"Vector Annotation"},{"_key":"21OOfbwx","label":"Polygons","value":"Polygons"},{"_key":"SPABaoXN","label":"Sama Engineering","value":"Sama Engineering"},{"_key":"O5nmmFbm","label":"Featured","value":"Featured"}],"title":"Fast Vector Annotation with Machine Learning Assisted Annotation"},{"_createdAt":"2020-11-16T23:15:17Z","author":{"_id":"f972de8a-10c1-45e3-97c9-ac490eaceabe","avatar":{"_type":"image","asset":{"_ref":"image-4aa17073cfd70d2e8f7d8ed85325c14cb1519577-692x691-jpg","_type":"reference"}},"bio":"Loic has over 20 years of industry experience in the Cloud services and AI industry. At Sama he works as the VP of Research \u0026 Development. His experience includes Fortune 500 Companies such as Salesforce.com, Unity Technologies, and AT\u0026T where he led the development of large scale AI, data analytics, and cloud solutions. Loic received his MS in computer science from UTBM, France.","name":"Loic Juillard","slug":{"_type":"slug","current":"loic-juillard"}},"config":{"description":"Sama was a partner of the McGill Engineering Hackathon, the largest annual hackathon run by the McGill Electrical, Computer, and Software Engineering StudentÃƒÂ¢Ã¢â€šÂ¬Ã¢â€žÂ¢ Society.","openGraphImage":null,"title":"Code.Jam(2020)-McGill Hackathon: and the winner is A Virtual Fitting Room"},"estimatedReadingTime":3,"featured_image":{"_type":"image","asset":{"_ref":"image-8b71517603a306e0f80ea070e3d0d532f0039105-1024x512-png","_type":"reference"}},"plaintextBody":"For the second consecutive year, Sama was a Terabyte partner of the McGill Engineering Hackathon, the largest annual hackathon run by the McGill Electrical, Computer, and Software Engineering Student’ Society. This as part of our close partnership with McGill University and the broader Montreal Machine learning Technology community.\n\nIn this year defined by COVID-19, the CodeJam team opted for the very fitting “Digital by Default” theme. Staying on topic, we proposed our very own challenge with an “Online Retail and Shopping Smart App”, wherein students would get to interact with a custom fashion segmentation API trained on our iMaterialist open dataset.\n\nThe participation was incredible! Out of 27 teams, 9 tackled our challenge. The submissions were split into two categories:\n\nRecommendation Engine: “I have seen someone wear this, where can I find it?”\n\nThe Virtual Fitting Room: “How would this look on me”\n\nMost recommendation approaches involved extracting one or multiple pieces of clothing using the segmentation API and querying an image repository for similar items. The models were producing good recommendations when the piece of clothing was well defined. Results were less accurate when the quality of the source image segmentation was approximate. A number of factors such as the type of clothes, occlusion (hair, jewelry, etc.), and ambient picture attributes affect segmentation and produce an approximate product match.\n\n\n\nMy Wardrobe\n\nTeam “GradientBoys” took on the task of virtually showing clothes on a subject, a virtual fitting room of sort. They implemented a complex pipeline that involved segmenting the subject picture (person looking to try the clothes), extracting the style of clothing mask, and segmenting the article of clothing from the library. This was followed by clever usage of the OpenPose model for keypoint identification that allowed to extract a set of local and global distortions to modify the new piece of clothing to fit the subject. Completely taking the in-person shopping experience out of the equation, trying on clothes has never been this smart. Perhaps even more impressive considering it was built in less than 36 hours.\n\nAwesome work, team GradientBoys, and thank you McGill, and all of CodeJam Student Execs for organizing this amazing event. Looking forward to the years to come!","slug":{"_type":"slug","current":"codejam-2020-mcgill-hackathon"},"tags":[{"_key":"Fd3ZhNBx","label":"Sama Engineering","value":"Sama Engineering"},{"_key":"3M2wKXav","label":"Events","value":"Events"}],"title":"Code.Jam(2020)-McGill Hackathon: and the winner is A Virtual Fitting Room"},{"_createdAt":"2019-12-20T02:00:00Z","author":{"_id":"97dc2368-fffb-4c41-82aa-5a9cbe2ec670","avatar":{"_type":"image","asset":{"_ref":"image-af58425525bb33d8cffdc1f1b10f02bf1e4faf57-1916x2028-jpg","_type":"reference"}},"bio":"Sharon is the Content Marketing Manager at Sama where she's responsible for telling the story behind the company's impact sourcing mission and human-powered training data solutions. Sharon holds a MS in Integrated Marketing Communications and is passionate about helping social enterprises transform abstract concepts into results-driven marketing.","name":"Sharon L. Hadden","slug":{"_type":"slug","current":"sharon-l-hadden"}},"config":{"description":"In this interview, we chat with Head of AI at Samasource about AI trends to expect in 2020, as well as frequently asked questions about AI and machine learning.","openGraphImage":null,"title":"8 Answers to Your Questions About AI and Machine Learning"},"estimatedReadingTime":18,"featured_image":{"_type":"image","asset":{"_ref":"image-ade4117797bfbff2e90f48d6ad558ab992aca273-3423x2283-jpg","_type":"reference"}},"plaintextBody":"McKinsey Global Institute shared that among the top 5 limitations to adopt AI, two common challenges are labeling training data and obtaining datasets.\n\nIn this interview, Frédéric Ratle, Head of Artificial Intelligence at Sama answers frequently asked questions about AI and machine learning.\n\nRatle also shares AI trends you can expect to see in 2020, how Sama helps enterprise organizations overcome training data challenges, and his thoughts on how AI is shaping the world around us.\n\naudio embed\n\nTranscript: \"Ask Me Anything\" Part 2: 8 Answers to Your Questions About AI and Machine Learning\n\n(00:04) Hello, and welcome to the Ask Me Anything series by Sama, where we interview subject matter experts working in artificial intelligence. I'm your host, Sharon L. Hadden, an AI enthusiast and content marketing manager at Sama.\n\nFrédéric Ratle is the Head of Artificial Intelligence at Sama. He brings 15+ years of R\u0026D experience in machine learning, AI, NLP and computer vision to his role at the company. Frédéric holds a PhD in machine learning, has published numerous research papers and has experience bringing products to market across multiple industries, including healthcare, automotive, consumer electronics and retail.\n\n(00:56) Hi Frédéric. Thanks for joining me today. A little bit about my background, I worked at NVIDIA for a number of years, and it was my first intro into machine learning, deep learning, artificial intelligence. The number one question we were always asked is what's the difference between machine learning and artificial intelligence?\n\n(01:21) It's a really good question. Well, machine learning specifically is concerned with algorithms that can efficiently learn from data. For example, building a classifier that can learn to distinguish, let's say lion images from tiger images based on a set of labeled images is a typical machine learning problem, but it's a subset of AI because AI also includes approaches that aren't data-driven, which I refer to as symbolic approaches. For example, rule-based systems or knowledge or ontology based systems.\n\nThose are AI, but they're not specifically machine learning. Knowing this, the distinction has been a little bit blurred. So we tend to call everything AI. Another difference is that AI implies some kind of goal of mimicking human intelligence, while machine learning clearly is in engineering territory and really aimed at building data-driven decision making systems.\n\n(02:18) That's a great way to put it, Frédéric, and where does deep learning fit into the picture of all of this?\n\n(02:26) Right, so deep learning is a part of machine learning. It's a class of models in machine learning so it's also concerned with algorithms that can learn from data. But the difference with other kinds of models is that it's specifically concerned with so-called deep architectures. These are a model that stack multiple layers of representation and that is believed to lead to the model being able to learn more meaningful features as opposed to traditional shallow models like support vector machines, for example.\n\nIt's a field that has existed for a while actually. People have been trying to use deep neural networks since the 80s, I think, but it only started gaining some traction about 10 years ago, mostly because researchers have found mathematical tricks to optimize those models in an end-to-end way. And also thanks to the increasing availability of computational power, so really cloud computing.\n\n(03:26) Anytime I'm doing research around the history of AI, it's remarkable to see how long we've been using it, but have only just arrived at being able to maximize the technology. Let's talk a little bit about AI versus general artificial intelligence.\n\n(03:46) What we describe as a general artificial intelligence, sometimes is also called strong AI. It's the idea that a machine would be able to be trained and then, it would be able to learn any task that a human can learn.\n\nIt's more of an academic notion really, and I believe we're very far from that because we lack a lot of scientific knowledge about many mechanisms that underlie human intelligence and reasoning. The other kind of AI that you mentioned is the one that's been most successful, in my view. And people in academia typically refer to it as narrow AI.\n\nIt's the ability to really, for machines to mimic your very specific cognitive ability that's normally associated with humans, like speech for example, and in a way that is useful to us. This is mostly on the engineering side and most AI work in industry fall into that category. But that being said, a strong versus narrow AI or AI versus general AI, is really more of an academic debate in my view.\n\nThere's an article that was written last year by professor Michael Jordan from Berkeley, which I really liked, where he presented a framework to categorize different kinds of AI work in a way that is meaningful. I really encourage our listeners to take a look at it. It's called, Artificial Intelligence—The Revolution Hasn't Happened Yet. The three categories that he outlines, there are first what he calls the human imitative AI, which is mostly an academic field of work where people try to build an intelligence that somewhat resembles that of humans.\n\nThe second category is what he calls intelligence augmentation. And this is really an engineering domain that aims at augmenting human intelligence with things like web search, machine translation, and these are things that that really changed the way we interact with information.\n\nAnd the third category is called intelligent infrastructure. So everything around internet of things, sensors—these are basically systems that capture information about the world and try to make intelligent decisions based on that.\n\n(06:08) That really sounds like a crash course on AI, the way you've described it. I would love to know if you have any just examples of technology that isn't artificial intelligence. I know when you were describing kind of the difference between machine learning and AI, you talked a little bit about rule-based systems and I think often in movies media, AI is depicted as you know, anything smart is AI. Could you just share a few examples of technology that isn't AI? Maybe some things that are even commonly mistaken as AI?\n\n(06:49) Actually, I don't really want to single out a particular field because I think in every field there is room for research and development around something that is AI. I do want to point out that many fields of research and many fields of engineering are now called AI because as you say, the media talks a lot about it.\n\nEven practitioners in those domains are starting to talk about AI, but those domains often have a history of their own and most importantly, they have challenges and goals of their own. If you think, for example, the fields like data mining, like operations research, like control theory, even some parts of statistics for example.\n\nI don't think it's necessarily useful to call them out as AI in that sense because we can easily lose sight of what the goal of those disciplines is. For example, if you look at operations research, its goal really is to solve some very specific optimization problems in business. But if you call it an AI, it kind of blurs the notion of what the goal of that field is.\n\n(07:50)Well, thanks. Thanks for laying that out Frédéric. I'd love to talk more about your work at Sama, specific to our training data annotation platform.\n\n(08:02) Humans are much better than machines at recognizing and judging complex situations and use cases. Sama is a human-in-the-loop labeling platform where our customers can upload their data and receive annotations. We really want to make this platform more intuitive and more helpful to AI practitioners, in terms of tools that are available to slice and dice data sets, for example, to sample data sets for labeling. Because sometimes you have huge videos, but it's not exactly clear if you need to label the whole of it or whether you need to apply some smart way of just picking the frames that are important.\n\nWe support many types of annotation formats within that platform, but there are many features that are in the making, in my team, that will soon make their way into production.\n\n(08:57)Thanks for really pointing out that human-to-machine interaction. I think it often gets lost that humans help train AI. So thanks so much for pointing that out. Of the features that you're working on within the platform, how do you see that technology benefiting our customers at Sama?\n\n(09:21) So things like object detection and image segmentation in computer vision should really be an integral part of a data labeling solution. I think that what's really at the top of our team's list, in terms of priority is better quality in terms of annotation, and better efficiency. So that's really what we're looking to achieve. I believe customers can benefit from even better quality of our in-house labeling and also our ability to take on larger sets of data in a matter that is efficient and scalable. It's about being able to scale and also preserving the quality\n\n(10:06) In terms of limitations around AI—I read so much from McKinsey Global Institute, and I think it was last year, they shared a report of the top five limitations to adopt AI. Two of those common challenges are labeling training data and then obtaining datasets. How can Sama help with this? \n\n(10:28) Of course, Sama as a training data provider can of course help with this aspect, more profoundly, as machine learning and AI gain importance in technology, but also generally in our lives, so will the importance of those data sets and more specifically how those data sets are gathered, if you will.\n\nSo I think that beyond the core labeling services, providing expertise on data acquisition and data labeling not only in technical terms, but also from a social and ethical perspective is really essential and will be increasingly important as society evolves, and also as regulation evolves.\n\n(11:13) Well, in your opinion, Frédéric, how is AI shaping the world around us?\n\n(11:20) Oh wow. That's a really good and open question. I have a few ideas about it. Uh, of course, this is only a subset of all of the ways that I think the world will be shaped. But I think it's shaping the world in many ways and whether those ways are positive or negative are really dependent on how we make use of those advances.\n\nSo first from engineering perspective, I think it's really pushing the boundary of many fields that we thought really were more the realm of humans. For example, if we think of speech recognition, machine translation, and also computer vision, advances in AI have rendered those systems very close to human capability.\n\nWhile maybe 10 or 15 years ago that technology really wasn't there yet, and using these systems really felt still cumbersome. So in that sense, I think it's changing the way we interact with technology and the way we interact with information in general. From a social perspective, I think it's pushing a lot of automation and decision making, in ways that can be both good and bad.\n\nI think it can be very good because machines are much better than humans at making very large scale inferences and taking a lot of factors into account when making that decision leading to, you know, high quality decision systems. It can also be very bad, because if we give too much control to these systems without any human oversight, it can be dangerous. And, the nature of the data used to train these models will ultimately determine its performance.\n\nAlso, if you look at a system that's working in a given context and that's been trained in a given context, it may have unpredictable behavior if we slightly changed the context. So that can be also very dangerous.\n\nAnd finally, I think it's of course changing the dynamics of the job market. And the impact that will have on society is I guess in the realm of politics more than technology, but you know, how do we make the greater number of people benefit from the advances that are provided by AI.\n\n(13:40) Frédéric, 2020 is just around the corner and lots of reports are coming out on the state of AI. Are there any trends that you want to call out that we could expect to see in 2020 regarding AI?\n\n(13:58) There are many exciting fields right now that are progressing very, very quickly in AI, but I'd like to call out three things.\n\nI think in the last one or two years there's been a lot of progress in natural language processing. I think what happened in vision, you know, around 2015, 2016 is happening now in NLP because the accuracy of models is increasing quickly. So I think we can expect some more progress in that area in general.\n\nOf course human communication is not only based on words. I think to reach a really human level of understanding of semantics, we'll have to integrate other modalities like nonverbal communication, visual cues, etc. But there's still room for progress, just based on text.\n\nThe second thing I really find very interesting is causality, so the study of cause and effect between different co-variants and explainability, the ability to explain the why it model is making the prediction it's making will keep growing as a topic of interest. It's actually growing very fast right now, and on another level I think that another area that is a really, really interesting is lower resource machine learning.\n\nSo models that can function with energy constraints or size constraints. Because as we grow more conscious about the energetic and environmental impact with machine learning, there's been a couple of articles recently in various newspapers, so models that require less computing will become more popular.\n\nThis is overall a pretty positive trend I think. Not only because it's more environmentally friendly, but also because I think it puts less of an advantage on very large players in the industry that have an unlimited access to computing power.\n\n(16:01) For sure. So, are we talking at all about AI at the edge?\n\n(16:08) Yeah, we're seeing that a lot. On a lot of devices you can see that you have models that you run on the platform, and I think that's something that will only grow because there will also be more regulations pertaining to data exchange.\n\n(16:24) For sure. For sure. Well, is there anything else that stands out to you regarding the state of AI or any challenges with AI adoption?\n\n(16:34) I think there's a couple of things, but just to call out, two of them, you mentioned earlier the data labeling aspect that is difficult for organizations, but I think it goes deeper than that.\n\nI think even the raw data really is an issue because very often in various organizations, depending on their level of technical savviness, the raw data, if available even is often distributed across different departments and various formats, you know—Excel sheets, SQL databases, etc. So it's very difficult to actually take that data and even just do anything with it. I think that's a very big challenge that organizations really need to tackle.\n\nThe second aspect I wanted to call out is also the increasing debate on the use of data in many applications where massive data is collected from users. I think it's a very welcome debate because we're all aware of that regulation has somewhat been lagging behind in that respect, and it has led to a number of problematic situations. But, I think it will be very interesting to see how will the technologies be impacted by all of the new regulations with respect to data ownership and privacy.\n\n(17:50) It's been incredibly eye opening talking with you today, and I think my last question for you is just, what do you love most about working in AI?\n\n(18:02) Most definitely I think it's the ability to constantly work on new problems, and also the ability to really work with really smart and talented people all the time. I also enjoy really shaping a whole new engineering discipline. I think that's very exciting.\n\n\n\nThis interview is the second installment of a new audio blogging series titled, \"Ask Me Anything,\" where Sama interviews subject matter experts working in artificial intelligence.","slug":{"_type":"slug","current":"8-answers-to-your-questions-about-ai-and-machine-learning"},"tags":[{"_key":"Y7bAcGrE","label":"Machine Learning","value":"Machine Learning"},{"_key":"qhgHNaG5","label":"AI","value":"AI"},{"_key":"0PqR2xB7","label":"Best of","value":"Best of"},{"_key":"NoohHt2W","label":"Sama Engineering","value":"Sama Engineering"}],"title":"8 Answers to Your Questions About AI and Machine Learning"},{"_createdAt":"2019-12-02T19:00:00Z","author":{"_id":"1a59f036-e3fe-4f02-9a34-688ce45de143","avatar":{"_type":"image","asset":{"_ref":"image-7d8f236ba010dd4927d0c5a93368bdce1f712843-390x390-webp","_type":"reference"}},"bio":"Currently a Project Manager at Sama, Taylor Rouleau has a passion for ensuring ethical and sustainable practices in tech. After 5 years leading production teams for our customers, Taylor's expertise is applied internally in our Project Management Office. She heads up efforts to maintain our industry-leading data training processes with a special focus on Security \u0026 Compliance.","name":"Taylor Rouleau","slug":{"_type":"slug","current":"taylor-rouleau"}},"config":{"description":"Samasource was proud to sponsor CodeJam 2019, an annual hackathon at McGill University, from November 15 - 17, 2019.","openGraphImage":null,"title":"Highlights from McGill CodeJam 2019"},"estimatedReadingTime":2,"featured_image":{"_type":"image","asset":{"_ref":"image-980fdc38382a1cc5d9c462b19141b595b5d9ea31-974x460-png","_type":"reference"}},"plaintextBody":"Sama was proud to sponsor CodeJam 2019, an annual hackathon at McGill University, from November 15 - 17, 2019.\n\nThe event was organized by the McGill Electrical, Computer and Software Engineering Student Society and gathered 300+ students with diverse skill sets to form teams and spend 36 intense hours collaborating to solve cutting-edge problems with code.\n\n\n\nPHOTO: Sama President and COO Wendy Gonzalez (left) and Loic Juillard, VP of Engineering presenting at McGill University CodeJam 2019\n\n\n\nTo kick off the weekend, event sponsors presented on their organization's purpose and shared details of their Sponsor Challenge. Sama’s challenge focused on edge detection, a process which allows the major features and events in images to be automatically cataloged.\n\nSama VP of Engineering, Loic Juillard explained the challenge of extracting structured information from images, as well as the most common approaches and models (usually deep learning) used for this purpose, during his presentation.\n\n\n\nPHOTO: Example of edge detection\n\n\n\nObject detection and tracking is an important and growing field of computer vision, and to give challenge-takers a head start, Frederic Ratle, Head of Artificial Intelligence at Sama, presented a workshop on machine learning techniques for object detection.\n\n\n\nPHOTO: S Head of AI, Frederic Ratle presenting at CodeJam 2019 in Montreal\n\n\n\nRepresentatives from Sama were onsite all weekend to support teams as mentors and witness the University's ecosystem of social innovation and engineering first-hand.\n\nWith the explosive growth of machine learning, and the rise of Montreal as an AI Hub, Canada was a natural choice as the location of our R\u0026D hub. We look forward to continuing to support and connect with the McGill community.","slug":{"_type":"slug","current":"highlights-from-mcgill-codejam-2019"},"tags":[{"_key":"FYGgyErZ","label":"Sama Engineering","value":"Sama Engineering"},{"_key":"bymHQxr3","label":"Events","value":"Events"}],"title":"Highlights from McGill CodeJam 2019"}],"morePosts":[],"slug":"sama-engineering","tagName":"Sama Engineering","pageConfig":{"title":"Sama Blog | Training Data, AI and Impact Sourcing Insights","description":"From machine learning to training data strategy, the Sama blog covers research, news and other AI trends from thought leaders across the globe."}}},"__N_SSG":true},"page":"/blog/tag/[slug]","query":{"slug":"sama-engineering"},"buildId":"dMo_2owL0njcN1tTH_wi3","isFallback":false,"dynamicIds":[4941,425,3551],"gsp":true,"appGip":true,"scriptLoader":[]}</script></body></html>