{"pageProps":{"config":{"footerNav":{"items":[{"_key":"f255606f8f25","_type":"navDropdownMenu","items":[{"_key":"76389ad94cbb","_type":"navItem","title":"Autonomous Transportation","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"training-data-for-autonomous-driving"}}}},{"_key":"5f64a8d6a69d","_type":"navItem","title":"E-Commerce","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"training-data-for-ecommerce"}}}},{"_key":"f10e54ae04d0","_type":"navItem","title":"AR/VR","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"training-data-for-ar-vr"}}}},{"_key":"fd729b522a77","_type":"navItem","title":"Data Quality","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"data-quality"}}}}],"title":"Guides","url":null},{"_key":"681ef7d8763a","_type":"navDropdownMenu","items":[{"_key":"6238a422b667","_type":"navItem","title":"Our Story","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"our-story"}}}},{"_key":"258985d6d46b","_type":"navItem","title":"Our Team","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"our-team"}}}},{"_key":"e0a76077324a","_type":"navItem","title":"Our Mission","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"mission-vision-values"}}}},{"_key":"239e49661b0d","_type":"navItem","title":"Careers","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"careers"}}}},{"_key":"e005a740cd80","_type":"navItem","title":"Contact","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"company-contact"}}}}],"title":"Company","url":null}]},"logo":{"asset":{"_createdAt":"2021-12-09T21:42:35Z","_id":"image-4f4e2f86a8fad952c02dffffd7008aa35f83c850-2380x636-svg","_rev":"7Z7VDk3xHzg51hvomGzc99","_type":"sanity.imageAsset","_updatedAt":"2021-12-09T21:42:35Z","assetId":"4f4e2f86a8fad952c02dffffd7008aa35f83c850","extension":"svg","metadata":{"_type":"sanity.imageMetadata","blurHash":"D009jvfQfQfQfQfQfQfQfQfQ","dimensions":{"_type":"sanity.imageDimensions","aspectRatio":3.742138364779874,"height":636,"width":2380},"hasAlpha":true,"isOpaque":false,"lqip":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAFCAYAAABFA8wzAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAr0lEQVQYlU3QyUoDQBAE0HeIEjeixAVyESRI3BNFJSZxAU/+/wdJQSEehu6Zqa6uKnjBGz7xgTuc4gAj7LWe4LD9cc8Y+9jBUed84QGrEt/gEleYY9Ylr3jGU/tV77fFLirId0nWBYYgoACi+r3D6YPN0vwFm4VxmNlgfkK4qcLUZdVMcdHB+75FzWPfzzCpxXNcFxuOP2uxnhNLASaX5LjbnJJf6jYG2PpXh812/AvSEQ+GGZqgYgAAAABJRU5ErkJggg==","palette":{"_type":"sanity.imagePalette","darkMuted":{"_type":"sanity.imagePaletteSwatch","background":"#040404","foreground":"#fff","population":100.29,"title":"#fff"},"darkVibrant":{"_type":"sanity.imagePaletteSwatch","background":"#424242","foreground":"#fff","population":0,"title":"#fff"},"dominant":{"_type":"sanity.imagePaletteSwatch","background":"#040404","foreground":"#fff","population":100.29,"title":"#fff"},"lightMuted":{"_type":"sanity.imagePaletteSwatch","background":"#4c4c4c","foreground":"#fff","population":0,"title":"#fff"},"lightVibrant":{"_type":"sanity.imagePaletteSwatch","background":"#bcbcbc","foreground":"#000","population":0,"title":"#fff"},"muted":{"_type":"sanity.imagePaletteSwatch","background":"#4c4c4c","foreground":"#fff","population":0,"title":"#fff"},"vibrant":{"_type":"sanity.imagePaletteSwatch","background":"#7f7f7f","foreground":"#fff","population":0,"title":"#fff"}}},"mimeType":"image/svg+xml","originalFilename":"e20f8cc53e5f74df10ae9a822edb7ec2c4d00f02-2380x636.svg","path":"images/76e3r62u/production/4f4e2f86a8fad952c02dffffd7008aa35f83c850-2380x636.svg","sha1hash":"4f4e2f86a8fad952c02dffffd7008aa35f83c850","size":2009,"uploadId":"jTUF9DIFqAwpLJ0GcI9bRqb17D69QQlN","url":"https://cdn.sanity.io/images/76e3r62u/production/4f4e2f86a8fad952c02dffffd7008aa35f83c850-2380x636.svg"}},"mainNav":{"items":[{"_key":"58c18e9aa9ea","_type":"navDropdownMenu","items":[{"_key":"b5b5b8bee78b","_type":"navCat","items":[{"_key":"0e80156a2f1a","_type":"navItem","title":"How it Works","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"how-it-works"}}}},{"_key":"40bacee029b4","_type":"navItem","title":"Video Annotation","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"video-annotation"}}}},{"_key":"32650ef07503","_type":"navItem","title":"Image Annotation","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"image-annotation"}}}},{"_key":"fe9137cd0167","_type":"navItem","title":"3D & LiDAR Annotation","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"3d-lidar"}}}},{"_key":"d9a1316d400a","_type":"navItem","title":"Natural Language Processing","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"natural-language-processing"}}}},{"_key":"ac12c7c5d70a","_type":"navItem","title":"Data Curation (Beta)","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"data-curation"}}}}],"title":"Platform","url":null},{"_key":"37ff4fa913bd","_type":"navCat","items":[{"_key":"6026b1a9314e","_type":"navItem","title":"Semantic Segmentation","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"semantic-segmentation"}}}},{"_key":"f4611b19b406","_type":"navItem","title":"Polygons","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"polygons"}}}},{"_key":"5155d874d6c8","_type":"navItem","title":"Bounding Boxes","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"bounding-boxes"}}}},{"_key":"9ef3c1e21e74","_type":"navItem","title":"Key Points","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"key-points"}}}},{"_key":"314d4c00d351","_type":"navItem","title":"Cuboids","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"cuboids"}}}},{"_key":"8e17a6388d74","_type":"navItem","title":"Lines & Arrows","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"lines-and-arrows"}}}}],"title":"Shapes","url":null}],"title":"Platform","url":null},{"_key":"112867ca4d03","_type":"navDropdownMenu","items":[{"_key":"22699c7e06cb","_type":"navItem","items":null,"title":"Transportation & Navigation","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"transportation-navigation"}}}},{"_key":"122ae5928d6d","_type":"navItem","items":null,"title":"Retail & E-Commerce","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"retail-ecommerce"}}}},{"_key":"7bb234b69fb0","_type":"navItem","items":null,"title":"Consumer & Media","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"consumer-media"}}}},{"_key":"33e6a886b39d","_type":"navItem","items":null,"title":"Biotech & Medtech","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"biotech-medtech"}}}},{"_key":"d095b2619c4e","_type":"navItem","items":null,"title":"Robotics & Manufacturing","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"robotics-and-manufacturing"}}}},{"_key":"2c4b82a94d79","_type":"navItem","items":null,"title":"Food & Agriculture","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"training-data-food-agriculture"}}}}],"title":"Industries","url":null},{"_key":"c47e8763a906","_type":"navDropdownMenu","items":[{"_key":"1d563df30b3f","_type":"navItem","items":null,"title":"Quality","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"quality-training-data"}}}},{"_key":"041725f35d96","_type":"navItem","items":null,"title":"Security","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"security-and-trust"}}}},{"_key":"fd64ede25798","_type":"navItem","items":null,"title":"Ethical AI","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"our-impact"}}}},{"_key":"398dcbb1c95d","_type":"navItem","items":null,"title":"Compare","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"compare"}}}},{"_key":"93bdfdd87879","_type":"navItem","items":null,"title":"Partners","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"partners"}}}}],"title":"Why Sama","url":null},{"_key":"1d38bf63df54","_type":"navDropdownMenu","items":[{"_key":"be81659b38a5","_type":"navItem","items":null,"title":"API Documentation","url":{"_type":"link","externalUrl":"https://docs.sama.com/reference/overview","internalLink":null}},{"_key":"2cec80e94962","_type":"navItem","items":null,"title":"Blog","url":{"_type":"link","internalLink":null,"internalLink_custom":"/blog"}},{"_key":"09e284fcb1d3","_type":"navItem","items":null,"title":"Events","url":{"_type":"link","internalLink":null,"internalLink_custom":"/events"}}],"title":"Resources","url":null},{"_key":"dbee93713c19","_type":"navDropdownMenu","items":[{"_key":"12d594a568bf","_type":"navItem","items":null,"title":"Our Story","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"our-story"}}}},{"_key":"ce36540a102d","_type":"navItem","items":null,"title":"Our Team","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"our-team"}}}},{"_key":"34fc328e8022","_type":"navItem","items":null,"title":"Careers","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"careers"}}}},{"_key":"c1fe2961020a","_type":"navItem","items":null,"title":"Contact","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"company-contact"}}}},{"_key":"ebd81873e538","_type":"navItem","items":null,"title":"Press","url":{"_type":"link","internalLink":null,"internalLink_custom":"/press"}}],"title":"Company","url":null}],"nav_cta":{"_type":"button","link":{"_type":"link","internalLink":{"_ref":"136788cb-06a6-4f27-b75b-07faf403bfa6","_type":"reference"}},"title":"Request a Demo","type":"secondary"}}},"data":{"firstLoad":[{"_createdAt":"2020-11-16T23:15:17Z","author":{"_id":"f972de8a-10c1-45e3-97c9-ac490eaceabe","avatar":{"_type":"image","asset":{"_ref":"image-4aa17073cfd70d2e8f7d8ed85325c14cb1519577-692x691-jpg","_type":"reference"}},"bio":"Loic has over 20 years of industry experience in the Cloud services and AI industry. At Sama he works as the VP of Research & Development. His experience includes Fortune 500 Companies such as Salesforce.com, Unity Technologies, and AT&T where he led the development of large scale AI, data analytics, and cloud solutions. Loic received his MS in computer science from UTBM, France.","name":"Loic Juillard","slug":{"_type":"slug","current":"loic-juillard"}},"estimatedReadingTime":3,"featured_image":{"_type":"image","asset":{"_ref":"image-8b71517603a306e0f80ea070e3d0d532f0039105-1024x512-png","_type":"reference"}},"meta_description":"Sama was a partner of the McGill Engineering Hackathon, the largest annual hackathon run by the McGill Electrical, Computer, and Software Engineering StudentÃƒÂ¢Ã¢â€šÂ¬Ã¢â€žÂ¢ Society.","openGraphImage":null,"plaintextBody":"For the second consecutive year, Sama was a Terabyte partner of the McGill Engineering Hackathon, the largest annual hackathon run by the McGill Electrical, Computer, and Software Engineering Student’ Society. This as part of our close partnership with McGill University and the broader Montreal Machine learning Technology community.\n\nIn this year defined by COVID-19, the CodeJam team opted for the very fitting “Digital by Default” theme. Staying on topic, we proposed our very own challenge with an “Online Retail and Shopping Smart App”, wherein students would get to interact with a custom fashion segmentation API trained on our iMaterialist open dataset.\n\nThe participation was incredible! Out of 27 teams, 9 tackled our challenge. The submissions were split into two categories:\n\nRecommendation Engine: “I have seen someone wear this, where can I find it?”\n\nThe Virtual Fitting Room: “How would this look on me”\n\nMost recommendation approaches involved extracting one or multiple pieces of clothing using the segmentation API and querying an image repository for similar items. The models were producing good recommendations when the piece of clothing was well defined. Results were less accurate when the quality of the source image segmentation was approximate. A number of factors such as the type of clothes, occlusion (hair, jewelry, etc.), and ambient picture attributes affect segmentation and produce an approximate product match.\n\n\n\nMy Wardrobe\n\nTeam “GradientBoys” took on the task of virtually showing clothes on a subject, a virtual fitting room of sort. They implemented a complex pipeline that involved segmenting the subject picture (person looking to try the clothes), extracting the style of clothing mask, and segmenting the article of clothing from the library. This was followed by clever usage of the OpenPose model for keypoint identification that allowed to extract a set of local and global distortions to modify the new piece of clothing to fit the subject. Completely taking the in-person shopping experience out of the equation, trying on clothes has never been this smart. Perhaps even more impressive considering it was built in less than 36 hours.\n\nAwesome work, team GradientBoys, and thank you McGill, and all of CodeJam Student Execs for organizing this amazing event. Looking forward to the years to come!","seo_title":"Code.Jam(2020)-McGill Hackathon: and the winner is A Virtual Fitting Room","slug":{"_type":"slug","current":"codejam-2020-mcgill-hackathon"},"tags":[{"_key":"Fd3ZhNBx","label":"Sama Engineering","value":"Sama Engineering"},{"_key":"3M2wKXav","label":"Events","value":"Events"}],"title":"Code.Jam(2020)-McGill Hackathon: and the winner is A Virtual Fitting Room"},{"_createdAt":"2020-02-04T22:15:39Z","author":{"_id":"6dd25391-26af-42ff-a2a2-f0778ce1b002","avatar":{"_type":"image","asset":{"_ref":"image-4e1c47984bb16cb0370855b6272663d6744216d3-450x450-jpg","_type":"reference"}},"bio":"Currently a Senior Product Manager at Sama, Audrey guides cross-functional teams to create thoughtful product solutions. She has guided teams of designers and engineers at HUGE Inc. and NBCUniversal, and monitored user analytics at the Wall Street Journal. With a BA in history from Harvard, an MA in anthropology from Columbia and an MBA from UNC Chapel Hill KFBS, Audrey is passionate a using technology and data analytics facilitate social impact and environmental solutions through technology.","name":"Audrey Boguchwal","slug":{"_type":"slug","current":"audrey-boguchwal"}},"estimatedReadingTime":4,"featured_image":{"_type":"image","asset":{"_ref":"image-df3e88916d772584990f79eaa76fbbf00fb8c830-3872x2592-jpg","_type":"reference"}},"meta_description":"During the REWORK Deep Learning Summit, Sama shared how top organizations obtain secure, high-quality training data, fighting AI bias in the process.","openGraphImage":null,"plaintextBody":"I recently presented a talk at the ReWork Deep Learning Summit titled, “Fighting AI Bias: How to Obtain Secure, High-Quality Training Data,\" but I think it’s equally important this knowledge is also shared outside of the summit.\n\nBias can make its way into your model at any stage of the training data lifecycle, potentially compromising the accuracy and performance of your algorithms. And as more organizations develop their own AI and ML programs, the necessity of superior quality data is even more pertinent.\n\nImpact of Biased Data in Computer Vision\n\nAI bias can creep in at any stage of the training data lifecycle, and bias presents itself most commonly in three categories: dataset bias, training bias and algorithmic bias.\n\nDataset bias is as you might expect—the data does not provide enough information for the model to learn the problem, or it’s unrepresentative of reality in some way. Training bias is the result of poor quality or inconsistent data labeling, and lastly, algorithmic bias occurs when the algorithm itself makes poor predictions or produces poor results.\n\nModels trained on biased data not only produce inaccurate algorithms, they also present ethical, legal and safety problems. And in some cases, biased data in computer vision can perpetuate historical, negative stereotypes across race and gender.\n\nLeft unchecked, algorithms trained on biased data greatly impact the lives of people using the very technologies meant to enhance their everyday experiences.\n\n\n\nCountering Bias in Training Data\n\nCountering bias in training data starts by having an effective training data strategy.\n\nLast year at Embedded Vision Summit, I presented a talk on practical training data strategies to avoid bias, sharing four ways to mitigate unwanted bias in training data. \n\nI want to echo my thoughts here that an effective training data strategy makes for a strong defense against AI bias. Fighting bias in training data means determining your data needs, developing training rules to cover known uses cases, and diversifying data to cover edge cases.\n\nAs your model learns, countering bias means evolving rules and sourcing more data when needed—all while keeping apprised of legal and ethical sourcing considerations.\n\nObtaining Superior Quality Datasets\n\nTop organizations understand that if they want smarter models, they need ethically sourced, quality data (https://www.samasource.com/quality-scale). Your quality requirements might vary, depending on your model, but the fact remains that diverse, high-quality data helps counter AI bias.\n\nFor over a decade, hundreds of organizations, including 25% of the Fortune 50 have relied on Samasource to deliver secure, high-quality training data and model validation for machine learning.\n\nWe’ve helped organizations like Walmart improve their retail item coverage, and others like Vulcan Inc., improve turnaround time to process training datasets. We’ve even partnered with organizations like Cornell Tech to produce an open-source dataset of our own. \n\nHere are a few things to keep in mind when sourcing superior quality datasets:\n\nBe aware of local privacy and property laws as you collect data.\n\nEnsure you have legal user consent for data capture.\n\nStay informed of the security protocols of facilities processing your training data.\n\nWhen possible, stay informed of the working conditions of the workers labeling your data, and support that pay living wages and benefits.\n\nFrom pilots to multi-year projects, Samasource securely trains and validates computer vision and NLP models. We work on a range of use cases ranging from e-commerce to autonomous transportation, manufacturing, navigation, retail, AR/VR, and biotech, and if your goal is to build smarter AI, contact our team to discuss your training data needs. ","seo_title":"Fighting AI Bias by Obtaining High-Quality Training Data","slug":{"_type":"slug","current":"fighting-ai-bias-by-obtaining-high-quality-training-data"},"tags":[{"_key":"JOSMV7ei","label":"Events","value":"Events"},{"_key":"X1b43vej","label":"Training Data","value":"Training Data"},{"_key":"zoYY0bth","label":"Data Quality","value":"Data Quality"},{"_key":"8v2jDpFA","label":"AI Bias","value":"AI Bias"}],"title":"Fighting AI Bias by Obtaining High-Quality Training Data"},{"_createdAt":"2019-12-02T19:00:00Z","author":{"_id":"1a59f036-e3fe-4f02-9a34-688ce45de143","avatar":{"_type":"image","asset":{"_ref":"image-7d8f236ba010dd4927d0c5a93368bdce1f712843-390x390-webp","_type":"reference"}},"bio":"Currently a Project Manager at Sama, Taylor Rouleau has a passion for ensuring ethical and sustainable practices in tech. After 5 years leading production teams for our customers, Taylor's expertise is applied internally in our Project Management Office. She heads up efforts to maintain our industry-leading data training processes with a special focus on Security & Compliance.","name":"Taylor Rouleau","slug":{"_type":"slug","current":"taylor-rouleau"}},"estimatedReadingTime":2,"featured_image":{"_type":"image","asset":{"_ref":"image-980fdc38382a1cc5d9c462b19141b595b5d9ea31-974x460-png","_type":"reference"}},"meta_description":"Samasource was proud to sponsor CodeJam 2019, an annual hackathon at McGill University, from November 15 - 17, 2019.","openGraphImage":null,"plaintextBody":"Sama was proud to sponsor CodeJam 2019, an annual hackathon at McGill University, from November 15 - 17, 2019.\n\nThe event was organized by the McGill Electrical, Computer and Software Engineering Student Society and gathered 300+ students with diverse skill sets to form teams and spend 36 intense hours collaborating to solve cutting-edge problems with code.\n\n\n\nPHOTO: Sama President and COO Wendy Gonzalez (left) and Loic Juillard, VP of Engineering presenting at McGill University CodeJam 2019\n\n\n\nTo kick off the weekend, event sponsors presented on their organization's purpose and shared details of their Sponsor Challenge. Sama’s challenge focused on edge detection, a process which allows the major features and events in images to be automatically cataloged.\n\nSama VP of Engineering, Loic Juillard explained the challenge of extracting structured information from images, as well as the most common approaches and models (usually deep learning) used for this purpose, during his presentation.\n\n\n\nPHOTO: Example of edge detection\n\n\n\nObject detection and tracking is an important and growing field of computer vision, and to give challenge-takers a head start, Frederic Ratle, Head of Artificial Intelligence at Sama, presented a workshop on machine learning techniques for object detection.\n\n\n\nPHOTO: S Head of AI, Frederic Ratle presenting at CodeJam 2019 in Montreal\n\n\n\nRepresentatives from Sama were onsite all weekend to support teams as mentors and witness the University's ecosystem of social innovation and engineering first-hand.\n\nWith the explosive growth of machine learning, and the rise of Montreal as an AI Hub, Canada was a natural choice as the location of our R&D hub. We look forward to continuing to support and connect with the McGill community.","seo_title":"Highlights from McGill CodeJam 2019","slug":{"_type":"slug","current":"highlights-from-mcgill-codejam-2019"},"tags":[{"_key":"FYGgyErZ","label":"Sama Engineering","value":"Sama Engineering"},{"_key":"bymHQxr3","label":"Events","value":"Events"}],"title":"Highlights from McGill CodeJam 2019"},{"_createdAt":"2019-11-08T19:30:00Z","author":{"_id":"26fb3cc4-608d-40e1-bb4d-955bceda232a","avatar":{"_type":"image","asset":{"_ref":"image-f1fd7fbcc4633299cdbedddba22cb44e24f17317-518x518-svg","_type":"reference"}},"bio":"From self-driving cars to smart hardware, Sama fuels AI. Founded over a decade ago, we’re experts in image, video and sensor data annotation and validation for machine learning algorithms in industries including automotive, navigation, AR/VR, biotech, agriculture, manufacturing, and e-commerce. Our staff are driven by a mission to expand opportunity for low-income people through the digital economy, and our social business model has helped over 50,000 people lift themselves out of poverty.","name":"Sama Team","slug":{"_type":"slug","current":"sama-team"}},"estimatedReadingTime":2,"featured_image":{"_type":"image","asset":{"_ref":"image-c66363bf6c32d33afe6180066409aa883cde1821-1500x1000-jpg","_type":"reference"}},"meta_description":"ICCV 2019 provided a welcoming platform for the distribution and discussion of scholarly and technical work in computer vision.","openGraphImage":null,"plaintextBody":"The workshops and presentations during the International Conference on Computer Vision 2019 (ICCV) were a great preview into what’s next in computer vision.\n\nHere are a few highlights from the premier international computer vision event for students, academics and industry researchers.\n\nPHOTO: Sama Team at ICCV 2019\n\nKey Takeaways from ICCV 2019\n\nICCV attendance increased by 150 percent from 2017 to 2019, going from 3,000 attendees to 7,500. Likewise, ICCV received more than 4,000 papers, up by 50 percent since 2017. Everything from convolutional neural networks to neural architecture search, to robust object detection was published. This article highlights the best paper awards received.\n\nOn the exhibit floor, we found varied applications of computer vision, from a pair of cameras detecting a person's face, age, and emotional state to algorithms helping doctors detect breast cancer more easily. There were also some interesting poster presentations, some of which were highlighted in the ICCV Daily 2019 such as, “Align, Attend and Locate: Chest X-ray Diagnosis via Contrast Induced Attention Network with Limited Supervision.”\n\nPHOTO: Exhibitor booth at ICCV 2019\n\nThroughout the conference, opinions ran high around the use of lidar and radar vs camera sensors only for autonomous driving. There were various approaches on display outlining specific use cases for different sensor packages.\n\nPHOTO: Qualcomm exhibitor display at ICCV 2019\n\n\n\nICCV 2019 provided a welcoming platform for the distribution and discussion of scholarly and technical work, and the Sama team looks forward to continuing conversations on how to securely train and validate computer vision.","seo_title":"3 Takeaways from ICCV 2019","slug":{"_type":"slug","current":"3-takeaways-from-iccv-2019"},"tags":[{"_key":"y8AlUYn8","label":"Best of","value":"Best of"},{"_key":"67uTPs7c","label":"Events","value":"Events"}],"title":"3 Takeaways from ICCV 2019"},{"_createdAt":"2019-06-28T21:59:52Z","author":{"_id":"80db3356-b402-44cb-a717-2e0f9c9fa3e4","avatar":{"_type":"image","asset":{"_ref":"image-399295e959574e371fe2ba7bfd55ba4aee8a589d-320x320-jpg","_type":"reference"}},"bio":"Heather is passionate about bringing world-changing technologies to market and using supply chain purchasing power for good. She is a data-driven strategist experienced in developing and leading go-to-market, communications, and sustainability initiatives at start-ups and multi-national organizations. Heather is most happy when she’s growing companies that make a positive impact, enjoying the outdoors, and spending time with her family.","name":"Heather Gadonniex","slug":{"_type":"slug","current":"heather-gadonniex"}},"estimatedReadingTime":3,"featured_image":{"_type":"image","asset":{"_ref":"image-d37fc7fdfe75a66d5f87a4ab2829f5f7c4030be8-4032x3024-jpg","_type":"reference"}},"meta_description":"From facial recognition to AR/VR, computer vision is changing the way we interact with the world around us. Here are some highlights from CVPR 2019.","openGraphImage":null,"plaintextBody":"Last week, the Sama team attended the Computer Vision and Pattern Recognition (CVPR) conference in Long Beach, one of the largest computer vision conferences in the world.\n\nWith a heavy focus on research presentations, the conference was well attended by students, academics, industry researchers and professionals working in the field of computer vision and machine learning.\n\nWe’ve seen explosive growth in the conference since first attending - a strong indication that the field of computer vision is continuing to grow faster than analysts predict. This year, a record 5,160 papers were submitted, 2,451 more than last year. 1,294 papers were accepted, and there were 9,227 registered attendees, a 32.6% increase from 2018.\n\nPHOTO: Heather Gadonniex and Abha Laddha, exploring Sense Photonics (https://sensephotonics.com/) latest solid-state LiDAR technology. The company recently launched and announced a $26M Series A.\n\n\n\nFor us, this was also a breakthrough conference.\n\nWe released our first open source data set for research and development, in collaboration with Cornell Tech. It was used as part of the Sixth Workshop on Fine-Grained Visual Categorization. You can download it for free on Git Hub.\n\nWe constructed our first ever 20x20 booth, built from scratch to ensure our brand values - trust, quality, scale, and impact - were accurately represented. \n\nWe talked with some of the brightest minds working to address long-standing development challenges in areas like health, agriculture and education at the Computer Vision for Global Challenges dinner hosted by Facebook and Partnership on AI.\n\nWe had countless meetings with prospective employees, customers, and those that want to work with Sama to obtain high-quality training data. They were equally impressed by our technology, training data expertise, and our impact mission.\n\n\n\nPHOTO: Douglas Welcome manning the Sama booth, built from recycled and recyclable materials with East African inspired swag that was impact sourced.\n\n\n\nFrom facial recognition to augmented reality, computer vision is changing the way we interact with the world around us. CVPR 2019 was a reflection of that, and the Sama team looks forward to collaborating with the many organizations leading the way.","seo_title":"Highlights from CVPR 2019","slug":{"_type":"slug","current":"highlights-from-cvpr-2019"},"tags":[{"_key":"9fySqjzW","label":"AI","value":"AI"},{"_key":"8IsWZFwF","label":"Events","value":"Events"},{"_key":"AlpYkFWc","label":"Computer Vision","value":"Computer Vision"}],"title":"Highlights from CVPR 2019"},{"_createdAt":"2019-06-10T20:30:00Z","author":{"_id":"26fb3cc4-608d-40e1-bb4d-955bceda232a","avatar":{"_type":"image","asset":{"_ref":"image-f1fd7fbcc4633299cdbedddba22cb44e24f17317-518x518-svg","_type":"reference"}},"bio":"From self-driving cars to smart hardware, Sama fuels AI. Founded over a decade ago, we’re experts in image, video and sensor data annotation and validation for machine learning algorithms in industries including automotive, navigation, AR/VR, biotech, agriculture, manufacturing, and e-commerce. Our staff are driven by a mission to expand opportunity for low-income people through the digital economy, and our social business model has helped over 50,000 people lift themselves out of poverty.","name":"Sama Team","slug":{"_type":"slug","current":"sama-team"}},"estimatedReadingTime":3,"featured_image":{"_type":"image","asset":{"_ref":"image-b8d4bb8d54ed036603c03aa7803647dcd5b0709c-1500x1125-jpg","_type":"reference"}},"meta_description":"The Sama Hackathon in Costa Rica encouraged 56 students to look for ways to use data science to help reduce poverty and income inequality.","openGraphImage":null,"plaintextBody":"Can we create platforms to help those in need overcome their current situation?\n\nThis is the question 53 Computer Engineering students set out to answer during the Sama Hackathon on May 31 - June 1, 2019.\n\nThe Hackathon was held at the Jose Figueres Ferrer Library, Costa Rica Institute of Technology, Main Campus - Cartago and encouraged students to look for ways to use data science to help reduce poverty and inequality.\n\nPHOTO: Computer Engineering students at the Sama Hackathon\n\nCosta Rica has multiple sources of data available for analysis, ranging from census data gathered by the National Institute of Statistics and Census, to the Ministry of Economics.\n\nDuring the Hackathon, participants explored whether we can use those data sources to find ideas and promote initiatives to improve employment rates, development of skills, etc.\n\n13 teams set out to look for historical trends on why regions are more/less developed based on market trends, and the three winning teams were awarded based on their ability to come up with public policy ideas based on that data.\n\n\n\nPHOTO: First place winners of the Sama Hackathon\n\nThe Winning Team\n\nThe first place winners organized exports data from Costa Rica to the largest economies in the world and evaluated the potential for new small businesses to be created, depending on their region.\n\nFor instance, if Germany is importing a lot of flowers (in general) and the fraction coming from Costa Rica is small, then could we identify the regions where there is flower growing potential and promote policies to incentivize farmers to move from their current products to flowers.\n\nPHOTO: Second place winners of the Sama Hackathon\n\nThe Runner-Up\n\nThe second place winners combined multiple data sources; probably the most out of any group. They established multiple relationships and synthesized them into their own data sources.\n\nThis team used the most technology out of all the teams, attempting to implement several of the key tools and frameworks suggested, in order to find overarching patterns in data related to economic development from Costa Rican institutions.\n\nPHOTO: Third place winners of the Sama Hackathon\n\nThird Place Winners\n\nThe team characterized small businesses in Costa Rica using factors from multiple databases, such as vertical, economic geo region, and others. In Costa Rica, about 50% of small businesses don't make it to year three. They analyzed if there were companies that were more or less likely to survive the three year anniversary, as a proxy for success.\n\nThey tried to cluster those businesses in order to determine the characteristics of successful ones, in hopes of finding patterns for new entrepreneurs to follow.\n\n\n\nCongratulations to the winning teams and many thanks to all who participated.\n\nThe Samae Hackathon is just one of the many programs and initiatives Sama facilitates. Among others is our scholarship program and GiveWork challenge. Learn more about our impact programs here.","seo_title":"53 Students Use Data Science to Reduce Poverty and Income Inequality","slug":{"_type":"slug","current":"56-students-use-machine-learning-to-reduce-poverty-and-inequality"},"tags":[{"_key":"3Lg7mrMf","label":"Ethical AI","value":"Ethical AI"},{"_key":"WFav2zTI","label":"Events","value":"Events"},{"_key":"A0q55Xjf","label":"Impact","value":"Impact"}],"title":"53 Students Use Data Science to Reduce Poverty and Income Inequality"},{"_createdAt":"2019-06-07T20:00:00Z","author":{"_id":"97dc2368-fffb-4c41-82aa-5a9cbe2ec670","avatar":{"_type":"image","asset":{"_ref":"image-af58425525bb33d8cffdc1f1b10f02bf1e4faf57-1916x2028-jpg","_type":"reference"}},"bio":"Sharon is the Content Marketing Manager at Sama where she's responsible for telling the story behind the company's impact sourcing mission and human-powered training data solutions. Sharon holds a MS in Integrated Marketing Communications and is passionate about helping social enterprises transform abstract concepts into results-driven marketing.","name":"Sharon L. Hadden","slug":{"_type":"slug","current":"sharon-l-hadden"}},"estimatedReadingTime":3,"featured_image":{"_type":"image","asset":{"_ref":"image-bbc3a22b57173b2dd2c6020fe7af40171efed6a9-2250x1500-jpg","_type":"reference"}},"meta_description":"13 open source datasets for machine learning, including one dataset featured in the Fine-Grained Visual Categorization (FGVC) workshop at CVPR 2019.","openGraphImage":null,"plaintextBody":"tion (FGVC) workshop at CVPR 2019 on June 17.\n\nMachine Learning Datasets\n\nCOVID-19 Open Research Dataset Allen Institute for AI partnered with leading research groups to prepare this research dataset of over 45,000 scholarly articles about COVID-19 and the coronavirus family of viruses.\n\nGoogle Open Images Google AI introduced over 9 million images spanning 6,000 categories--”enough to train a deep neural network from scratch.”\n\nWaymo Open Dataset Waymo released one of the largest, most diverse autonomous driving datasets to date. All you need is a Gmail account, and you can access the dataset.\n\nImageNet If you’re looking for an image database organized according to the WordNet hierarchy, give ImageNet a try.\n\niMaterialist-Fashion Sama and Cornell Tech announced the iMaterialist-Fashion dataset in May 2019, with over 50K clothing images labeled for fine-grained segmentation. The dataset was used in the FGVC workshop at CVPR, co-sponsored by Google AI.\n\nFishnet.AI Working together with Sama, The Nature Conservancy released Fishnet.AI, an AI training dataset for fisheries. This dataset of approximately 35,000 images with an average of 5 bounding boxes per image was collected from on-board monitoring cameras for long line tuna fishing activity in the Western and Central Pacific.\n\nVisual Genome Visual Genome is the product of 9 technology professionals with a goal of connecting structured image concepts to language. \n\nUCI Machine Learning Repository The University of California - Irvine (UCI) maintains 474 datasets as a service to the machine learning community. \n\nPew Research Center Gain access to raw data from survey research via Pew Research Center. An account is required to access their datasets, but registration is easy.\n\nLabelme Use the Labelme Matlab toolbox to access a large dataset of annotated images. \n\nLabelled Faces in the Wild (LFW) Develop your facial recognition application using LFW, a collection of over 13,000 face photographs collected from around the web.\n\nDataset Finders\n\nKaggle Data scientists and machine learners can find and publish datasets on Kaggle, an online community that was acquired by Google in 2017. Kaggle’s master list of datasets boasts a wide range of niche data sources.\n\nAmazon Web Services (AWS) With over 110 datasets and counting, you’ll find a web crawl of billions of web pages, NASA satellite imagery and more on the Registry of Open Data for AWS. If you want to add to the registry, of course there’s an AWS Labs GitHub repository (https://github.com/awslabs/open-data-registry/) for that.\n\nGoogle Dataset Search Google Dataset Search indexes datasets from digital libraries, personal websites and publisher pages, so you can find them when you need them. It’s currently in beta, but the predictive interface makes it easy to see what datasets are available on your selected topic at a glance.\n\nThis is just a small sample of the free, open source datasets that are available for machine learning use cases. If you have a dataset or dataset finder you’d like to add, hit us up and let us know.","seo_title":"13 Open Source Datasets for Machine Learning","slug":{"_type":"slug","current":"11-open-source-datasets-for-machine-learning"},"tags":[{"_key":"h28cARez","label":"Machine Learning","value":"Machine Learning"},{"_key":"dDZvCu9M","label":"Best of","value":"Best of"},{"_key":"WAc2iHgu","label":"Events","value":"Events"},{"_key":"hPEDVzwV","label":"Open Datasets","value":"Open Datasets"}],"title":"13 Open Source Datasets for Machine Learning"},{"_createdAt":"2019-06-05T14:21:10Z","author":{"_id":"6dd25391-26af-42ff-a2a2-f0778ce1b002","avatar":{"_type":"image","asset":{"_ref":"image-4e1c47984bb16cb0370855b6272663d6744216d3-450x450-jpg","_type":"reference"}},"bio":"Currently a Senior Product Manager at Sama, Audrey guides cross-functional teams to create thoughtful product solutions. She has guided teams of designers and engineers at HUGE Inc. and NBCUniversal, and monitored user analytics at the Wall Street Journal. With a BA in history from Harvard, an MA in anthropology from Columbia and an MBA from UNC Chapel Hill KFBS, Audrey is passionate a using technology and data analytics facilitate social impact and environmental solutions through technology.","name":"Audrey Boguchwal","slug":{"_type":"slug","current":"audrey-boguchwal"}},"estimatedReadingTime":5,"featured_image":{"_type":"image","asset":{"_ref":"image-1184784eeedb6464bcb4703dda418de157e6b21c-2250x1500-jpg","_type":"reference"}},"meta_description":"Samasource exhibited and presented at the venerable 2019 Embedded Vision Summit in Santa Clara, California.","openGraphImage":null,"plaintextBody":"Last week, Sama exhibited and presented at the venerable Embedded Vision Summit in Santa Clara, California.\n\nEmbedded Vision Alliance has hosted EBS since 2012, bringing together technology providers across industries who are enabling innovative and practical applications for computer vision.\n\nThis year for the first time, I represented Sama at the talks and presented a technical talk about Training Data Strategy: Avoiding Bias and Legal and Ethical Sourcing Considerations. Take a look at my related blog post for more details. \n\nAI at the Edge\n\nOn the exhibition floor, the hottest topic was AI at the edge and edge computing. AI at the edge brings AI into small, powerful computer chips located directly in devices, such as phones.\n\nWith the computing happening in the device itself, AI devices no longer need to send data back to the cloud. AI at the edge enables remote and distributed applications, such as using a cell phone to diagnose crop diseases on rural farms.\n\nFor Sama, AI at the edge means new, flexible applications for computer vision that might require new training data strategies and solutions.\n\nHigh quality training data annotation continues to be a high priority need for companies developing AI and computer vision. The challenge remains: high quality data at a reasonable cost and timeline.\n\nBlending highly trained workforces with annotation automation and auto-labeling can make significant gains in efficiency. Sama’s expert, experienced workforce combined with our technology continues to be an offering that is well-positioned to help address these problems with real-world data.\n\nPHOTO: Computer vision in action on the showcase floor recognizes Audrey's face\n\nData Capture\n\nAt the Sama booth, we had many great conversations about data capture. Attendees who stopped by our booth also asked questions about semantic segmentation and smarter video interpolation.\n\nThough computer vision has advanced in many ways, data capture remains a challenge - particularly as algorithms become more sophisticated and require bespoke data sourcing.\n\nThat’s one of the many reasons why Sama plans to expand our data capture offering.\n\n\n\nPHOTO: Wendy Gonzales (left), Audrey Boguchwal and Heather Gadonniew at the Sama booth\n\nData Bias\n\nTowards the end of the two-day event, I presented on training data strategy in the Technical Insights II Track.\n\nIn my presentation, I discussed approaches to training data strategy for avoiding data bias and considering legal and ethical sourcing issues. Data can be biased if it does not represent reality accurately: it is missing examples of use cases, or doesn’t have enough examples of use cases (even if a few are present).\n\nBiased data can result in poor algorithm performance if an algorithm simply doesn’t work as designed. Even worse, biased data can cause problems if say, a facial recognition algorithm identifies the wrong person as a criminal, or an autonomous vehicle algorithm fails to detect the presence of a cyclist.\n\nI also discussed legal and ethical sourcing considerations: knowing regional laws and considering privacy and other gray areas when sourcing data. We’ll update this blog post to link to the video of my presentation when EBS posts it. Also be sure to check out my related blog post for more details.\n\nPHOTO: Audrey mid-sentence while presenting on training data strategy\n\n2019 Women in Vision Reception\n\nWe were invited to attend the first Women in Vision Reception ever held at EBS. It was inspiring to hear the exciting work in which other women are involved. We hope to attend future receptions!\n\n\n\nPHOTO: Women in Vision Reception group photo\n\n\nAs always, Sama continuously seeks to be ahead of the training data market. We are always researching the latest technologies, applications and business cases to be able to offer the most comprehensive training data solutions and strategies.\n\nOur time at the Embedded Vision Summit 2019 gave us an inside look into the computer vision industries’ current needs and challenges. We can’t wait to see what’s next!","seo_title":"Highlights from the 2019 Embedded Vision Summit","slug":{"_type":"slug","current":"highlights-from-the-2019-embedded-vision-summit"},"tags":[{"_key":"2Ur4O0La","label":"Best of","value":"Best of"},{"_key":"8FMbOtTY","label":"Events","value":"Events"}],"title":"Highlights from the 2019 Embedded Vision Summit"},{"_createdAt":"2019-05-20T18:30:00Z","author":{"_id":"6dd25391-26af-42ff-a2a2-f0778ce1b002","avatar":{"_type":"image","asset":{"_ref":"image-4e1c47984bb16cb0370855b6272663d6744216d3-450x450-jpg","_type":"reference"}},"bio":"Currently a Senior Product Manager at Sama, Audrey guides cross-functional teams to create thoughtful product solutions. She has guided teams of designers and engineers at HUGE Inc. and NBCUniversal, and monitored user analytics at the Wall Street Journal. With a BA in history from Harvard, an MA in anthropology from Columbia and an MBA from UNC Chapel Hill KFBS, Audrey is passionate a using technology and data analytics facilitate social impact and environmental solutions through technology.","name":"Audrey Boguchwal","slug":{"_type":"slug","current":"audrey-boguchwal"}},"estimatedReadingTime":4,"featured_image":{"_type":"image","asset":{"_ref":"image-f9a4a7493982c6bf1be96e8260df8a98e4467b28-2250x1500-jpg","_type":"reference"}},"meta_description":"During Embedded Vision Summit 2019, Audrey Boguchwal will share four training data strategies that help AI teams avoid training data bias.","openGraphImage":null,"plaintextBody":"According to a new McKinsey Global Survey, adoption of artificial intelligence continues to advance, but the same foundational barriers still remain when trying to create value from AI at scale.\n\nAmong the challenges to successfully adopt AI is bias in data and algorithms. Audrey Boguchwal, Senior Product Manager at Sama shares four practical approaches to training data strategy that can help AI teams avoid the effects of training data bias. \n\n\n\nThe Impact of Biased Data\n\n\nModels trained on biased data can be less accurate, resulting in insufficient training for your algorithm. Recent studies have shown that biased data can result in problems with facial recognition used in identification, surveillance, and law enforcement. Biased data can also perpetuate historical, negative stereotypes across race and gender. \n\nEnsuring that reality is always represented in your data is a constructive way to minimize the impact of data bias, however, a clear training data strategy to legally and ethically source the data AI requires is fundamental for developing smarter models.\n\n\n\nAvoid Data Bias with a Practical Training Data Strategy\n\nClearly articulate your end training goal and know what data is needed to get to it. When you start with the end goal in mind, you're primed to think through the skill set, tool set and milestones needed to achieve your training goal. For example, for object classifiers, your training data strategy might include preprocessing data to capture bias or offset dataset bias. You recognize that images may look similar before you begin data collection and make a plan to consider transformations i.e. flip or automatically crop images so they vary.\n\nMap out ways bias can enter data and proactively source data to avoid it. Keep in mind that humans are inherently biased, so eliminating all forms of bias is near impossible. Rigorously examine your own biases and the biases of those providing data/information to you.\n\nAvoid selection bias by varying search terms and data sources, and avoid negative set bias by varying data i.e. collect data that contains background scenes in addition to objects of interest. Test before and after training, on a wide range of data. If in the end, you find your model to be of low variance and high bias, use cross-validation to influence the degree of flexibility of your model. Methods like cross dataset generalization can also help determine how reliant your model is on the \"native\" dataset, when compared to other representative datasets.\n\nEnsure data represents reality for your training goal in quantity and diversity, and replenish data often. High-tech, automotive, retail-- there isn't a single industry adopting AI that shows signs of stagnating growth. Refresh data often to stay ahead of trends. Use more than one training set, especially if it's a stock set. Success comes from being iterative, source and label new data as the world changes.\n\nAn effective training data strategy can help you determine ways to mitigate unwanted bias. Audrey Boguchwal, Senior Product Manager at Sama will present \"Practical Approaches to Training Data Strategy: Bias, Legal and Ethical Considerations this year at the 2019 Embedded Vision Summit conference.\n\nAudrey's presentation will expand on the four strategies to avoid training data bias in this post by exploring use-cases that show how unintended bias can creep into datasets, sharing tests to detect dataset bias, and outlining legal and ethical data sourcing considerations.\n\nIf you'll be attending Embedded Vision Summit, stop by booth #621 to discuss your training data needs with the Sama team, or click below to request a demo of our cloud-based data annotation platform.","seo_title":"4 Training Data Strategies to Avoid Bias","slug":{"_type":"slug","current":"4-training-data-strategies-to-avoid-bias"},"tags":[{"_key":"79xbZuT2","label":"Events","value":"Events"},{"_key":"hX284VAD","label":"Training Data","value":"Training Data"},{"_key":"VrBfZ9C5","label":"AI Bias","value":"AI Bias"}],"title":"4 Training Data Strategies to Avoid Bias"},{"_createdAt":"2018-09-08T19:40:00Z","author":{"_id":"26fb3cc4-608d-40e1-bb4d-955bceda232a","avatar":{"_type":"image","asset":{"_ref":"image-f1fd7fbcc4633299cdbedddba22cb44e24f17317-518x518-svg","_type":"reference"}},"bio":"From self-driving cars to smart hardware, Sama fuels AI. Founded over a decade ago, we’re experts in image, video and sensor data annotation and validation for machine learning algorithms in industries including automotive, navigation, AR/VR, biotech, agriculture, manufacturing, and e-commerce. Our staff are driven by a mission to expand opportunity for low-income people through the digital economy, and our social business model has helped over 50,000 people lift themselves out of poverty.","name":"Sama Team","slug":{"_type":"slug","current":"sama-team"}},"estimatedReadingTime":8,"featured_image":{"_type":"image","asset":{"_ref":"image-ed0c756ff646aae4c0602af5a501a53f5046d398-1500x1000-jpg","_type":"reference"}},"meta_description":"Coming AI events to attend September - October 2018","openGraphImage":null,"plaintextBody":"With AI increasingly growing, so are the number of events. To strike the balance between the latest insights from respected industry leaders and get accessible, practical machine learning tactics to apply to real use cases today, here’s our list of events to attend.\n\n\n\n1. ECCV 2018 - EUROPEAN CONFERENCE ON COMPUTER VISION\n\n\nThe 15th European Conference on Computer Vision (ECCV) is happening in München, September 8 – September 14th. It’s one of the top conferences in computer vision with premier academic and technical presentations, co-locating a company exhibition. Many professors, researchers, entrepreneurs and technologists share their ideas and research through papers and poster sessions at focused workshops and tutorials. The expo at the Gasteig Cultural Center showcases companies that are pushing the boundaries of what’s possible with computer vision and machine learning technologies today.\n\nNotable sponsors include many of our clients, amongst others, Microsoft, Facebook and Google AI, as well as other automotive, high tech and ecommerce companies highly adopting AI. We’re proud to be sponsor of ECCV, meet us at the exhibition floor, stand no. 30.\n\n\n2. GTC JAPAN\n\n\nGTC Japan is the largest GPU Technology Event for AI development in Japan, hosted by NVIDIA, September 13-14 in Tokyo. They showcase the latest breakthroughs in autonomous vehicles as well as other AI applications, including smart cities, healthcare and virtual reality. As home base of many of the world’s largest car manufacturers, Japan showcases world-leading technology to accelerate the development of self-driving cars.\n\nThere’s a great line up of over 160 interesting speakers. Some of the speeches we look forward to attend, include Osama Itoh (Honda) on predicting performance of pedestrian protection and Hiroaki Mikami (Sony) on Distributed DNN Training with Adaptive Batch-size Control. Next to that, it will be great to visit Ryuji Hamamoto on Analysing medical data applying AI to diagnosis and treatment. And without a doubt, with the advancements in our proprietary technology platform, we look forward to hear from Yukiko Yanagawa (Omron) speaking about Object type recognition by 3D point cloud LiDAR data.\n\nMeet our VP Sales Houk Nolten or Business Development Director Dennis van Herk, or register to set a time upfront to meet our training data experts during the conference.\n\n\n\n3. AUTOSENS BRUSSELS\n\n\nCalled the leading technical summit for ADAS and autonomous vehicle perception technology, AutoSens is the place to be for engineers that go beyond the hype. From 17-20 September, Brussels will be visited by over 500 senior technical experts from the automotive OEMs and Tier 1 suppliers, offering insightful technical presentations focused around the vehicle perception process chain.\n\nLike Robert Stead, Conference Director of Autosens said, ‘I decided to start this event, because I saw a need to through the hype and create a meeting that is truly for the engineers - the guys and girls who are applying their knowledge and solving engineering problems on a daily basis. Autosens is a technical conference, but set in the business context of a complex supply chain. By assembling a representative advisory board, we can get to the heart of the challenges facing the army of engineers crunching the numbers and running the trials that move ideas from the drawing board into production vehicles.’\n\nMeet our Business Development Director Dennis van Herk or Celie Jenkins or get in touch with our Director Advisory Services Mélodie Kinet, or register to set a time upfront to meet our training data experts during the conference.\n\n\n\n4. AI SUMMIT SAN FRANCISCO\n\n\nNow in its 3rd year, The AI Summit is the world’s first and largest conference & exhibition to look at the practical implications of AI for enterprise organizations, the actual solutions that are transforming business productivity. The AI Summit aims to help the business leader, data scientist and engineer successfully implement their AI projects.\n\nWe previously attended the AI Summit in London earlier this year, and will be attending again in San Francisco, September 19 - 20. It will be held at the beautiful Palace of Fine Arts, with over 200 speakers and exhibitors. Make sure to catch our team there at booth 908 to talk training data and all of its applications!\n\n\n\n5. AUTO AI BERLIN\n\n\nWe.Connect hosts Europe’s number 1 event on deep driving & Level +5 automation in Berlin, September 23 – September 25. The event brings together the ADAS scene with all stakeholders who play an active role in the deep driving, imaging, computer vision, sensor fusion and perception and Level +5 automation. The 2-day program provides extensive learning and networking at the Maritim proArte Hotel Berlin.\n\nThe event’s mission is to be an active part of the full automation and self-driving cars reality. You’ll explore how AI, ML & CV will change technology concepts, architectures, software platforms, ADAS, and further technical requirements. Discuss the challenges of AI supported, cognitive cars, discover latest trends in sensor and perception technology and cover the full scope of deep driving in Level 4 & Level 5 from market leaders.\n\nMake sure not to miss the icebreaker on Sunday, September 23, and Auto.AI Evening Dinner to network with peers. Let’s meet at one of the events or come to visit us at our booth no.2.\n\n\n\n6. GTC EUROPE\n\n\n‘Over three thousand attendees, more than 200 speaking sessions and over 80 exhibitors with brilliant demos’, those are among the top reasons you can’t afford to miss GTC Europe 2018 according to NVIDIA. The premier AI Conference, October 9 – 11 in Munich, is the place to discover how deep learning is taking part of NVIDIA’s offerings, meet great people from all over the world and learn from breakthrough work in the fields of artificial intelligence, deep learning, virtual reality, high performance computing, autonomous machines and much more.\n\nThe event host speakers from many of our clients, including NVIDIA, Google and Facebook, as well as many others from the automotive car manufacturing and mapping companies, hightech and ecommerce scene. Like to learn more from 10 years of best practices in training the AI models of companies alike? Meet us at our booth E13 near the Nvidia Square.\n\n\n\n7. WORLD AI\n\n\nAfter a great success last year, Amsterdam will host another World Summit AI at October 10 - 11 for the entire AI ecosystem from Enterprise to Big Tech, Startups, Investors and Science. Doubling the size of the summit expecting 6.000 attendees and over 140 of the brightest brains on stage, the event will tackle burning AI issues for 2018 and beyond. Particularly interesting are the applied solutions for enterprises as well as discussions on AI ethics and AI4good.\n\nWith the Netherlands being Sama stepping stone to Europe last year to train the eyes of self-driving cars, robotics and e-commerce platforms to enhance visual search, we didn’t want to close the list without mentioning the great Dutch AI events. Interested to learn from our best practices to train AI? We’d like to welcome you at our European Head Office in the WTC in the Hague!","seo_title":"AI Events to Attend in Fall of 2018","slug":{"_type":"slug","current":"coming-ai-events-to-attend-september-october-2018"},"tags":[{"_key":"t34DnXsa","label":"AI","value":"AI"},{"_key":"m7M7ftb0","label":"Events","value":"Events"}],"title":"AI Events to Attend in Fall of 2018"},{"_createdAt":"2018-03-20T21:00:00Z","author":{"_id":"54800006-1861-42a5-a4bb-0aa2441aef30","avatar":{"_type":"image","asset":{"_ref":"image-ffd42fb1fe492d2a4a5421d537e379ee2ef7850b-299x299-jpg","_type":"reference"}},"bio":"Steve is a Senior Account Executive for Sama focusing on AI applications for the automotive industry.","name":"Steve Allen","slug":{"_type":"slug","current":"steve-allen"}},"estimatedReadingTime":6,"featured_image":{"_type":"image","asset":{"_ref":"image-200df2138bcc00249eacdf9d29fa4c78fc756b0a-844x1500-jpg","_type":"reference"}},"meta_description":"Steve Allen of Sama share's the key questions he gets asked about LiDAR and Point Cloud Annotation.","openGraphImage":null,"plaintextBody":"Sama was recently at the Auto.AI show in San Francisco and over half of the questions asked there were about LiDAR, point cloud data and 3D imagery. Below, we give you the view from our window.\n\n\nWhat is LiDAR and Why is it Such a Hot Topic?\n\nTo answer the first: LiDAR (Light Detection and Ranging) is a laser-based surveying method that builds up a depth-based image of the world by shining out laser lights and then measuring how long it takes for the reflected pulse to bounce back to the sensor. (Read more about LiDAR technology and its uses in this Techcrunch article.)\n\nTo answer the second question; LiDAR is one of the critical technologies in the development and deployment of autonomous vehicles. It is also a part of the development mix in other hot topics like border security, drone mapping and much more.\n\nBecause of the ability to collect three-dimensional measurements, laser scanning systems are used for surveying the built environment (such as buildings, road networks, and railways) as well as creating digital terrain (DTM) and elevation models (DEMs) of specific landscapes.\n\n\n\nEnvironmental applications also benefit from LiDAR – laser scanning is a popular method of map design, including mapping flood risk, carbon stocks in forestry and monitoring coastal erosion.\n\nIn the field of autonomous driving, LiDAR's strength is being able to visualize a 360 view around the vehicle. The only significant company currently not on board is Tesla, who is using camera and radar only at this point. Their abstention from the technology is also widely talked about. And LiDAR is the technology that Waymo/Google and Uber had gone to court over.\n\n\nKey Challenge and Aspirations Around LiDAR and Point Cloud Annotation\n\nMany companies - Sama included - can annotate 2D images, and recognize that LiDAR and 3D (also called Point Cloud) labeling is rapidly growing need. However, labeling 3D data presents some unique challenges.\n\nPoint Cloud Data labeling challenge 1 - navigating/labeling in a 3D space requires a carefully designed UI. Many companies develop proprietary tools - but then need to find a workforce that can be trained to use it. (Sama can help with that.) We are also in the process of developing our own Point Cloud Annotation tool, including a Point Cloud Viewer, that we can make available to clients.\n\nPoint Cloud Data labeling challenge 2 - depending on the sensor you are using, the resolution and the clarity can be miserable - making it hard to differentiate between objects. To get higher resolution, one solution is to use a more sensitive sensor. The top of the line model of these in existence today can cost between $60,000 to $80,000. These systems are primarily used in test and data collection vehicles and are too expensive for production cars.\n\nThe most common solution to this last challenge is that many companies use multiple, but less expensive LiDAR sensors. (We often see test cars from the major brands driving around our neighborhood in San Francisco. They’re usually set up with multiple sensors to build datasets for testing.)\n\nFrom the conversations we had at Auto.AI, we learned that there is a growing need for a broadly available 3D annotation platform that has:\n\nLabeling UI that is intuitive enough for a non-data scientist to use.\n\nFlexibility to be used for more than one customer/industry\n\nAccess to skilled workers who can be trained to provide high quality and consistent annotation (did we mention we help with that?)\n\nThe ability to work on and transfer large files. Large 3D file sizes can make it challenging to share files over the cloud.\n\nThe ability to work on and transfer sensitive files. There is a growing need to ensure the security and privacy of data gathered.\n\nThe ability to produce a top quality data set - the basis of a top quality algorithm.\n\n\nTrends in LiDAR Adoption\n\nAs we have come to expect with technology, investments lead to scale, which leads to price drops. In the past year, GM's Cruise project and Google spinoff Waymo, among many others, made significant investments in LiDAR technology, which is predicted to lead to an order of magnitude cost reduction.\n\nAlong with price drops, we also see considerable gains in capabilities. Tech leader Velodyne's new VLS-128 sensor set a new record by doubling the number of laser beams on its previous top-of-the-line LiDAR system to a massive 128 while shrinking the overall size of the sensor by 70 percent. Velodyne has announced price cuts on other products as well.\n\nOne trend that Sama can speak to personally is LiDAR equipment manufacturers partnering with annotation and labeling partners to develop a full, affordable solution to companies wishing to put this technology to use on their projects. (We help with that.)\n\nWe learned a lot talking to people at the AutoAI show and we look forward to being part of another lively conversation at the Nvidia GPU show later this month. (If you're going, can we buy you a drink?)\n\nPlease drop us a line if you would like to schedule some time to talk about your plans and aspirations in LiDAR or other data-related areas.\n\n","seo_title":"What's the Latest with Lidar and Point Cloud Annotation?","slug":{"_type":"slug","current":"whats-the-latest-with-lidar-and-point-cloud-annotation"},"tags":[{"_key":"DAOlTUFZ","label":"Events","value":"Events"},{"_key":"8LUsgcYe","label":"Data Annotation","value":"Data Annotation"},{"_key":"CvHVKSNz","label":"LiDAR","value":"LiDAR"}],"title":"What's the Latest with Lidar and Point Cloud Annotation?"},{"_createdAt":"2018-03-15T18:31:04Z","author":{"_id":"71091c91-664a-44a6-9474-acc40eb12457","avatar":{"_type":"image","asset":{"_ref":"image-bc776336801adf71e2599337e8d6f02186b109d0-500x500-jpg","_type":"reference"}},"bio":"Matthew leads the product team at Sama, responsible for the platform that enables Sama's AI/ML data enrichment teams, internal enterprise operations tools to ensure quality and scalability, and all new product initiatives for the evolution of algorithm development and human-powered automation.","name":"Matthew Landry","slug":{"_type":"slug","current":"matthew-landry"}},"estimatedReadingTime":7,"featured_image":{"_type":"image","asset":{"_ref":"image-9296abfc6dc641ff3c4469e7235289ee1aa70b58-1500x1000-jpg","_type":"reference"}},"meta_description":"Last week, Sama visited the Auto.AI event, which bills itself as the platform bringing together the stakeholders who play an active role in the deep driving, computer vision, and sensor fusion.","openGraphImage":null,"plaintextBody":"This week, Sama visited the Auto.AI event, which bills itself as the platform bringing together the stakeholders who play an active role in the deep driving, computer vision, sensor fusion, perception and cognitive vehicles scene.\n\nOr to put it another way: anyone who is anyone in the greater ecosystem for self-driving cars.\n\nIn attendance were leading OEMs (Original Equipment Manufacturers) like GM, Toyota, Hyundai, Ford, Mercedes-Benz, and so on. Also, there were many hardware technology companies developing sensors and cameras, as well as myriad providers of technology to make useful the vast data output from these sensors. (ICYMI - We are in the latter category).\n\n\n\nSee our Account Executive Steve Allen answer questions on camera… no pressure!\n\n\n\nA Keen Interest in Data Annotation\n\nWe spoke to many attendees, from both OEMs and sensor tech companies who were very curious about our offerings in the annotation space. They asked how we could annotate data sets, what kinds of data we could work with, and what makes Sama stand out from other options.\n\nOur differentiators in brief: there is great appreciation for the value of our skilled agents, as we can train them directly during pilot projects, leading to immediate and consistent quality output during the project; they are on staff and work in Sama-operated offices with structured supervision and project management; and our ISO 9001 certification was very appreciated.\n\nAnd the fact that we have a measurable social impact mission adds value to the company’s CSR departments. (Stay tuned for our upcoming blogs where we go into a little more detail on these aspects.)\n\nAutonomous Driving (AD) Tech Trends\n\nIt was notable that there were several speakers talking about the need for car/passenger interaction design. Some examples:\n\nAcknowledgement of the importance of the vehicle's ability to communicate to the passenger what objects the car is noticing and that influence self-driving decisions.\n\nExposing these decision hints visually will help people to gain trust in the AI and understand how it works. There is an intrinsic problem where it's really hard to explain why a neural network makes a given decision -- but this could help.\n\nThe awareness that autonomous vehicles need to take suggestions from the passenger on Spotify stations or travel route preferences, keep track of who is in the car, and have an internal sense of the state of the car (spilled coffee, forgotten phones).\n\nThere were many examples of the research around reading facial cues or what human commands or questions really mean.\n\nWhat this tells me is that the technology is getting to a more mature state where we can put effort into these secondary considerations are nonetheless necessary to making this technology viable and successful in the mainstream.\n\nThe conference ended with a demo from GM of a L2 autonomous system - currently available at a Caddy dealership near you. While not fully autonomous, the vehicle can take control on the highway and proactively vibrate seats and flash lights to keep drivers alert and engaged.\n\nThey also showed demo videos of their L4 autonomous Chevy Bolts fielded by the Cruise Automation subsidiary operating in San Francisco. Though there were passengers in the car who could take control if necessary, it was a compelling proof of concept in a denser, more dangerous environment with bicyclists and pedestrian traffic. GM/Cruise hasn’t officially announced when their automated ride hailing service will launch to the public, but it’s already operating in beta for San Francisco employees! In short, GM is demonstrating pretty rapid rapid development and growth.\n\n\n\nAn overview of some of the players that was shared at the conference. GM is leader of the pack. Navigant Research\n\nA company called Renovo Motors shared their vision of an OS for self-driving taxis - tying together a coalition of technologies around the projected $1 trillion “robotaxi” marketplace. They make bold, exciting predictions that by 2030, 95% of all urban miles will be driven by autonomous taxis.\n\nA trend worth watching - “multitasking networks.” These autonomous driving algorithms are able to run different perception routines in parallel and switch between them as desired for changing environments or scenarios. For example, you might have one algorithm that's controlling the car while you're in clear daylight conditions. A little helper algorithm detects that you’ve driven into a fog bank (as I do every morning when crossing the Golden Gate Bridge!), and the car may switch to a different control algorithm optimized for fog navigation.\n\nCombining this multitasking with interest in “sensor fusion” definitely shows that there's a need for multimodal sensor data and more sophisticated algorithms to operate in transient situations.\n\nSama Commitment to Best Annotation Technology\n\nWe continuously look for the data types that are of interest to our customers. There is so much data needed for 2D and Video annotation technology that we can support. We also heard a lot of interest around Lidar 3D technology. Look for future reports from the field on this blog.\n\nAs head of product for Sama, I can tell you we are always looking to how we can incorporate emerging data sources into our platform to provide rapid, high quality annotation. If there’s one thing the Sama journey has shown, it’s that the benefit of a skilled workforce is their flexibility and adaptability to varied tasks with uncompromising quality.\n\nI think that puts us in a very good position as new sensor data becomes more important for more sophisticated algorithms. I feel very confident we're going to be able to build those workflows into our platform and train up our workforce to deliver.\n\nIf you’d like to learn any more about our capabilities and approaches to supporting your data needs in this area, we would urge you to download our solution brief about our Annotated Data Sets. We look forward to joining you in this adventure unfolding on the the roads of the world.","seo_title":"Takeaways from AutoAI Conference 2018","slug":{"_type":"slug","current":"takeaways-from-autoai-conference"},"tags":[{"_key":"2fZXZfik","label":"AI","value":"AI"},{"_key":"bS0jYOSG","label":"Events","value":"Events"},{"_key":"hKUrqBWv","label":"Autonomous Transportation","value":"Autonomous Transportation"}],"title":"Takeaways from AutoAI Conference 2018"}],"morePosts":[{"_createdAt":"2017-03-21T20:31:18Z","author":{"_id":"ba12f1d5-b083-4eb7-929d-7e1639ef64c5","avatar":{"_type":"image","asset":{"_ref":"image-f1fd7fbcc4633299cdbedddba22cb44e24f17317-518x518-svg","_type":"reference"}},"bio":"Daniele is a Sales Operations and Engineering Senior Manager for Sama working with teams in EMEA and APAC.","name":"Daniele Packard","slug":{"_type":"slug","current":"daniele-packard"}},"estimatedReadingTime":4,"featured_image":{"_type":"image","asset":{"_ref":"image-e7af72154b4cdac45f8526a3934f9ac612e23d37-513x321-jpg","_type":"reference"}},"meta_description":"3 Computer Vision Applications You Haven't Heard Of... Yet","openGraphImage":null,"plaintextBody":"From the looming reality of fully autonomous vehicles to a farmer in Japan using TensorFlow to sort cucumbers, computer vision machine learning is becoming an increasingly accessible and ubiquitous phenomenon. As a service provider in this space, Sama has a unique view into the breadth of computer vision applications. While certain applications of machine learning are prevalent in mainstream media (e.g. autonomous vehicles), there are equally interesting and lesser-known applications that are similarly revolutionary:\n\n\n\n1. Environmental\n\nSatellites orbiting the earth have been collecting comprehensive image data sets of the globe for decades. With recent advancement in computer vision, these images can be efficiently and automatically combed for valuable information. By applying deep learning models to sets of satellite imagery, companies like Orbital Insight can extract data on global surface water levels, informing communities, planners, and policymakers in making critical decisions about water resources.\n\n\n\nFigure 1 San Luis Reservoir. Left: Raw image. Middle: Landsat 8 water mask. Right: Orbital insight water mask. Orbital Insights, 2015\n\n\n2. Biomedical\n\nImage analysis in the medical field is a time-consuming process; using computer vision machine learning to automate portions of this analysis could help make the process more efficient and less costly for the general public. A bottleneck to progress in this field has been the lack of training data, which requires specialists to create (though there’s evidence that specialists can be substituted by pigeons!) and the data is typically confidential. The availability of large datasets with diagnostic labels, such as the Alzheimer’s Disease Neuroimaging Initiative (ADNI), has paved the way for new applications, using computer vision machine learning to provide early diagnosis of Alzheimer’s disease.\n\n\n3.Agricultural\n\nThe advancement of drone and computer vision technology has both lowered the cost of gathering huge sets of aerial imagery, and also increased our ability to intelligently extract information from that imagery. These technologies together can help farmers identify crop diseases or predict crop yields, automating a process where the alternative was manual inspection. In collaboration with Paul Allen towards his Great Elephant Census project, Sama facilitated the creation of a computer vision machine learning model used to track and count elephants as a part of anti-poaching efforts.\n\n\n\nElephant Sanctuary in Kenya, Sama, 2016\n\nUltimately applications of computer vision machine learning are limited only by human imagination. We look forward to seeing what applications this evolving technology will create, and are thrilled to count ourselves as part of this thriving community.\n\nIf you want to learn more about the future of practical computer vision, please join us at the Embedded Vision Summit, being held in Santa Clara from May 1-3. It’s the preeminent event for anyone adding computer vision capabilities to their products. We’ll be participating in the Vision Technology Showcase and invite you to stop by our booth.\n\nThe event is designed to inspire you to use vision technology in new ways and to empower you with the practical know-how you need to integrate vision capabilities into your products. Over three days and four tracks, you’ll meet innovators, luminaries, and colleagues in this fast-growing field.\n\nSama Sales team at Embedded Vision Summit, 2016","seo_title":"3 Computer Vision Applications You Haven't Heard Of...Yet","slug":{"_type":"slug","current":"3-computer-vision-applications-you-havent-heard-of-yet"},"tags":[{"_key":"zGqHljfi","label":"AI","value":"AI"},{"_key":"QcKBgjbl","label":"Events","value":"Events"},{"_key":"mpmDlNYK","label":"Computer Vision","value":"Computer Vision"}],"title":"3 Computer Vision Applications You Haven't Heard Of...Yet"}],"slug":"events","tagName":"Events","pageConfig":{"title":"Sama Blog | Training Data, AI and Impact Sourcing Insights","description":"From machine learning to training data strategy, the Sama blog covers research, news and other AI trends from thought leaders across the globe."}}},"__N_SSG":true}