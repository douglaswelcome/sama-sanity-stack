<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"/><meta name="viewport" content="initial-scale=1.0, width=device-width, viewport-fit=cover"/><title>Sama Blog | Training Data, AI and Impact Sourcing Insights</title><link rel="canonical" href="https://sama.com/blog/tag/data-annotation"/><meta name="description" content="From machine learning to training data strategy, the Sama blog covers research, news and other AI trends from thought leaders across the globe."/><meta property="og:type" content="website"/><meta property="og:description" content="From machine learning to training data strategy, the Sama blog covers research, news and other AI trends from thought leaders across the globe."/><meta property="og:title" content="Sama Blog | Training Data, AI and Impact Sourcing Insights"/><meta property="og:url" content="https://sama.com/blog/tag/data-annotation"/><meta name="twitter:card" content="summary"/><meta property="twitter:description" content="From machine learning to training data strategy, the Sama blog covers research, news and other AI trends from thought leaders across the globe."/><meta property="twitter:title" content="Sama Blog | Training Data, AI and Impact Sourcing Insights"/><meta name="msapplication-TileColor" content="#28282a"/><meta name="theme-color" content="#ffffff"/><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"/><link rel="shortcut icon" href="/favicon.ico"/><meta name="next-head-count" content="17"/><link rel="preload" href="/_next/static/css/bd60e2be2420db639f1f.css" as="style"/><link rel="stylesheet" href="/_next/static/css/bd60e2be2420db639f1f.css" data-n-g=""/><link rel="preload" href="/_next/static/css/bfc9e7bd13d5ad59bf17.css" as="style"/><link rel="stylesheet" href="/_next/static/css/bfc9e7bd13d5ad59bf17.css" data-n-p=""/><link rel="preload" href="/_next/static/css/0d4bd6b9e4f2c8d7d433.css" as="style"/><link rel="stylesheet" href="/_next/static/css/0d4bd6b9e4f2c8d7d433.css"/><link rel="preload" href="/_next/static/css/3b03d00d40d80e105549.css" as="style"/><link rel="stylesheet" href="/_next/static/css/3b03d00d40d80e105549.css"/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-a40ef1678bae11e696dba45124eadd70.js"></script><script defer="" src="/_next/static/chunks/1776.90811ba0e7bd8502bb5d.js"></script><script defer="" src="/_next/static/chunks/4934.37651fffe4e244d39030.js"></script><script defer="" src="/_next/static/chunks/1952.4c2f01af24514bddf484.js"></script><script defer="" src="/_next/static/chunks/3551.489d89a23d509b64ee09.js"></script><script src="/_next/static/chunks/webpack-9615afa105ea2a1b9f2a.js" defer=""></script><script src="/_next/static/chunks/framework-bdc1b4e5e48979e16d36.js" defer=""></script><script src="/_next/static/chunks/main-6409a04df91a58e5134b.js" defer=""></script><script src="/_next/static/chunks/pages/_app-8b37b83a9f5e9f7f90ae.js" defer=""></script><script src="/_next/static/chunks/commons-6a0b6196cd807097f68d.js" defer=""></script><script src="/_next/static/chunks/pages/blog/tag/%5Bslug%5D-03b8b977e5f158c8d5d1.js" defer=""></script><script src="/_next/static/eTLsVxZNIPc5-poyOFiPZ/_buildManifest.js" defer=""></script><script src="/_next/static/eTLsVxZNIPc5-poyOFiPZ/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="container"><header class="header_outer__yu9q7 "><nav class="umoja-l-grid--12 header_wrapper__3Ghzm"><a class="header_logo__eiLSq" href="/"></a><button class="header_hamburger__1ZcbZ" type="button"><span class="header_hamburger_box__RZ7CY"><span class="header_hamburger_box_inner__1PmWZ"></span></span></button><ul class="header_navBar__37eSJ"><li class="header_navItem__1HiGN"><p class="header_navItem_label__fN3Cy">Platform</p><div class="header_dropDown__6NxUb"><div class="header_dropDown_group__2BZXC"><p class="header_dropdown_group_label__tmND8">Platform</p><a class="header_navLink__1ARu5" href="/how-it-works">How it Works</a><a class="header_navLink__1ARu5" href="/video-annotation">Video Annotation</a><a class="header_navLink__1ARu5" href="/image-annotation">Image Annotation</a><a class="header_navLink__1ARu5" href="/3d-lidar">3D &amp; LiDAR Annotation</a><a class="header_navLink__1ARu5" href="/natural-language-processing">Natural Language Processing</a><a class="header_navLink__1ARu5" href="/data-curation">Data Curation (Beta)</a></div><div class="header_dropDown_group__2BZXC"><p class="header_dropdown_group_label__tmND8">Shapes</p><a class="header_navLink__1ARu5" href="/semantic-segmentation">Semantic Segmentation</a><a class="header_navLink__1ARu5" href="/polygons">Polygons</a><a class="header_navLink__1ARu5" href="/bounding-boxes">Bounding Boxes</a><a class="header_navLink__1ARu5" href="/key-points">Key Points</a><a class="header_navLink__1ARu5" href="/cuboids">Cuboids</a><a class="header_navLink__1ARu5" href="/lines-and-arrows">Lines &amp; Arrows</a></div></div></li><li class="header_navItem__1HiGN"><p class="header_navItem_label__fN3Cy">Industries</p><div class="header_dropDown__6NxUb"><a class="header_navLink__1ARu5" href="/transportation-navigation">Transportation &amp; Navigation</a><a class="header_navLink__1ARu5" href="/retail-ecommerce">Retail &amp; E-Commerce</a><a class="header_navLink__1ARu5" href="/consumer-media">Consumer &amp; Media</a><a class="header_navLink__1ARu5" href="/biotech-medtech">Biotech &amp; Medtech</a><a class="header_navLink__1ARu5" href="/robotics-and-manufacturing">Robotics &amp; Manufacturing</a><a class="header_navLink__1ARu5" href="/training-data-food-agriculture">Food &amp; Agriculture</a></div></li><li class="header_navItem__1HiGN"><p class="header_navItem_label__fN3Cy">Why Sama</p><div class="header_dropDown__6NxUb"><a class="header_navLink__1ARu5" href="/quality-training-data">Quality</a><a class="header_navLink__1ARu5" href="/security-and-trust">Security</a><a class="header_navLink__1ARu5" href="/our-impact">Ethical AI</a><a class="header_navLink__1ARu5" href="/compare">Compare</a><a class="header_navLink__1ARu5" href="/partners">Partners</a></div></li><li class="header_navItem__1HiGN"><p class="header_navItem_label__fN3Cy">Resources</p><div class="header_dropDown__6NxUb"><a href="https://docs.sama.com/reference/overview" class="header_navLink__1ARu5" target="_blank">API Documentation</a><a class="header_navLink__1ARu5" href="/blog">Blog</a><a class="header_navLink__1ARu5" href="/events">Events</a></div></li><li class="header_navItem__1HiGN"><p class="header_navItem_label__fN3Cy">Company</p><div class="header_dropDown__6NxUb"><a class="header_navLink__1ARu5" href="/our-story">Our Story</a><a class="header_navLink__1ARu5" href="/our-team">Our Team</a><a class="header_navLink__1ARu5" href="/careers">Careers</a><a class="header_navLink__1ARu5" href="/company-contact">Contact</a><a class="header_navLink__1ARu5" href="/press">Press</a></div></li></ul><div class="header_cta__3J8I7"><a class="button_wrapper__3lRbv button__secondary__1pZ5q button__small__2kIwW" href="/[object%20Object]"><button class="button_btn__1qxP1"><h3 class="button_text__3_sCS">Request a Demo</h3></button></a></div></nav></header><main class="content"><section class="umoja-l-grid-section umoja-u-bg--white"><div class="umoja-l-grid--12"><div class="blog-tag_name__-rmiA"><h1>Data Annotation</h1></div></div></section><section class="umoja-l-grid-section umoja-u-bg--white"><div class="umoja-l-grid--12 umoja-l-grid-gap--row-1"><div class="blog-post_postCard__i0O-O"><a class="blog-post_postCard_image__3k8eA" href="/[object%20Object]"><div style="display:block;overflow:hidden;position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;margin:0"><img alt="" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="fill" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover"/><noscript><img alt="" sizes="100vw" srcSet="https://cdn.sanity.io/images/76e3r62u/production/65b7fac1b60586a0a7b9ff75006684e2c2467f1e-1800x900.png?w=640&amp;q=75&amp;fit=clip&amp;auto=format 640w, https://cdn.sanity.io/images/76e3r62u/production/65b7fac1b60586a0a7b9ff75006684e2c2467f1e-1800x900.png?w=750&amp;q=75&amp;fit=clip&amp;auto=format 750w, https://cdn.sanity.io/images/76e3r62u/production/65b7fac1b60586a0a7b9ff75006684e2c2467f1e-1800x900.png?w=828&amp;q=75&amp;fit=clip&amp;auto=format 828w, https://cdn.sanity.io/images/76e3r62u/production/65b7fac1b60586a0a7b9ff75006684e2c2467f1e-1800x900.png?w=1080&amp;q=75&amp;fit=clip&amp;auto=format 1080w, https://cdn.sanity.io/images/76e3r62u/production/65b7fac1b60586a0a7b9ff75006684e2c2467f1e-1800x900.png?w=1200&amp;q=75&amp;fit=clip&amp;auto=format 1200w, https://cdn.sanity.io/images/76e3r62u/production/65b7fac1b60586a0a7b9ff75006684e2c2467f1e-1800x900.png?w=1920&amp;q=75&amp;fit=clip&amp;auto=format 1920w, https://cdn.sanity.io/images/76e3r62u/production/65b7fac1b60586a0a7b9ff75006684e2c2467f1e-1800x900.png?w=2048&amp;q=75&amp;fit=clip&amp;auto=format 2048w, https://cdn.sanity.io/images/76e3r62u/production/65b7fac1b60586a0a7b9ff75006684e2c2467f1e-1800x900.png?w=3840&amp;q=75&amp;fit=clip&amp;auto=format 3840w" src="https://cdn.sanity.io/images/76e3r62u/production/65b7fac1b60586a0a7b9ff75006684e2c2467f1e-1800x900.png?w=3840&amp;q=75&amp;fit=clip&amp;auto=format" decoding="async" data-nimg="fill" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover" loading="lazy"/></noscript></div></a><div class="blog-post_postCard_data__2vfn3"><a href="/[object%20Object]"><h3>ML Assisted Annotation Powered by MicroModels</h3></a><a class="blog-post_postCard_author__Y7RjO" href="/[object%20Object]">Saul Miller</a><p class="blog-post_postCard_date__hrDMA ">July 29, 2021<!-- --> | 7 Min Read</p></div></div><div class="blog-post_postCard__i0O-O"><a class="blog-post_postCard_image__3k8eA" href="/[object%20Object]"><div style="display:block;overflow:hidden;position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;margin:0"><img alt="" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="fill" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover"/><noscript><img alt="" sizes="100vw" srcSet="https://cdn.sanity.io/images/76e3r62u/production/2be5ee7e7ae0847f3bedec01bb88266c371e3eb9-4000x2250.jpg?w=640&amp;q=75&amp;fit=clip&amp;auto=format 640w, https://cdn.sanity.io/images/76e3r62u/production/2be5ee7e7ae0847f3bedec01bb88266c371e3eb9-4000x2250.jpg?w=750&amp;q=75&amp;fit=clip&amp;auto=format 750w, https://cdn.sanity.io/images/76e3r62u/production/2be5ee7e7ae0847f3bedec01bb88266c371e3eb9-4000x2250.jpg?w=828&amp;q=75&amp;fit=clip&amp;auto=format 828w, https://cdn.sanity.io/images/76e3r62u/production/2be5ee7e7ae0847f3bedec01bb88266c371e3eb9-4000x2250.jpg?w=1080&amp;q=75&amp;fit=clip&amp;auto=format 1080w, https://cdn.sanity.io/images/76e3r62u/production/2be5ee7e7ae0847f3bedec01bb88266c371e3eb9-4000x2250.jpg?w=1200&amp;q=75&amp;fit=clip&amp;auto=format 1200w, https://cdn.sanity.io/images/76e3r62u/production/2be5ee7e7ae0847f3bedec01bb88266c371e3eb9-4000x2250.jpg?w=1920&amp;q=75&amp;fit=clip&amp;auto=format 1920w, https://cdn.sanity.io/images/76e3r62u/production/2be5ee7e7ae0847f3bedec01bb88266c371e3eb9-4000x2250.jpg?w=2048&amp;q=75&amp;fit=clip&amp;auto=format 2048w, https://cdn.sanity.io/images/76e3r62u/production/2be5ee7e7ae0847f3bedec01bb88266c371e3eb9-4000x2250.jpg?w=3840&amp;q=75&amp;fit=clip&amp;auto=format 3840w" src="https://cdn.sanity.io/images/76e3r62u/production/2be5ee7e7ae0847f3bedec01bb88266c371e3eb9-4000x2250.jpg?w=3840&amp;q=75&amp;fit=clip&amp;auto=format" decoding="async" data-nimg="fill" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover" loading="lazy"/></noscript></div></a><div class="blog-post_postCard_data__2vfn3"><a href="/[object%20Object]"><h3>Sama&#x27;s Gold Tasks: ML Training Data with Gold-Standard Quality</h3></a><a class="blog-post_postCard_author__Y7RjO" href="/[object%20Object]">Abha Laddha</a><p class="blog-post_postCard_date__hrDMA ">March 23, 2021<!-- --> | 4 Min Read</p></div></div><div class="blog-post_postCard__i0O-O"><a class="blog-post_postCard_image__3k8eA" href="/[object%20Object]"><div style="display:block;overflow:hidden;position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;margin:0"><img alt="" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="fill" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover"/><noscript><img alt="" sizes="100vw" srcSet="https://cdn.sanity.io/images/76e3r62u/production/04a6a4a0b08c6631b9ea6592f4d5e29e4ca09ba5-4496x3000.jpg?w=640&amp;q=75&amp;fit=clip&amp;auto=format 640w, https://cdn.sanity.io/images/76e3r62u/production/04a6a4a0b08c6631b9ea6592f4d5e29e4ca09ba5-4496x3000.jpg?w=750&amp;q=75&amp;fit=clip&amp;auto=format 750w, https://cdn.sanity.io/images/76e3r62u/production/04a6a4a0b08c6631b9ea6592f4d5e29e4ca09ba5-4496x3000.jpg?w=828&amp;q=75&amp;fit=clip&amp;auto=format 828w, https://cdn.sanity.io/images/76e3r62u/production/04a6a4a0b08c6631b9ea6592f4d5e29e4ca09ba5-4496x3000.jpg?w=1080&amp;q=75&amp;fit=clip&amp;auto=format 1080w, https://cdn.sanity.io/images/76e3r62u/production/04a6a4a0b08c6631b9ea6592f4d5e29e4ca09ba5-4496x3000.jpg?w=1200&amp;q=75&amp;fit=clip&amp;auto=format 1200w, https://cdn.sanity.io/images/76e3r62u/production/04a6a4a0b08c6631b9ea6592f4d5e29e4ca09ba5-4496x3000.jpg?w=1920&amp;q=75&amp;fit=clip&amp;auto=format 1920w, https://cdn.sanity.io/images/76e3r62u/production/04a6a4a0b08c6631b9ea6592f4d5e29e4ca09ba5-4496x3000.jpg?w=2048&amp;q=75&amp;fit=clip&amp;auto=format 2048w, https://cdn.sanity.io/images/76e3r62u/production/04a6a4a0b08c6631b9ea6592f4d5e29e4ca09ba5-4496x3000.jpg?w=3840&amp;q=75&amp;fit=clip&amp;auto=format 3840w" src="https://cdn.sanity.io/images/76e3r62u/production/04a6a4a0b08c6631b9ea6592f4d5e29e4ca09ba5-4496x3000.jpg?w=3840&amp;q=75&amp;fit=clip&amp;auto=format" decoding="async" data-nimg="fill" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover" loading="lazy"/></noscript></div></a><div class="blog-post_postCard_data__2vfn3"><a href="/[object%20Object]"><h3>The Traffic Light Problem for Autonomous Vehicles</h3></a><a class="blog-post_postCard_author__Y7RjO" href="/[object%20Object]">Sama Team</a><p class="blog-post_postCard_date__hrDMA ">April 23, 2020<!-- --> | 5 Min Read</p></div></div><div class="blog-post_postCard__i0O-O"><a class="blog-post_postCard_image__3k8eA" href="/[object%20Object]"><div style="display:block;overflow:hidden;position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;margin:0"><img alt="" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="fill" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover"/><noscript><img alt="" sizes="100vw" srcSet="https://cdn.sanity.io/images/76e3r62u/production/7eb4854393afff7be19269b7e8414936e411403f-5506x3671.jpg?w=640&amp;q=75&amp;fit=clip&amp;auto=format 640w, https://cdn.sanity.io/images/76e3r62u/production/7eb4854393afff7be19269b7e8414936e411403f-5506x3671.jpg?w=750&amp;q=75&amp;fit=clip&amp;auto=format 750w, https://cdn.sanity.io/images/76e3r62u/production/7eb4854393afff7be19269b7e8414936e411403f-5506x3671.jpg?w=828&amp;q=75&amp;fit=clip&amp;auto=format 828w, https://cdn.sanity.io/images/76e3r62u/production/7eb4854393afff7be19269b7e8414936e411403f-5506x3671.jpg?w=1080&amp;q=75&amp;fit=clip&amp;auto=format 1080w, https://cdn.sanity.io/images/76e3r62u/production/7eb4854393afff7be19269b7e8414936e411403f-5506x3671.jpg?w=1200&amp;q=75&amp;fit=clip&amp;auto=format 1200w, https://cdn.sanity.io/images/76e3r62u/production/7eb4854393afff7be19269b7e8414936e411403f-5506x3671.jpg?w=1920&amp;q=75&amp;fit=clip&amp;auto=format 1920w, https://cdn.sanity.io/images/76e3r62u/production/7eb4854393afff7be19269b7e8414936e411403f-5506x3671.jpg?w=2048&amp;q=75&amp;fit=clip&amp;auto=format 2048w, https://cdn.sanity.io/images/76e3r62u/production/7eb4854393afff7be19269b7e8414936e411403f-5506x3671.jpg?w=3840&amp;q=75&amp;fit=clip&amp;auto=format 3840w" src="https://cdn.sanity.io/images/76e3r62u/production/7eb4854393afff7be19269b7e8414936e411403f-5506x3671.jpg?w=3840&amp;q=75&amp;fit=clip&amp;auto=format" decoding="async" data-nimg="fill" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover" loading="lazy"/></noscript></div></a><div class="blog-post_postCard_data__2vfn3"><a href="/[object%20Object]"><h3>Object Tracking with Frame Level-Labeling</h3></a><a class="blog-post_postCard_author__Y7RjO" href="/[object%20Object]">Audrey Boguchwal</a><p class="blog-post_postCard_date__hrDMA ">April 2, 2020<!-- --> | 3 Min Read</p></div></div><div class="blog-post_postCard__i0O-O"><a class="blog-post_postCard_image__3k8eA" href="/[object%20Object]"><div style="display:block;overflow:hidden;position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;margin:0"><img alt="" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="fill" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover"/><noscript><img alt="" sizes="100vw" srcSet="https://cdn.sanity.io/images/76e3r62u/production/7e68dc7b894694d301fe5feff694b3df88f8beda-2048x1365.jpg?w=640&amp;q=75&amp;fit=clip&amp;auto=format 640w, https://cdn.sanity.io/images/76e3r62u/production/7e68dc7b894694d301fe5feff694b3df88f8beda-2048x1365.jpg?w=750&amp;q=75&amp;fit=clip&amp;auto=format 750w, https://cdn.sanity.io/images/76e3r62u/production/7e68dc7b894694d301fe5feff694b3df88f8beda-2048x1365.jpg?w=828&amp;q=75&amp;fit=clip&amp;auto=format 828w, https://cdn.sanity.io/images/76e3r62u/production/7e68dc7b894694d301fe5feff694b3df88f8beda-2048x1365.jpg?w=1080&amp;q=75&amp;fit=clip&amp;auto=format 1080w, https://cdn.sanity.io/images/76e3r62u/production/7e68dc7b894694d301fe5feff694b3df88f8beda-2048x1365.jpg?w=1200&amp;q=75&amp;fit=clip&amp;auto=format 1200w, https://cdn.sanity.io/images/76e3r62u/production/7e68dc7b894694d301fe5feff694b3df88f8beda-2048x1365.jpg?w=1920&amp;q=75&amp;fit=clip&amp;auto=format 1920w, https://cdn.sanity.io/images/76e3r62u/production/7e68dc7b894694d301fe5feff694b3df88f8beda-2048x1365.jpg?w=2048&amp;q=75&amp;fit=clip&amp;auto=format 2048w, https://cdn.sanity.io/images/76e3r62u/production/7e68dc7b894694d301fe5feff694b3df88f8beda-2048x1365.jpg?w=3840&amp;q=75&amp;fit=clip&amp;auto=format 3840w" src="https://cdn.sanity.io/images/76e3r62u/production/7e68dc7b894694d301fe5feff694b3df88f8beda-2048x1365.jpg?w=3840&amp;q=75&amp;fit=clip&amp;auto=format" decoding="async" data-nimg="fill" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover" loading="lazy"/></noscript></div></a><div class="blog-post_postCard_data__2vfn3"><a href="/[object%20Object]"><h3>4 Ways AI Makes a Positive Impact on Communities in East Africa</h3></a><a class="blog-post_postCard_author__Y7RjO" href="/[object%20Object]">Liliosa Mbirimi Muturi</a><p class="blog-post_postCard_date__hrDMA ">February 28, 2020<!-- --> | 6 Min Read</p></div></div><div class="blog-post_postCard__i0O-O"><a class="blog-post_postCard_image__3k8eA" href="/[object%20Object]"><div style="display:block;overflow:hidden;position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;margin:0"><img alt="" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="fill" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover"/><noscript><img alt="" sizes="100vw" srcSet="https://cdn.sanity.io/images/76e3r62u/production/31269a4f01b8e495d1ec30a473f063a4957254c6-5760x3840.jpg?w=640&amp;q=75&amp;fit=clip&amp;auto=format 640w, https://cdn.sanity.io/images/76e3r62u/production/31269a4f01b8e495d1ec30a473f063a4957254c6-5760x3840.jpg?w=750&amp;q=75&amp;fit=clip&amp;auto=format 750w, https://cdn.sanity.io/images/76e3r62u/production/31269a4f01b8e495d1ec30a473f063a4957254c6-5760x3840.jpg?w=828&amp;q=75&amp;fit=clip&amp;auto=format 828w, https://cdn.sanity.io/images/76e3r62u/production/31269a4f01b8e495d1ec30a473f063a4957254c6-5760x3840.jpg?w=1080&amp;q=75&amp;fit=clip&amp;auto=format 1080w, https://cdn.sanity.io/images/76e3r62u/production/31269a4f01b8e495d1ec30a473f063a4957254c6-5760x3840.jpg?w=1200&amp;q=75&amp;fit=clip&amp;auto=format 1200w, https://cdn.sanity.io/images/76e3r62u/production/31269a4f01b8e495d1ec30a473f063a4957254c6-5760x3840.jpg?w=1920&amp;q=75&amp;fit=clip&amp;auto=format 1920w, https://cdn.sanity.io/images/76e3r62u/production/31269a4f01b8e495d1ec30a473f063a4957254c6-5760x3840.jpg?w=2048&amp;q=75&amp;fit=clip&amp;auto=format 2048w, https://cdn.sanity.io/images/76e3r62u/production/31269a4f01b8e495d1ec30a473f063a4957254c6-5760x3840.jpg?w=3840&amp;q=75&amp;fit=clip&amp;auto=format 3840w" src="https://cdn.sanity.io/images/76e3r62u/production/31269a4f01b8e495d1ec30a473f063a4957254c6-5760x3840.jpg?w=3840&amp;q=75&amp;fit=clip&amp;auto=format" decoding="async" data-nimg="fill" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover" loading="lazy"/></noscript></div></a><div class="blog-post_postCard_data__2vfn3"><a href="/[object%20Object]"><h3>Revamped 2D Vector Segmentation</h3></a><a class="blog-post_postCard_author__Y7RjO" href="/[object%20Object]">Audrey Boguchwal</a><p class="blog-post_postCard_date__hrDMA ">July 11, 2019<!-- --> | 2 Min Read</p></div></div><div class="blog-post_postCard__i0O-O"><a class="blog-post_postCard_image__3k8eA" href="/[object%20Object]"><div style="display:block;overflow:hidden;position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;margin:0"><img alt="" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="fill" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover"/><noscript><img alt="" sizes="100vw" srcSet="https://cdn.sanity.io/images/76e3r62u/production/a49d2248810c7d2141277314bfa9b78a95c2a9a3-2250x1500.jpg?w=640&amp;q=75&amp;fit=clip&amp;auto=format 640w, https://cdn.sanity.io/images/76e3r62u/production/a49d2248810c7d2141277314bfa9b78a95c2a9a3-2250x1500.jpg?w=750&amp;q=75&amp;fit=clip&amp;auto=format 750w, https://cdn.sanity.io/images/76e3r62u/production/a49d2248810c7d2141277314bfa9b78a95c2a9a3-2250x1500.jpg?w=828&amp;q=75&amp;fit=clip&amp;auto=format 828w, https://cdn.sanity.io/images/76e3r62u/production/a49d2248810c7d2141277314bfa9b78a95c2a9a3-2250x1500.jpg?w=1080&amp;q=75&amp;fit=clip&amp;auto=format 1080w, https://cdn.sanity.io/images/76e3r62u/production/a49d2248810c7d2141277314bfa9b78a95c2a9a3-2250x1500.jpg?w=1200&amp;q=75&amp;fit=clip&amp;auto=format 1200w, https://cdn.sanity.io/images/76e3r62u/production/a49d2248810c7d2141277314bfa9b78a95c2a9a3-2250x1500.jpg?w=1920&amp;q=75&amp;fit=clip&amp;auto=format 1920w, https://cdn.sanity.io/images/76e3r62u/production/a49d2248810c7d2141277314bfa9b78a95c2a9a3-2250x1500.jpg?w=2048&amp;q=75&amp;fit=clip&amp;auto=format 2048w, https://cdn.sanity.io/images/76e3r62u/production/a49d2248810c7d2141277314bfa9b78a95c2a9a3-2250x1500.jpg?w=3840&amp;q=75&amp;fit=clip&amp;auto=format 3840w" src="https://cdn.sanity.io/images/76e3r62u/production/a49d2248810c7d2141277314bfa9b78a95c2a9a3-2250x1500.jpg?w=3840&amp;q=75&amp;fit=clip&amp;auto=format" decoding="async" data-nimg="fill" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover" loading="lazy"/></noscript></div></a><div class="blog-post_postCard_data__2vfn3"><a href="/[object%20Object]"><h3>10 Organizations Leading the Way in Computer Vision</h3></a><a class="blog-post_postCard_author__Y7RjO" href="/[object%20Object]">Sharon L. Hadden</a><p class="blog-post_postCard_date__hrDMA ">June 12, 2019<!-- --> | 4 Min Read</p></div></div><div class="blog-post_postCard__i0O-O"><a class="blog-post_postCard_image__3k8eA" href="/[object%20Object]"><div style="display:block;overflow:hidden;position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;margin:0"><img alt="" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="fill" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover"/><noscript><img alt="" sizes="100vw" srcSet="https://cdn.sanity.io/images/76e3r62u/production/9a175242c5b6d9d34ab8d001420280c4279ffe11-1125x1500.jpg?w=640&amp;q=75&amp;fit=clip&amp;auto=format 640w, https://cdn.sanity.io/images/76e3r62u/production/9a175242c5b6d9d34ab8d001420280c4279ffe11-1125x1500.jpg?w=750&amp;q=75&amp;fit=clip&amp;auto=format 750w, https://cdn.sanity.io/images/76e3r62u/production/9a175242c5b6d9d34ab8d001420280c4279ffe11-1125x1500.jpg?w=828&amp;q=75&amp;fit=clip&amp;auto=format 828w, https://cdn.sanity.io/images/76e3r62u/production/9a175242c5b6d9d34ab8d001420280c4279ffe11-1125x1500.jpg?w=1080&amp;q=75&amp;fit=clip&amp;auto=format 1080w, https://cdn.sanity.io/images/76e3r62u/production/9a175242c5b6d9d34ab8d001420280c4279ffe11-1125x1500.jpg?w=1200&amp;q=75&amp;fit=clip&amp;auto=format 1200w, https://cdn.sanity.io/images/76e3r62u/production/9a175242c5b6d9d34ab8d001420280c4279ffe11-1125x1500.jpg?w=1920&amp;q=75&amp;fit=clip&amp;auto=format 1920w, https://cdn.sanity.io/images/76e3r62u/production/9a175242c5b6d9d34ab8d001420280c4279ffe11-1125x1500.jpg?w=2048&amp;q=75&amp;fit=clip&amp;auto=format 2048w, https://cdn.sanity.io/images/76e3r62u/production/9a175242c5b6d9d34ab8d001420280c4279ffe11-1125x1500.jpg?w=3840&amp;q=75&amp;fit=clip&amp;auto=format 3840w" src="https://cdn.sanity.io/images/76e3r62u/production/9a175242c5b6d9d34ab8d001420280c4279ffe11-1125x1500.jpg?w=3840&amp;q=75&amp;fit=clip&amp;auto=format" decoding="async" data-nimg="fill" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover" loading="lazy"/></noscript></div></a><div class="blog-post_postCard_data__2vfn3"><a href="/[object%20Object]"><h3>Training Your AI in 3D</h3></a><a class="blog-post_postCard_author__Y7RjO" href="/[object%20Object]">Matthew Landry</a><p class="blog-post_postCard_date__hrDMA ">December 14, 2018<!-- --> | 8 Min Read</p></div></div><div class="blog-post_postCard__i0O-O"><a class="blog-post_postCard_image__3k8eA" href="/[object%20Object]"><div style="display:block;overflow:hidden;position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;margin:0"><img alt="" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="fill" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover"/><noscript><img alt="" sizes="100vw" srcSet="https://cdn.sanity.io/images/76e3r62u/production/af58edf394d9676203b9bc44284f08ccf52125a6-1000x1500.jpg?w=640&amp;q=75&amp;fit=clip&amp;auto=format 640w, https://cdn.sanity.io/images/76e3r62u/production/af58edf394d9676203b9bc44284f08ccf52125a6-1000x1500.jpg?w=750&amp;q=75&amp;fit=clip&amp;auto=format 750w, https://cdn.sanity.io/images/76e3r62u/production/af58edf394d9676203b9bc44284f08ccf52125a6-1000x1500.jpg?w=828&amp;q=75&amp;fit=clip&amp;auto=format 828w, https://cdn.sanity.io/images/76e3r62u/production/af58edf394d9676203b9bc44284f08ccf52125a6-1000x1500.jpg?w=1080&amp;q=75&amp;fit=clip&amp;auto=format 1080w, https://cdn.sanity.io/images/76e3r62u/production/af58edf394d9676203b9bc44284f08ccf52125a6-1000x1500.jpg?w=1200&amp;q=75&amp;fit=clip&amp;auto=format 1200w, https://cdn.sanity.io/images/76e3r62u/production/af58edf394d9676203b9bc44284f08ccf52125a6-1000x1500.jpg?w=1920&amp;q=75&amp;fit=clip&amp;auto=format 1920w, https://cdn.sanity.io/images/76e3r62u/production/af58edf394d9676203b9bc44284f08ccf52125a6-1000x1500.jpg?w=2048&amp;q=75&amp;fit=clip&amp;auto=format 2048w, https://cdn.sanity.io/images/76e3r62u/production/af58edf394d9676203b9bc44284f08ccf52125a6-1000x1500.jpg?w=3840&amp;q=75&amp;fit=clip&amp;auto=format 3840w" src="https://cdn.sanity.io/images/76e3r62u/production/af58edf394d9676203b9bc44284f08ccf52125a6-1000x1500.jpg?w=3840&amp;q=75&amp;fit=clip&amp;auto=format" decoding="async" data-nimg="fill" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover" loading="lazy"/></noscript></div></a><div class="blog-post_postCard_data__2vfn3"><a href="/[object%20Object]"><h3>Introducing Object Tracking with Video Annotation</h3></a><a class="blog-post_postCard_author__Y7RjO" href="/[object%20Object]">Matthew Landry</a><p class="blog-post_postCard_date__hrDMA ">July 16, 2018<!-- --> | 5 Min Read</p></div></div><div class="blog-post_postCard__i0O-O"><a class="blog-post_postCard_image__3k8eA" href="/[object%20Object]"><div style="display:block;overflow:hidden;position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;margin:0"><img alt="" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="fill" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover"/><noscript><img alt="" sizes="100vw" srcSet="https://cdn.sanity.io/images/76e3r62u/production/200df2138bcc00249eacdf9d29fa4c78fc756b0a-844x1500.jpg?w=640&amp;q=75&amp;fit=clip&amp;auto=format 640w, https://cdn.sanity.io/images/76e3r62u/production/200df2138bcc00249eacdf9d29fa4c78fc756b0a-844x1500.jpg?w=750&amp;q=75&amp;fit=clip&amp;auto=format 750w, https://cdn.sanity.io/images/76e3r62u/production/200df2138bcc00249eacdf9d29fa4c78fc756b0a-844x1500.jpg?w=828&amp;q=75&amp;fit=clip&amp;auto=format 828w, https://cdn.sanity.io/images/76e3r62u/production/200df2138bcc00249eacdf9d29fa4c78fc756b0a-844x1500.jpg?w=1080&amp;q=75&amp;fit=clip&amp;auto=format 1080w, https://cdn.sanity.io/images/76e3r62u/production/200df2138bcc00249eacdf9d29fa4c78fc756b0a-844x1500.jpg?w=1200&amp;q=75&amp;fit=clip&amp;auto=format 1200w, https://cdn.sanity.io/images/76e3r62u/production/200df2138bcc00249eacdf9d29fa4c78fc756b0a-844x1500.jpg?w=1920&amp;q=75&amp;fit=clip&amp;auto=format 1920w, https://cdn.sanity.io/images/76e3r62u/production/200df2138bcc00249eacdf9d29fa4c78fc756b0a-844x1500.jpg?w=2048&amp;q=75&amp;fit=clip&amp;auto=format 2048w, https://cdn.sanity.io/images/76e3r62u/production/200df2138bcc00249eacdf9d29fa4c78fc756b0a-844x1500.jpg?w=3840&amp;q=75&amp;fit=clip&amp;auto=format 3840w" src="https://cdn.sanity.io/images/76e3r62u/production/200df2138bcc00249eacdf9d29fa4c78fc756b0a-844x1500.jpg?w=3840&amp;q=75&amp;fit=clip&amp;auto=format" decoding="async" data-nimg="fill" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover" loading="lazy"/></noscript></div></a><div class="blog-post_postCard_data__2vfn3"><a href="/[object%20Object]"><h3>What&#x27;s the Latest with Lidar and Point Cloud Annotation?</h3></a><a class="blog-post_postCard_author__Y7RjO" href="/[object%20Object]">Steve Allen</a><p class="blog-post_postCard_date__hrDMA ">March 20, 2018<!-- --> | 6 Min Read</p></div></div><div class="blog-post_postCard__i0O-O"><a class="blog-post_postCard_image__3k8eA" href="/[object%20Object]"><div style="display:block;overflow:hidden;position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;margin:0"><img alt="" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="fill" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover"/><noscript><img alt="" sizes="100vw" srcSet="https://cdn.sanity.io/images/76e3r62u/production/73eb0ced8bf91c8bd71f48061438e8d61ce0f8b6-597x398.png?w=640&amp;q=75&amp;fit=clip&amp;auto=format 640w, https://cdn.sanity.io/images/76e3r62u/production/73eb0ced8bf91c8bd71f48061438e8d61ce0f8b6-597x398.png?w=750&amp;q=75&amp;fit=clip&amp;auto=format 750w, https://cdn.sanity.io/images/76e3r62u/production/73eb0ced8bf91c8bd71f48061438e8d61ce0f8b6-597x398.png?w=828&amp;q=75&amp;fit=clip&amp;auto=format 828w, https://cdn.sanity.io/images/76e3r62u/production/73eb0ced8bf91c8bd71f48061438e8d61ce0f8b6-597x398.png?w=1080&amp;q=75&amp;fit=clip&amp;auto=format 1080w, https://cdn.sanity.io/images/76e3r62u/production/73eb0ced8bf91c8bd71f48061438e8d61ce0f8b6-597x398.png?w=1200&amp;q=75&amp;fit=clip&amp;auto=format 1200w, https://cdn.sanity.io/images/76e3r62u/production/73eb0ced8bf91c8bd71f48061438e8d61ce0f8b6-597x398.png?w=1920&amp;q=75&amp;fit=clip&amp;auto=format 1920w, https://cdn.sanity.io/images/76e3r62u/production/73eb0ced8bf91c8bd71f48061438e8d61ce0f8b6-597x398.png?w=2048&amp;q=75&amp;fit=clip&amp;auto=format 2048w, https://cdn.sanity.io/images/76e3r62u/production/73eb0ced8bf91c8bd71f48061438e8d61ce0f8b6-597x398.png?w=3840&amp;q=75&amp;fit=clip&amp;auto=format 3840w" src="https://cdn.sanity.io/images/76e3r62u/production/73eb0ced8bf91c8bd71f48061438e8d61ce0f8b6-597x398.png?w=3840&amp;q=75&amp;fit=clip&amp;auto=format" decoding="async" data-nimg="fill" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover" loading="lazy"/></noscript></div></a><div class="blog-post_postCard_data__2vfn3"><a href="/[object%20Object]"><h3>Better Algorithms, Better Lives: Reducing Poverty Through Training Data</h3></a><a class="blog-post_postCard_author__Y7RjO" href="/[object%20Object]">Audrey Boguchwal</a><p class="blog-post_postCard_date__hrDMA ">May 24, 2017<!-- --> | 4 Min Read</p></div></div><div class="blog-post_postCard__i0O-O"><a class="blog-post_postCard_image__3k8eA" href="/[object%20Object]"><div style="display:block;overflow:hidden;position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;margin:0"><img alt="" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="fill" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover"/><noscript><img alt="" sizes="100vw" srcSet="https://cdn.sanity.io/images/76e3r62u/production/bde2f64018e50b7615f188245cc00fe516e84936-693x462.jpg?w=640&amp;q=75&amp;fit=clip&amp;auto=format 640w, https://cdn.sanity.io/images/76e3r62u/production/bde2f64018e50b7615f188245cc00fe516e84936-693x462.jpg?w=750&amp;q=75&amp;fit=clip&amp;auto=format 750w, https://cdn.sanity.io/images/76e3r62u/production/bde2f64018e50b7615f188245cc00fe516e84936-693x462.jpg?w=828&amp;q=75&amp;fit=clip&amp;auto=format 828w, https://cdn.sanity.io/images/76e3r62u/production/bde2f64018e50b7615f188245cc00fe516e84936-693x462.jpg?w=1080&amp;q=75&amp;fit=clip&amp;auto=format 1080w, https://cdn.sanity.io/images/76e3r62u/production/bde2f64018e50b7615f188245cc00fe516e84936-693x462.jpg?w=1200&amp;q=75&amp;fit=clip&amp;auto=format 1200w, https://cdn.sanity.io/images/76e3r62u/production/bde2f64018e50b7615f188245cc00fe516e84936-693x462.jpg?w=1920&amp;q=75&amp;fit=clip&amp;auto=format 1920w, https://cdn.sanity.io/images/76e3r62u/production/bde2f64018e50b7615f188245cc00fe516e84936-693x462.jpg?w=2048&amp;q=75&amp;fit=clip&amp;auto=format 2048w, https://cdn.sanity.io/images/76e3r62u/production/bde2f64018e50b7615f188245cc00fe516e84936-693x462.jpg?w=3840&amp;q=75&amp;fit=clip&amp;auto=format 3840w" src="https://cdn.sanity.io/images/76e3r62u/production/bde2f64018e50b7615f188245cc00fe516e84936-693x462.jpg?w=3840&amp;q=75&amp;fit=clip&amp;auto=format" decoding="async" data-nimg="fill" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover" loading="lazy"/></noscript></div></a><div class="blog-post_postCard_data__2vfn3"><a href="/[object%20Object]"><h3>How Samasource Moves People Out of Poverty with Digital Work</h3></a><a class="blog-post_postCard_author__Y7RjO" href="/[object%20Object]">Audrey Boguchwal</a><p class="blog-post_postCard_date__hrDMA ">May 15, 2017<!-- --> | 3 Min Read</p></div></div></div></section><section class="umoja-l-grid-section umoja-l-grid-section--flat-top umoja-u-bg--white"><div class="button_wrapper__3lRbv button__secondary__1pZ5q "><button class="button_btn__1qxP1"><h3 class="button_text__3_sCS">Load More</h3></button></div></section></main><footer class="footer_wrapper__2VAfJ"><div class="umoja-l-grid--12"><div class="footer_upper__2a6XG"><div><h4>Newsletter</h4><p>Subscribe today and be the first to receive the latest from Sama.</p></div><div class="footer_upper_right__cpliC"><div><p class="footer_nav_head__1keQK">Guides</p><a class="footer_nav_link__X1RNI" href="/training-data-for-autonomous-driving">Autonomous Transportation</a><a class="footer_nav_link__X1RNI" href="/training-data-for-ecommerce">E-Commerce</a><a class="footer_nav_link__X1RNI" href="/training-data-for-ar-vr">AR/VR</a><a class="footer_nav_link__X1RNI" href="/data-quality">Data Quality</a></div><div><p class="footer_nav_head__1keQK">Company</p><a class="footer_nav_link__X1RNI" href="/our-story">Our Story</a><a class="footer_nav_link__X1RNI" href="/our-team">Our Team</a><a class="footer_nav_link__X1RNI" href="/mission-vision-values">Our Mission</a><a class="footer_nav_link__X1RNI" href="/careers">Careers</a><a class="footer_nav_link__X1RNI" href="/company-contact">Contact</a></div></div></div><div class="footer_middle__iiTSJ"><div class="footer_middle_left__3ff78"><a href="/"></a></div><div class="footer_middle_right__2b-lC"><div class="footer_social__1NFfV"><a href="https://www.facebook.com/samaartificialintelligence" class="footer_social_icon__wI2OK" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 35.1 64.89"><title>facebook</title><g id="fb3209c8-23d4-4288-984d-a2b8f32b7f0c" data-name="Layer 2"><g id="ba1a815d-13f1-45f9-9321-18e9a3cfa5db" data-name="Layer 1"><path d="M35.1,11.26V1.36A1.35,1.35,0,0,0,33.76,0H25.35A15.34,15.34,0,0,0,14,4.35C11.24,7.2,9.78,11.22,9.78,16v7.34H1.34A1.34,1.34,0,0,0,0,24.66V35.32a1.35,1.35,0,0,0,1.34,1.35H9.78V63.55a1.34,1.34,0,0,0,1.34,1.34h11a1.34,1.34,0,0,0,1.34-1.34V36.67h9.87a1.35,1.35,0,0,0,1.34-1.35V24.66a1.37,1.37,0,0,0-.7-1.18,1.47,1.47,0,0,0-.67-.16H23.49V17.1c0-1.72.25-2.69.84-3.37s1.88-1.13,3.77-1.13h5.66A1.34,1.34,0,0,0,35.1,11.26Z"></path></g></g></svg></a><a href="https://www.instagram.com/sama_ai_" class="footer_social_icon__wI2OK" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 57 57"><title>insta</title><g id="ef02a3ef-c0d3-4be7-9d4e-2c42263777a3" data-name="Layer 2"><g id="a0a92778-6c5c-4e06-a08f-641546580dd0" data-name="Layer 1"><circle cx="28.5" cy="28.5" r="9.24"></circle><path d="M41.57,0H15.43A15.45,15.45,0,0,0,0,15.43V41.57A15.45,15.45,0,0,0,15.43,57H41.57A15.45,15.45,0,0,0,57,41.57V15.43A15.45,15.45,0,0,0,41.57,0ZM28.5,42.74A14.24,14.24,0,1,1,42.74,28.5,14.26,14.26,0,0,1,28.5,42.74ZM44.46,17a5,5,0,1,1,5-5A5,5,0,0,1,44.46,17Z"></path></g></g></svg></a><a href="https://twitter.com/SamaAI" class="footer_social_icon__wI2OK" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 62 51.19"><title>twitter</title><g id="e7743e55-1863-47ad-a995-4b7f1f924f07" data-name="Layer 2"><g id="ec8c6fd9-2f52-4507-9a24-804bc60dbc33" data-name="Layer 1"><path d="M23.59,51.19c-10.35,0-18.53-1.81-22.44-5l-.07-.06L1,46.1a3.19,3.19,0,0,1-.84-3.35l0-.1a3.24,3.24,0,0,1,3-2,26.57,26.57,0,0,0,7.06-1,13.45,13.45,0,0,1-7.07-8.16,2.92,2.92,0,0,1,1-3.38,3.06,3.06,0,0,1,.88-.45,19.52,19.52,0,0,1-4-7.18l0-.08,0-.09a3,3,0,0,1,1.4-3.23,3,3,0,0,1,1.43-.4,15.15,15.15,0,0,1-1.14-3.49A14.59,14.59,0,0,1,4.24,3.47l.38-.77a2.15,2.15,0,0,1,3.44-.56l.7.7c5.53,5.81,10.49,8.56,19.06,10.44a15.17,15.17,0,0,1,4.1-8.75A14.39,14.39,0,0,1,42.19,0h0c2.84,0,6.36,1.62,8.49,2.77,1.83-.6,4-1.53,6.32-2.51a2.88,2.88,0,0,1,3.22.57,2.85,2.85,0,0,1,.62,3.11c-.17.47-.36.92-.57,1.36a3.07,3.07,0,0,1,.84.58,3.13,3.13,0,0,1,.78,2.92l0,.1a11.92,11.92,0,0,1-4.78,6.56C56.73,35.23,41.84,51.19,23.59,51.19Z"></path></g></g></svg></a><a href="https://www.linkedin.com/company/sama-ai/" class="footer_social_icon__wI2OK" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 59.71 60.79"><title>linkedin</title><g id="bd073dff-b6ea-4cf0-bfeb-c3ce392ee6a5" data-name="Layer 2"><g id="f6b6eb77-7c10-4e27-a3d0-6f8b14e97f2e" data-name="Layer 1"><path d="M59.65,60.79l-12.35,0,0-19.36c0-4.62-.07-10.56-6.41-10.58s-7.44,5-7.45,10.21l0,19.7-12.36,0,.09-40.95,11.87,0v6.57h.16c1.66-3.13,5.7-6.42,11.73-6.41,12.51,0,14.81,8.28,14.79,19l-.05,21.85Z"></path><path d="M7.17,14.35a7.18,7.18,0,1,1,7.18-7.18A7.17,7.17,0,0,1,7.17,14.35Z"></path><rect x="0.98" y="19.8" width="12.39" height="40.95"></rect></g></g></svg></a><a href="https://www.youtube.com/c/SamaAI" class="footer_social_icon__wI2OK" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 65 44.63"><title>youtube</title><g id="b03ac260-8029-40d4-832d-1f2d757db2d5" data-name="Layer 2"><g id="eca1bfe4-f0ec-4c24-9169-be6d2f8b24b5" data-name="Layer 1"><path d="M55,0H10A10,10,0,0,0,0,10V34.63a10,10,0,0,0,10,10H55a10,10,0,0,0,10-10V10A10,10,0,0,0,55,0ZM40.89,24.41,28.3,31.18a2.31,2.31,0,0,1-3.41-2V15.48a2.3,2.3,0,0,1,3.42-2l12.6,6.89a2.31,2.31,0,0,1,0,4.06Z"></path></g></g></svg></a></div></div></div><div class="footer_lower__1z3Av"><div class="footer_lower_left__141hE"><a class="footer_nav_link__X1RNI" href="/terms-of-service">Terms</a><a class="footer_nav_link__X1RNI" href="/privacy-policy">Privacy</a><a class="footer_nav_link__X1RNI" href="/quality-and-information-policy">Quality &amp; Information</a></div><div class="footer_lower_right__22vMw"><h6>Copyright © <!-- -->0<!-- --> Sama Inc.</h6><h6>All rights reserved.</h6></div></div></div></footer></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"config":{"footerNav":{"items":[{"_key":"f255606f8f25","_type":"navDropdownMenu","items":[{"_key":"76389ad94cbb","_type":"navItem","title":"Autonomous Transportation","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"training-data-for-autonomous-driving"}}}},{"_key":"5f64a8d6a69d","_type":"navItem","title":"E-Commerce","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"training-data-for-ecommerce"}}}},{"_key":"f10e54ae04d0","_type":"navItem","title":"AR/VR","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"training-data-for-ar-vr"}}}},{"_key":"fd729b522a77","_type":"navItem","title":"Data Quality","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"data-quality"}}}}],"title":"Guides","url":null},{"_key":"681ef7d8763a","_type":"navDropdownMenu","items":[{"_key":"6238a422b667","_type":"navItem","title":"Our Story","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"our-story"}}}},{"_key":"258985d6d46b","_type":"navItem","title":"Our Team","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"our-team"}}}},{"_key":"e0a76077324a","_type":"navItem","title":"Our Mission","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"mission-vision-values"}}}},{"_key":"239e49661b0d","_type":"navItem","title":"Careers","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"careers"}}}},{"_key":"e005a740cd80","_type":"navItem","title":"Contact","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"company-contact"}}}}],"title":"Company","url":null}]},"logo":{"asset":{"_createdAt":"2021-12-09T21:42:35Z","_id":"image-4f4e2f86a8fad952c02dffffd7008aa35f83c850-2380x636-svg","_rev":"7Z7VDk3xHzg51hvomGzc99","_type":"sanity.imageAsset","_updatedAt":"2021-12-09T21:42:35Z","assetId":"4f4e2f86a8fad952c02dffffd7008aa35f83c850","extension":"svg","metadata":{"_type":"sanity.imageMetadata","blurHash":"D009jvfQfQfQfQfQfQfQfQfQ","dimensions":{"_type":"sanity.imageDimensions","aspectRatio":3.742138364779874,"height":636,"width":2380},"hasAlpha":true,"isOpaque":false,"lqip":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAFCAYAAABFA8wzAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAr0lEQVQYlU3QyUoDQBAE0HeIEjeixAVyESRI3BNFJSZxAU/+/wdJQSEehu6Zqa6uKnjBGz7xgTuc4gAj7LWe4LD9cc8Y+9jBUed84QGrEt/gEleYY9Ylr3jGU/tV77fFLirId0nWBYYgoACi+r3D6YPN0vwFm4VxmNlgfkK4qcLUZdVMcdHB+75FzWPfzzCpxXNcFxuOP2uxnhNLASaX5LjbnJJf6jYG2PpXh812/AvSEQ+GGZqgYgAAAABJRU5ErkJggg==","palette":{"_type":"sanity.imagePalette","darkMuted":{"_type":"sanity.imagePaletteSwatch","background":"#040404","foreground":"#fff","population":100.29,"title":"#fff"},"darkVibrant":{"_type":"sanity.imagePaletteSwatch","background":"#424242","foreground":"#fff","population":0,"title":"#fff"},"dominant":{"_type":"sanity.imagePaletteSwatch","background":"#040404","foreground":"#fff","population":100.29,"title":"#fff"},"lightMuted":{"_type":"sanity.imagePaletteSwatch","background":"#4c4c4c","foreground":"#fff","population":0,"title":"#fff"},"lightVibrant":{"_type":"sanity.imagePaletteSwatch","background":"#bcbcbc","foreground":"#000","population":0,"title":"#fff"},"muted":{"_type":"sanity.imagePaletteSwatch","background":"#4c4c4c","foreground":"#fff","population":0,"title":"#fff"},"vibrant":{"_type":"sanity.imagePaletteSwatch","background":"#7f7f7f","foreground":"#fff","population":0,"title":"#fff"}}},"mimeType":"image/svg+xml","originalFilename":"e20f8cc53e5f74df10ae9a822edb7ec2c4d00f02-2380x636.svg","path":"images/76e3r62u/production/4f4e2f86a8fad952c02dffffd7008aa35f83c850-2380x636.svg","sha1hash":"4f4e2f86a8fad952c02dffffd7008aa35f83c850","size":2009,"uploadId":"jTUF9DIFqAwpLJ0GcI9bRqb17D69QQlN","url":"https://cdn.sanity.io/images/76e3r62u/production/4f4e2f86a8fad952c02dffffd7008aa35f83c850-2380x636.svg"}},"mainNav":{"items":[{"_key":"58c18e9aa9ea","_type":"navDropdownMenu","items":[{"_key":"b5b5b8bee78b","_type":"navCat","items":[{"_key":"0e80156a2f1a","_type":"navItem","title":"How it Works","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"how-it-works"}}}},{"_key":"40bacee029b4","_type":"navItem","title":"Video Annotation","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"video-annotation"}}}},{"_key":"32650ef07503","_type":"navItem","title":"Image Annotation","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"image-annotation"}}}},{"_key":"fe9137cd0167","_type":"navItem","title":"3D \u0026 LiDAR Annotation","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"3d-lidar"}}}},{"_key":"d9a1316d400a","_type":"navItem","title":"Natural Language Processing","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"natural-language-processing"}}}},{"_key":"ac12c7c5d70a","_type":"navItem","title":"Data Curation (Beta)","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"data-curation"}}}}],"title":"Platform","url":null},{"_key":"37ff4fa913bd","_type":"navCat","items":[{"_key":"6026b1a9314e","_type":"navItem","title":"Semantic Segmentation","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"semantic-segmentation"}}}},{"_key":"f4611b19b406","_type":"navItem","title":"Polygons","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"polygons"}}}},{"_key":"5155d874d6c8","_type":"navItem","title":"Bounding Boxes","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"bounding-boxes"}}}},{"_key":"9ef3c1e21e74","_type":"navItem","title":"Key Points","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"key-points"}}}},{"_key":"314d4c00d351","_type":"navItem","title":"Cuboids","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"cuboids"}}}},{"_key":"8e17a6388d74","_type":"navItem","title":"Lines \u0026 Arrows","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"lines-and-arrows"}}}}],"title":"Shapes","url":null}],"title":"Platform","url":null},{"_key":"112867ca4d03","_type":"navDropdownMenu","items":[{"_key":"22699c7e06cb","_type":"navItem","items":null,"title":"Transportation \u0026 Navigation","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"transportation-navigation"}}}},{"_key":"122ae5928d6d","_type":"navItem","items":null,"title":"Retail \u0026 E-Commerce","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"retail-ecommerce"}}}},{"_key":"7bb234b69fb0","_type":"navItem","items":null,"title":"Consumer \u0026 Media","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"consumer-media"}}}},{"_key":"33e6a886b39d","_type":"navItem","items":null,"title":"Biotech \u0026 Medtech","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"biotech-medtech"}}}},{"_key":"d095b2619c4e","_type":"navItem","items":null,"title":"Robotics \u0026 Manufacturing","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"robotics-and-manufacturing"}}}},{"_key":"2c4b82a94d79","_type":"navItem","items":null,"title":"Food \u0026 Agriculture","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"training-data-food-agriculture"}}}}],"title":"Industries","url":null},{"_key":"c47e8763a906","_type":"navDropdownMenu","items":[{"_key":"1d563df30b3f","_type":"navItem","items":null,"title":"Quality","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"quality-training-data"}}}},{"_key":"041725f35d96","_type":"navItem","items":null,"title":"Security","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"security-and-trust"}}}},{"_key":"fd64ede25798","_type":"navItem","items":null,"title":"Ethical AI","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"our-impact"}}}},{"_key":"398dcbb1c95d","_type":"navItem","items":null,"title":"Compare","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"compare"}}}},{"_key":"93bdfdd87879","_type":"navItem","items":null,"title":"Partners","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"partners"}}}}],"title":"Why Sama","url":null},{"_key":"1d38bf63df54","_type":"navDropdownMenu","items":[{"_key":"be81659b38a5","_type":"navItem","items":null,"title":"API Documentation","url":{"_type":"link","externalUrl":"https://docs.sama.com/reference/overview","internalLink":null}},{"_key":"2cec80e94962","_type":"navItem","items":null,"title":"Blog","url":{"_type":"link","internalLink":null,"internalLink_custom":"/blog"}},{"_key":"09e284fcb1d3","_type":"navItem","items":null,"title":"Events","url":{"_type":"link","internalLink":null,"internalLink_custom":"/events"}}],"title":"Resources","url":null},{"_key":"dbee93713c19","_type":"navDropdownMenu","items":[{"_key":"12d594a568bf","_type":"navItem","items":null,"title":"Our Story","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"our-story"}}}},{"_key":"ce36540a102d","_type":"navItem","items":null,"title":"Our Team","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"our-team"}}}},{"_key":"34fc328e8022","_type":"navItem","items":null,"title":"Careers","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"careers"}}}},{"_key":"c1fe2961020a","_type":"navItem","items":null,"title":"Contact","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"company-contact"}}}},{"_key":"ebd81873e538","_type":"navItem","items":null,"title":"Press","url":{"_type":"link","internalLink":null,"internalLink_custom":"/press"}}],"title":"Company","url":null}],"nav_cta":{"_type":"button","link":{"_type":"link","internalLink":{"_ref":"136788cb-06a6-4f27-b75b-07faf403bfa6","_type":"reference"}},"title":"Request a Demo","type":"secondary"}}},"data":{"firstLoad":[{"_createdAt":"2021-07-29T17:14:55Z","author":{"_id":"10ead718-57e1-41a8-b846-da3c81cc323a","avatar":{"_type":"image","asset":{"_ref":"image-a4c79da81bb1e23ce10fba84ea2cba5efe67a2a5-200x200-webp","_type":"reference"}},"bio":"Currently a Director of Product Management at Sama, Saul is passionate about the intersection of technology and social impact. He manages Sama’s data labelling products to ensure high quality training data efficiently and reliably reaches our customers. Experienced in both product and professional services, Saul is a proven leader who takes a data driven approach to expanding Sama’s capabilities and features. When not at work, you can usually find Saul enjoying the outdoors and spending time with his family.","name":"Saul Miller","slug":{"_type":"slug","current":"saul-miller"}},"config":{"description":"ML Assisted Annotation can help you generate high-quality pre-labeled and human-assisted annotations, for predictably higher quality data in half the time.","openGraphImage":null,"title":"ML Assisted Annotation Powered by MicroModels"},"estimatedReadingTime":7,"featured_image":{"_type":"image","asset":{"_ref":"image-65b7fac1b60586a0a7b9ff75006684e2c2467f1e-1800x900-png","_type":"reference"}},"plaintextBody":"Your machine learning model is only as good as the data it’s trained on. And with 80% of AI project time being spent training the large volume of data necessary to train a model, efficiency improvements early on in the process are sure to have compounding effects.\n\nAt Sama, we have a dedicated Machine Learning team working at the forefront of AI research to identify optimization opportunities just like this, so we can develop advanced annotation tools to smooth the path to production for our clients. One of the papers the team presented at last year’s CVPR—Human-Centric Efficiency Improvements in Image Annotation for Autonomous Driving—shared an approach to speeding up polygonal instance segmentation using ML.\n\nToday, this technology has been incorporated into our platform to make our clients’ labeling process more efficient.\n\nWe call it ML Assisted Annotation powered by our MICROMODEL technology, and it’s already helping clients predictably get higher quality training data in half the time.\n\nRead on for an overview of ML Assisted Annotation powered by MICROMODEL technology how it can help you develop models that are more scalable, robust and accurate – and can be brought into production more quickly.\n\nWhat is ML Assisted Annotation powered by MICROMODEL technology?\n\n\nML Assisted Annotation (MAA) powered by MICROMODEL technology is an architecture that allows Sama to expedite the labeling process by drawing from a library of models trained on specific use cases. MAA can be used to generate high-quality pre-labeled annotations, which annotators validate to help them continuously improve over time.\n\nThis powerful combination of skilled annotators and an AI-powered platform allows us to deliver a high standard of label quality to our customers every time, along with efficiency improvements and quicker time to market.\n\nHow it works\n\nIn order to understand how MAA works, we first need to discuss the DEXTR model. DEXTR, or “Deep Extreme Cut,” is a publicly available object segmentation model for images and videos. \n\n\n\nWe’ve outlined the DEXTR model and our approach in detail in this post.\n\n\n\nMany ML methods like DEXTR have been suggested to speed up the process of instance segmentation, but these are not typically tested in a high-scale production environment, nor are ML outputs easily edited by human annotators. This makes it difficult to confidently reach the label quality standards required to run a model in production.\n\nMAA combines the well-known DEXTR approach with a raster-to-polygon algorithm to make results easily editable by a human in the loop. We’ve found that this approach—which pairs skilled annotators with ML-powered automation—significantly increases labeling efficiency and quality.\n\nLet’s see what that looks like in practice, using an example from the Autonomous Vehicle industry.\n\nMachine-Assisted Polygon Annotation\n\n\n\nWhen an annotator logs into the Sama annotation platform, they are presented with this workspace. In this example, the workspace is customized to allow the annotator to draw instance segmentation polygons around each of these vehicles:\n\n\n\nYou’ll notice that there are several vehicles in this image. In a manual context, it could take a human several hours to deliver high-quality annotations of every single vehicle:\n\nWhat the manual annotation process would look like (sped up significantly): several clicks are required to draw a polygon around each of the vehicles.\n\n\n\nThis process is significantly accelerated with Machine-Assisted Polygon Annotation.\n\nThe model allows the annotator to use a crosshair tool to identify only four extreme points: left, right, top and bottom boundaries. These four clicks are the only inputs needed to create a heat map that is then sent to the inference server, returning an accurate prediction of a raster mask.\n\n\n\nWith Machine-Assisted Polygon Annotation, annotators only need to perform four clicks to produce an accurate raster mask prediction.\n\n\n\nMachine-Assisted Polygon Editing\n\n\n\nA polygon prediction can then be further refined by an annotator by switching into editing mode. This enables annotators to label precisely and ensure that high-quality requirements are met without compromise.\n\n\n\nIn this example, the raster mask prediction is edited by the annotator to ensure precise and high-quality labels.\n\n\n\nThis mode also enables annotators to use more than four extreme points in order to produce even more accurate predictions. A fifth user input point can easily be added, with the model immediately incorporating the new input to update its prediction.\n\nIf an ML model struggles to identify specific shapes, annotators can add a few more inference points to help result in a more accurate prediction, and then refine that prediction manually to ensure high-quality labels.\n\nResults from ML Assisted Annotation powered by MICROMODEL technology\n\n\nOur clients are already seeing impressive results from MAA powered by MICROMODEL technology:\n\nPredictably producing 94-98% IOU (Intersection over Union) accuracy\nBecause our models are pre-trained on specific use cases for better performance out of the gate, our clients are seeing a quicker time to accuracy.\n\n2-4x more efficient annotation process\nYou can clearly see above that using MAA over a more manual polygon labeling approach results in significant time savings. But it’s also an iterative process with a human annotator in the loop; modifications to the predictions get fed back into the training data pipeline to retrain the model, enabling it to perform better predictions over time.\n\n\nQuicker time to market\nThe end result for our clients is faster iterations and a quicker time to market. A more efficient annotation process results in more data returned quickly, and ultimately a significantly shorter path to production.\n\nWhat’s more: increasing the efficiency of this labor-intensive manual data annotation process reduces the barrier to entry for more ML teams... and not just those with large R\u0026D budgets. Technology like this can also help democratize data labeling by driving down cost, so we can see even more deserving companies leverage AI to drive value for their business.\n\nSmall teams who are getting started with labeling may not have yet defined what type of annotations they need, or how much data they need to be successful. MAA can help them iterate more quickly, developing models in short increments rather than in large, cumbersome workstreams. The end result is a quicker time to value, and ultimately, to market — for organizations of all shapes and sizes.\n\nLearn more about ML Assisted Annotation powered by MICROMODEL technology by watching our recent webinar here, or reading more about it here.","slug":{"_type":"slug","current":"ml-assisted-annotation-micromodels"},"tags":[{"_key":"giPx682h","label":"Machine Learning","value":"Machine Learning"},{"_key":"JOgmW0AV","label":"Data Annotation","value":"Data Annotation"},{"_key":"45fExC0n","label":"Autonomous Transportation","value":"Autonomous Transportation"}],"title":"ML Assisted Annotation Powered by MicroModels"},{"_createdAt":"2021-03-23T19:00:21Z","author":{"_id":"4e2e7cef-d6eb-4bb7-bd39-375c6299677e","avatar":{"_type":"image","asset":{"_ref":"image-f1a274bcfdb5e70d814f1bab2b6bbd644728e9be-1480x1462-jpg","_type":"reference"}},"bio":"With a background in Computer Science, Abha leads the Customer Success Engineering team at Sama. The team is responsible for managing technical relationships with customers and prospects to understand their business needs, ideate upon them, and manage the implementation and communication of the solutions developed. ","name":"Abha Laddha","slug":{"_type":"slug","current":"abha-laddha"}},"config":{"description":"Sama is an expert in efficiently designing annotation guidelines that enhance data quality. Gold Tasks refer to tasks that have been annotated perfectly.","openGraphImage":null,"title":"Sama's Gold Tasks: ML Training Data with Gold-Standard Quality"},"estimatedReadingTime":4,"featured_image":{"_type":"image","asset":{"_ref":"image-2be5ee7e7ae0847f3bedec01bb88266c371e3eb9-4000x2250-jpg","_type":"reference"}},"plaintextBody":"Several methods exist to help companies define and measure data quality. To define the correct annotation of given data, you want to start by creating annotation guidelines. On top of proposing a multi-level quality checks system, Sama’s experts have built unique know-how in efficiently designing annotation guidelines that enhance data quality.\n\nCue Gold Tasks; referring to tasks that have been annotated “perfectly” or meet the “gold standard”. Such tasks are often used by the client to communicate their expectations around precision and quality used as examples during training. At Sama, we use Gold Tasks in two more ways: During training, to assess annotators and identify those who are ready to move into production and during production to generate an automated metric on quality.\n\nGold Assessments\n\nEvery project launch at Sama is accompanied with a period of training, where annotators focus on requirements for the specific workflow, familiarizing themselves with the taxonomy, accuracy levels needed, and edge cases. Gold tasks come in as they move from classroom training to practice tasks.\n\nA gold set is created that is representative of the overall complexity of the dataset, ensuring a healthy mix of the edge cases. annotators practice on this set of which we already have “gold” answers. As each task is submitted we are able to compare the annotator answers with the gold task, generating custom metrics and error tags. The metrics are determined by the type of workflow and tool used, for e.g. in a semantic segmentation workflow, we focus on IoU calculations per label and depending on the client rubric each label may be weighed differently.\n\nThese metrics are then used to create trends and analyze each annotator’s performance individually, and provide relevant feedback. We are able to analyze trends at an asset label (which type of images are more difficult than others?), annotator level (which annotator is struggling exactly where?), and the impact of time (was today better than yesterday?).\n\nGold assessments, therefore, help us accelerate training by providing customized feedback to each annotator early on and allowing us to track their improvement over time. This enables Sama to quickly identify doubts, find edge cases, and have high confidence that annotators are ready to move on to production. Lastly, this allows us to calibrate and train the manual QAs on the specifics of this particular workflow, ensuring that nothing is missed.\n\n\nGold Metrics\n\nSimilar to gold assessments, gold metrics compare an annotator’s tasks against a known, completed task. These tasks, however, are interspersed within the production queue with the annotators unaware that these are gold tasks. These tasks then act as tests for the annotators, generating similar metrics as mentioned above. This allows the team to report upon the annotator’s performance against the gold tasks that increase insight into quality and help to further tailor training and coaching.\n\nGold metrics are most useful for clients looking to automate the quality loop on their side. Given that their Sama project team consistently samples and approves only high quality tasks, it is a neat way for them to save time and capacity.\n\nBecause no two AI projects are alike, you need to make sure that your quality assurance (QA) process is designed to meet the unique needs of your particular project. Learn more on how to supercharge your data quality with Sama's Automated Quality Accelerators. ","slug":{"_type":"slug","current":"sama-gold-tasks"},"tags":[{"_key":"YSoeJ1Gj","label":"Product","value":"Product"},{"_key":"AHeszSpi","label":"Training Data","value":"Training Data"},{"_key":"9nTBOCjn","label":"Data Annotation","value":"Data Annotation"},{"_key":"ku9OMDnM","label":"Data Quality","value":"Data Quality"}],"title":"Sama's Gold Tasks: ML Training Data with Gold-Standard Quality"},{"_createdAt":"2020-04-23T21:37:48Z","author":{"_id":"26fb3cc4-608d-40e1-bb4d-955bceda232a","avatar":{"_type":"image","asset":{"_ref":"image-f1fd7fbcc4633299cdbedddba22cb44e24f17317-518x518-svg","_type":"reference"}},"bio":"From self-driving cars to smart hardware, Sama fuels AI. Founded over a decade ago, we’re experts in image, video and sensor data annotation and validation for machine learning algorithms in industries including automotive, navigation, AR/VR, biotech, agriculture, manufacturing, and e-commerce. Our staff are driven by a mission to expand opportunity for low-income people through the digital economy, and our social business model has helped over 50,000 people lift themselves out of poverty.","name":"Sama Team","slug":{"_type":"slug","current":"sama-team"}},"config":{"description":"The traffic light problem for autonomous vehicles is critical for all vehicle safety, and unlike human-drivers, AVs rely solely on computer vision systems to navigate the world around us.","openGraphImage":null,"title":"The Traffic Light Problem for Autonomous Vehicles"},"estimatedReadingTime":5,"featured_image":{"_type":"image","asset":{"_ref":"image-04a6a4a0b08c6631b9ea6592f4d5e29e4ca09ba5-4496x3000-jpg","_type":"reference"}},"plaintextBody":"Solving the traffic light problem for autonomous vehicles is critical for all vehicle safety, not just autonomous vehicles. And unlike human-driven cars, AVs rely solely on their computer vision system and the data used to train them to navigate the world around us.\n\nCurrently, the best self-driving assistance systems incorrectly perceive something in their environment once every tens of thousands of hours. If that object is a traffic light, and the car gets it wrong, passengers, pedestrians, cyclists, etc., are all at risk. Here’s a look at the traffic light problem for autonomous cars from three perspectives. \n\n\n\n1. Traffic lights tell road users when to stop and go, but they only work when everyone follows the rules.\n\nTraffic lights are an engineered system, timed around traffic patterns. There are rules in place that tell cars when to stop and go, but the inherent human behavior of drivers, pedestrians and cyclists sometimes means these rules are loosely interpreted.\n\nThere are no physical barriers forcing road users to abide by traffic signals. They work because drivers follow the rules. AVs must also get the rules right, and that means countless hours of real-world exposure to the unspoken rules (or lack thereof) of the road.\n\nThe quality and accuracy of the data received from sensor packages must be precise, leaving no room for interpretation or inconsistencies. Also, license plates, faces or other personal identifying information (PII) may need to be anonymized to protect the privacy of people who might appear in the raw footage. \n\nEnsuring AVs learn the right rules requires unbiased, appropriately labeled and high-quality training data based on a range of driving scenarios.\n\n\n\n2. Traffic lights challenge both the vision system and the team developing the algorithms.\n\nBecause traffic lights are not a distance detection problem, AVs cannot use lidar or radar to navigate traffic signals. They must rely solely on their computer vision system to understand when to stop and go.\n\nThis can be difficult for both the vision system as well as the team developing the algorithm because the visibility of traffic lights may vary based on weather conditions like bright sunlight, rain, snow or fog. Similarly, not all intersections have traffic lights, so if the AV doesn’t detect one, that could be correct.\n\nThe treatment of out of service traffic signals can also vary. One city might use a plastic bag to indicate an out-of-service light while another city, or even county, might use masking tape or some other method to cover broken traffic signals.\n\nContext clues like head nods or hand signals help human drivers manage low visibility or the lack of a traffic light, but since AVs cannot register this supplemental visual information, machine learning and computer vision engineers must train the AV on such scenarios as they arise.\n\n3. Stock datasets aren’t enough to help AVs navigate traffic lights safely.\n\nThe volume and diversity of data required for autonomous vehicles is vast, and stock datasets cannot cover all use cases.\n\nDuncan Curtis, Sama VP of Product shares approximately 1,000 to 10,000 images are needed to deploy a solution onto a vehicle, and in order to launch a product publicly, approximately 10,000 to 100,000 images are needed per ADAS feature. That’s a lot of data, and the amount of data needed to account for edge cases like traffic lights in unique road conditions is unknown.\n\nSince AVs often need data from a specific camera or sensor package (because that’s what the car will use in production), generic stock data isn’t enough to help AVs navigate traffic lights safely. In addition to this, models need to be refreshed with ongoing training data as road rules, and the world around us changes.\n\nOur team of data labeling experts annotate over 4M tasks per month, and for AV use cases, we’ve achieved 99.5 percent quality SLAs with more than 100 tags per image. We’ve also successfully achieved full productivity within our labeling teams in as little as two to five days.\n\nIf you need ground truth data for your ML model, connect with our team for a virtual consultation and demo of our industry-leading data annotation platform.","slug":{"_type":"slug","current":"the-traffic-light-problem-for-autonomous-vehicles"},"tags":[{"_key":"ivLPY6Ly","label":"AI","value":"AI"},{"_key":"Liw4Inz5","label":"Training Data","value":"Training Data"},{"_key":"3LVFK315","label":"Data Annotation","value":"Data Annotation"},{"_key":"YhUSiZyI","label":"Data Quality","value":"Data Quality"},{"_key":"72HsQzsx","label":"Autonomous Transportation","value":"Autonomous Transportation"}],"title":"The Traffic Light Problem for Autonomous Vehicles"},{"_createdAt":"2020-04-02T20:30:00Z","author":{"_id":"6dd25391-26af-42ff-a2a2-f0778ce1b002","avatar":{"_type":"image","asset":{"_ref":"image-4e1c47984bb16cb0370855b6272663d6744216d3-450x450-jpg","_type":"reference"}},"bio":"Currently a Senior Product Manager at Sama, Audrey guides cross-functional teams to create thoughtful product solutions. She has guided teams of designers and engineers at HUGE Inc. and NBCUniversal, and monitored user analytics at the Wall Street Journal. With a BA in history from Harvard, an MA in anthropology from Columbia and an MBA from UNC Chapel Hill KFBS, Audrey is passionate a using technology and data analytics facilitate social impact and environmental solutions through technology.","name":"Audrey Boguchwal","slug":{"_type":"slug","current":"audrey-boguchwal"}},"config":{"description":"SamaHub's video and 3D object tracking with frame level labeling assists companies in quickly building models that better reflect real-world behavior.","openGraphImage":null,"title":"Object Tracking with Frame Level-Labeling"},"estimatedReadingTime":3,"featured_image":{"_type":"image","asset":{"_ref":"image-7eb4854393afff7be19269b7e8414936e411403f-5506x3671-jpg","_type":"reference"}},"plaintextBody":"We live in an ever-changing world, where AI-enabled technology has become a new normal for society. To assist top organizations in their efforts to build smarter computer vision algorithms, we’ve rolled out a new release for video and 3D object-tracking in our leading data annotation platform.\n\nThere are a number of applications that use computer vision to track how the world, and the objects in it, change overtime. For a self-driving car to navigate safely, it needs to track other moving objects on the road and make predictions about their future movement, so it can plan its driving path.\n\nAR and VR applications like video games need to track the motion of individual people to create a seamless digital experience. These vision applications have something in common: they all seek to understand the change in position, behavior and characteristics of unique objects over time.\n\nObject tracking annotation offers object tracking capabilities for complex scenarios, including path planning, traffic light status, sentiment analysis, etc.\n\nThis frame-level labeling technology allows unique objects to be dynamically tracked across a video or a sequence of 3D point cloud data from a Lidar sensor. Change in position and pose are captured with annotation shapes like cuboids, polygons and bounding boxes.\n\nSama supports custom label taxonomies for both the main object class (person, car, etc.) and dynamic labeling for other object characteristics that change over time, such as visibility percentage of an object or a specific set of characteristics like emotions. Sama’s built-in automated interpolation between video frames helps ensure efficient, high quality labeled training data for a variety of object tracking use cases.\n\nFor over 10 years, Sama has delivered turnkey, high-quality training data and validation to train the world's leading AI technologies. Video and 3D object tracking are no exception, and this update for video object tracking annotation in 2D RGB video and 3D Lidar data will continue to assist organizations in quickly building models that better reflect real-world behavior.\n\nSama has deep expertise working with training data for object tracking use cases across a variety of industries including autonomous vehicles, AR/VR, retail and e-commerce, communications, media and entertainment, to name just a few.\n\nDownload our solution brief to learn more about our secure training data annotation platform, or contact our team here.","slug":{"_type":"slug","current":"object-tracking-in-samahub-with-frame-level-labeling"},"tags":[{"_key":"OjIX5QKU","label":"Product","value":"Product"},{"_key":"2ZGxEWq0","label":"Video Annotation","value":"Video Annotation"},{"_key":"iBmdS1VE","label":"Training Data","value":"Training Data"},{"_key":"uZYhm6jx","label":"Data Annotation","value":"Data Annotation"}],"title":"Object Tracking with Frame Level-Labeling"},{"_createdAt":"2020-02-28T21:00:00Z","author":{"_id":"b4d4d096-8187-41fa-a386-87ce949c9915","avatar":{"_type":"image","asset":{"_ref":"image-45005425be54d2d450b1aa53f7d9435477531eb3-1664x1758-jpg","_type":"reference"}},"bio":"Liliosa is the Impact and Marketing Manager at Sama.","name":"Liliosa Mbirimi Muturi","slug":{"_type":"slug","current":"liliosa-mbirimi-muturi"}},"config":{"description":"As AI adoption expands, untapped communities are finding work at the cutting edge of AI. Here are 4 ways AI makes a positive impact on communities in East Africa.","openGraphImage":null,"title":"4 Ways AI Makes a Positive Impact on Communities in East Africa"},"estimatedReadingTime":6,"featured_image":{"_type":"image","asset":{"_ref":"image-7e68dc7b894694d301fe5feff694b3df88f8beda-2048x1365-jpg","_type":"reference"}},"plaintextBody":"In a small town in Northern Uganda, Ocem turns on his lights at 6am to get ready for work. It’s his second year working at Sama, and before becoming an AI trainer, he was a student at the local university, moonlighting as a farmer.\n\nWorking in AI was not a path Ocem imagined until he was recruited to work at Sama. In fact, like many students, his greatest uncertainty was whether he could put his degree to good use at all, post graduation.\n\nOcem’s story is like many other young professionals in East Africa, but as AI adoption explodes, these talented, untapped communities are finding work they never thought possible, at the cutting edge of AI.\n\nHere are four ways AI makes a positive impact on communities in East Africa.\n\nGainful Employment\n\nA report by the World Bank shared that 60 percent of the unemployed in Africa are youth, who after graduating from University will take on average, one year to find employment in their field of study. \n\nI believe this is due to the lack of formal career opportunities within the region, making it difficult for the employment industry to support the volume of college graduates entering the workforce.\n\nAs a result, talented graduates find work in menial jobs that often don’t provide a sustainable income, but Sama and other tech companies are working to fix that.\n\nThe explosion of AI has birthed the demand for high-quality, ground truth data for artificial intelligence, and with training, recent graduates like Ocem can find meaningful work in the growing digital economy.\n\nDriven by a mission to expand opportunity for low-income people, Sama's social impact business model has helped over 50,000 people move themselves out of poverty through digital work.\n\nWorkers in our East Africa centers are paid a living wage and competitive benefits. They also receive ongoing digital skills training and have access to professional development opportunities that further help increase the purchasing power of their communities, potentially ending a long cycle of poverty.\n\nSkills and Knowledge\n\nAs more tech companies expand operations to Africa, and more people start gaining on-the-job experience working in machine learning and AI, the positive impact AI makes on communities in East Africa will only expand.\n\nI’ve personally witnessed how energized the youth in East Africa are about the intricacies of enriching data for AI. For many, this budding tech boom has motivated them to enroll in courses centered around data sciences and AI.\n\nAlso, the Deep Learning Indaba, an annual meeting for AI enthusiasts in Africa has brought together a growing number of youths for week long discussions, debates and research on machine learning and AI.\n\nWith a reliable IT infrastructure, paired with increased trust in the skill level of youth workers in the region, East Africa has the potential to become a major hub for AI training data.\n\nAt Sama, our AI training classes fill up weekly, and this level of investment in skills development for young professionals is what’s needed to continue developing the talent pool. \n\nDiversity and Inclusion\n\nUnesco reported that 30% of the tech-workforce in sub-Saharan Africa are women, and despite the challenges of gender discrimination in the technology sector, the percentage of female workers is steadily growing. \n\nWomen in Africa have often been set apart from the tech industry because culturally, they were considered primary caretakers, expected to focus on their family, not their career. Now, the vast opportunities in AI are helping to change that.\n\nWe actively recruit women and youth to work in our East Africa centers, providing in-depth technical training, so underserved communities can pursue a career in AI. From scholarship programs to nursing rooms, we’ve also made it a priority to establish an office culture conducive to the success of women workers.\n\nOur baseline survey data found that women earned 70 cents for every dollar earned by men, before joining Sama. By ensuring we achieve gender parity in pay, while simultaneously paying a living wage, our workers can support themselves and their families sustainably.\n\nData Privacy and Protection\n\nAs more populations in Africa adopt mobile technology and digital apps, their exposure to AI-enabled technologies that collect data to improve experiences will only increase. These interactions can be as simple as product recommendations from an online store, or interactions with an automated messenger bot.\n\nGiven the rapid advancement of AI and the Internet of Things (IoT) , the Kenyan government recently passed a data protection law which complies with the European Union’s General Data Protection Regulation (GDPR). \n\nSimilar to GDPR, this legislation outlines restrictions on data handling and sharing, in an effort to regulate the processing of personal data and information in the country.\n\nThis commitment to increased digital security will no doubt have a positive impact on the citizens of Kenya, and in many ways, it sets the standard for data privacy and protection across the continent.\n\nMcKinsey Global Institute predicts that AI has the potential to deliver an additional economic output of $13 trillion by 2030. This economic growth, alongside the digital transformation of sub-Saharan Africa will have enormous positive impact on communities in East Africa.\n\nIf you’re interested in learning more about how Sama connects people to dignified digital work, you can read about our social business model here.","slug":{"_type":"slug","current":"4-ways-ai-makes-a-positive-impact-on-communities-in-east-africa"},"tags":[{"_key":"AEPEpJXp","label":"Ethical AI","value":"Ethical AI"},{"_key":"tc6Hsfbm","label":"AI","value":"AI"},{"_key":"VmY4sBKG","label":"Impact","value":"Impact"},{"_key":"iLYfly0n","label":"Data Annotation","value":"Data Annotation"}],"title":"4 Ways AI Makes a Positive Impact on Communities in East Africa"},{"_createdAt":"2019-07-11T22:00:00Z","author":{"_id":"6dd25391-26af-42ff-a2a2-f0778ce1b002","avatar":{"_type":"image","asset":{"_ref":"image-4e1c47984bb16cb0370855b6272663d6744216d3-450x450-jpg","_type":"reference"}},"bio":"Currently a Senior Product Manager at Sama, Audrey guides cross-functional teams to create thoughtful product solutions. She has guided teams of designers and engineers at HUGE Inc. and NBCUniversal, and monitored user analytics at the Wall Street Journal. With a BA in history from Harvard, an MA in anthropology from Columbia and an MBA from UNC Chapel Hill KFBS, Audrey is passionate a using technology and data analytics facilitate social impact and environmental solutions through technology.","name":"Audrey Boguchwal","slug":{"_type":"slug","current":"audrey-boguchwal"}},"config":{"description":"Samasource's revamped toolset for 2D image vector segmentation is ideal for computer vision projects using vector shapes to structure training data.","openGraphImage":null,"title":"Revamped 2D Vector Segmentation"},"estimatedReadingTime":2,"featured_image":{"_type":"image","asset":{"_ref":"image-31269a4f01b8e495d1ec30a473f063a4957254c6-5760x3840-jpg","_type":"reference"}},"plaintextBody":"Sama is pleased to announce production availability of our revamped toolset for 2D image vector segmentation using bounding boxes, polygons and lines.\n\nThe vector toolset is ideal for computer vision projects using vector shapes to structure training data for detection, classification, labeling and object tracking projects.\n\nThe new vector toolset is an update to our previous offering for annotating 2D images. The new tools contain UX enhancements and new features designed to further improve annotation efficiency and accuracy for our expert workforce.\n\nHoused in Sama’s proprietary platform, the toolset leverages existing Hub capabilities for work prioritization and quality management, to deliver the highest quality training data every time.\n\nQUALITY\n\nThe new vector toolset is optimized for quality. The annotation workspace has a larger image display and more precise drawing tools enabling Sama’s workforce to annotate with an even greater degree of accuracy.\n\nThe toolset gives individual annotators direct access to image and shape adjustment options (such as shape opacity), instead of restricting these options to the admin control panel.\n\nNow, individual annotators can adjust settings to get the best view of every unique image while annotating.\n\nEFFICIENCY\n\nThe toolset is also optimized for efficiency. The larger image, new keyboard hotkeys and more intelligent zoom features help reduce time spent panning and adjusting an image.\n\nWhat’s more, the toolset is built on a modern codebase that helps streamline development for new features built by Sama’s dedicated engineering team.\n\n\n\nCONSIDERING A 2D VECTOR SEGMENTATION PROJECT?\n\n\nFrom autonomous vehicle applications to high tech e-commerce initiatives, Sama has the annotation technology to put your project on track for success. Drop us a line, and request a demo of the new toolset in action.","slug":{"_type":"slug","current":"revamped-2d-vector-segmentation-on-samahub"},"tags":[{"_key":"oc26gThR","label":"Product","value":"Product"},{"_key":"8Uhzi08N","label":"Vector Annotation","value":"Vector Annotation"},{"_key":"W46d0x8z","label":"Training Data","value":"Training Data"},{"_key":"NwhOkizM","label":"Data Annotation","value":"Data Annotation"},{"_key":"46Go0q8m","label":"Data Quality","value":"Data Quality"}],"title":"Revamped 2D Vector Segmentation"},{"_createdAt":"2019-06-12T20:00:00Z","author":{"_id":"97dc2368-fffb-4c41-82aa-5a9cbe2ec670","avatar":{"_type":"image","asset":{"_ref":"image-af58425525bb33d8cffdc1f1b10f02bf1e4faf57-1916x2028-jpg","_type":"reference"}},"bio":"Sharon is the Content Marketing Manager at Sama where she's responsible for telling the story behind the company's impact sourcing mission and human-powered training data solutions. Sharon holds a MS in Integrated Marketing Communications and is passionate about helping social enterprises transform abstract concepts into results-driven marketing.","name":"Sharon L. Hadden","slug":{"_type":"slug","current":"sharon-l-hadden"}},"config":{"description":"Here are 7 organizations attending CVPR 2019 who are leading the way in computer vision, plus 3 noteworthy companies from around the web.","openGraphImage":null,"title":"10 Organizations Leading the Way in Computer Vision"},"estimatedReadingTime":4,"featured_image":{"_type":"image","asset":{"_ref":"image-a49d2248810c7d2141277314bfa9b78a95c2a9a3-2250x1500-jpg","_type":"reference"}},"plaintextBody":"Since 1983, the Computer Vision and Pattern Recognition (CVPR) conference has brought together industry-leading academics, researchers and companies, and CVPR 2019 is no exception.\n\nHere are seven organizations attending CVPR who are leading the way in computer vision, plus three noteworthy companies from around the web.\n\n\n\n\nAdobe Systems Inc. Last year, Adobe Research contributed 30 papers to CVPR 2018. Among the papers contributed to CVPR 2019 are Photmetrix Mesh Optimization for Video-Aligned 3D Object Reconstruction and Texture Mixer: A Network for Controllable Synthesis and Interpolation of Texture.\n\nAurora Innovation Founded by three scientists who are no stranger to autonomous vehicles, Chris Urmson, Sterling Anderson and Drew Bagnell of Aurora Innovation recently announced a deal with Fiat Chrysler to deploy self-driving car technology in its commercial vehicles.\n\nBaidu A Google search for “Baidu computer vision” yields countless scholarly articles. Baidu’s visual tech team took home first place in the ICME 2019 Grand Challenge of 106-p Facial Landmark Localization and continues to explore topics like temporal modeling approaches and sensor fusion.\n\nHyperSense HyprSense’s vision intelligence and human sensing technology for live 3D animation captures and conveys facial expressions in real-time. They’ll be exhibiting their motion capture software at CVPR 2019 near the Gaming Zone.\n\nOpenAI Nonprofit lab, OpenAI’s mission is to ensure artificial intelligence benefits all of humanity. They’ve partnered with companies like Google to visually map how AI systems understand the world around us, and their website features a resource section with free software for experimenting with AI.\n\nSama Sama delivers high quality training data at scale using a combination of our cloud annotation platform and a managed workforce for ethical human-powered training data and validation.\n\nWe’ll be at CVPR 2019 at booth #1561 to answer all your training data questions. Also, don’t miss the Fine-Grained Visual Categorization (FGVC) workshop on June 17. We collaborated with Cornell Tech to produce the fashion dataset being used in the competition.\n\nTwentyBN Featured on VentureBeat’s list of top AI companies in the world, TwentyBN develops technologies that “allow machines to interact with humans the way humans do.” The avatars and AI assistants made possible by TwentyBN put an intelligent spin on automating the retail experience.\n\nValeo Valeo’s innovative technologies help automakers reduce CO2 emissions. The company has built a strong portfolio of technologies and products, including driving assistance systems that use computer vision to fully automate parking, detect objects around the vehicle, etc.\n\nWayfair This may come as a surprise, but Wayfair’s technology blog includes anecdotes and explanations from their data science team. It gives insight to new projects at Wayfair like training image synthesis and reminds us that the use case for computer vision in retail and e-commerce will only continue to grow.\n\nXnor Xnor is helping companies adopt AI at the edge with hardware and software equipped to enable deep learning models on less than power than your phone.\n\nComputer vision often brings about daydreams of driverless cars and facial recognition, but as you can see, there’s more to it than only these applications.\n\nWhat companies are you keeping a close watch on? Leave a comment and let us know.","slug":{"_type":"slug","current":"10-organizations-leading-the-way-in-computer-vision"},"tags":[{"_key":"pm9yH2PJ","label":"Best of","value":"Best of"},{"_key":"YwMfUUBk","label":"Data Annotation","value":"Data Annotation"},{"_key":"NsG61NKf","label":"Computer Vision","value":"Computer Vision"}],"title":"10 Organizations Leading the Way in Computer Vision"},{"_createdAt":"2018-12-14T19:54:00Z","author":{"_id":"71091c91-664a-44a6-9474-acc40eb12457","avatar":{"_type":"image","asset":{"_ref":"image-bc776336801adf71e2599337e8d6f02186b109d0-500x500-jpg","_type":"reference"}},"bio":"Matthew leads the product team at Sama, responsible for the platform that enables Sama's AI/ML data enrichment teams, internal enterprise operations tools to ensure quality and scalability, and all new product initiatives for the evolution of algorithm development and human-powered automation.","name":"Matthew Landry","slug":{"_type":"slug","current":"matthew-landry"}},"config":{"description":"Training your AI in 3D","openGraphImage":null,"title":"Training Your AI in 3D"},"estimatedReadingTime":8,"featured_image":{"_type":"image","asset":{"_ref":"image-9a175242c5b6d9d34ab8d001420280c4279ffe11-1125x1500-jpg","_type":"reference"}},"plaintextBody":"The world has three dimensions. Why shouldn't your AI?\n\nToday, we're announcing the production availability of our new 3D annotation engine for the Sama. This new offering allows our expert annotation team to nimbly explore your lidar point clouds, searching for objects of interest and producing the highest quality 3D labels.\n\nAs much as we love a good \"3D\" joke, we can't leave out the important 4th dimension -- time. Building on the object tracking expertise from our video annotation toolkit, we support lengthy sequences of point cloud data, tracking moving objects through time and space.\n\nWAIT, WHAT ARE POINT CLOUDS?\n\nBefore we get too deep into the weeds on 3D annotation, let's take a step back and talk about 3D point clouds and why they are important.\n\nThroughout the history of computer vision technology, two-dimensional camera images have dominated. Techniques to detect edges and extract objects from the background, approaches to estimate relative distances in order to construct implied dimensions, and carefully-calibrated dual-camera captures that triangulate all incrementally nudged forward a machine's ability to \"see.\"\n\nAdvances in deep neural nets dramatically improved computer vision with a greater degree of robustness -- and kicked off the current race to bring self-driving cars to market.\n\nCars are heavy and filled with combustible material. They travel at high velocities. They share the same space as other hurtling vehicles, ambling humans, and wandering animals. This makes the perception of distance absolutely critical. Recognizing an obstruction in the road is step 1; step 2 is estimating distance and making the appropriate response (slam on the brakes? maneuver safely around?).\n\nHence, lidar technology has been rapidly adopted as the key enabler for self-driving cars. Kyle Vogt, CEO of GM Cruise, says, “sensors are a critical enabler for deploying self-driving cars at scale, and LIDARs are currently the bottleneck.” Rapid innovation to make lidar sensors faster, smaller, and cheaper with higher density, higher accuracy data make adoption near inevitable.\n\nWhat does the data look like? Rather than a 2D image displayed on a flat screen, a lidar sensor generates a 3D snapshot of the surrounding environment. It scans the area with pinpricks of light, measuring the time-of-flight of reflections, and thus precisely capturing a collection of (X, Y, Z) points. We can reconstruct this to display on our screens, much like an Xbox game.\n\nThe richness and fidelity of this type of data feeds a new generation of AI algorithms (e.g., see this paper about deep neural nets with direct voxel inputs by Apple), making depth perception much easier.\n\nMUCH MORE THAN SELF-DRIVING CARS\n\nIt's not just self-driving cars. 3D point clouds power a variety of machine vision use cases.\n\nWhile an autonomous vehicle may use a technique like SLAM to dynamically build a 3D map of the environment, providers of high fidelity mapping data collect their own point clouds so that they can provide the ground-truth reference map. For example, providers like HERE could build 3D into their HD Localization maps.\n\nDelivery robots are another exciting use case for lidar sensor based perception. The navigation concepts are very similar to automotive, but many delivery robots can more easily traverse sidewalks and coexist with pedestrians.\n\nIn a warehouse context, robots with 3D lidar sensing are able to shift materials around, rapidly picking up and transporting pallets to reconfigure the warehouse or optimize manufacturing workflows. Collision avoidance in the presence of human operators as well as indoor navigation and mapping round out the common ways that 3D point cloud data power cognitive robots.\n\nFinally, unmanned aerial vehicles -- otherwise known as drones -- can use lidar to collect high resolution 3D data, such as when \"corridor mapping\" a pipeline, surveying a construction site, or performing visual structural inspections.\n\nEmpowering the Human Element\n\nSama has extensive project experience annotating lidar sensor data from cutting-edge instruments. While we quickly ramped projects to produce those annotations on our clients’ own in-house apps, we wanted to create a tool flexible enough for the broader market.\n\nThat's why we're launching our 3D annotation tool that supports a wide range of 3D data types from essentially all lidar sensors. We have likely worked with sensor data from your lidar vendor of choice.\n\nThere's more to it beyond the raw data.\n\nThe Sama mission is to use human ingenuity to accelerate AI development, and one of our favorite tricks is to use algorithms to assist and interact with our expert human annotators. Our new 3D tool has some powerful time-saving features such as:\n\nautomatic cuboid estimation to quickly capture an entire object after clicking only a single point;\n\nautomatic ground-plan extraction to make objects and environs easier to discriminate;\n\nsynchronized camera images for 2D confirmation of the 3D points; and\n\nobject tracking estimation to accelerate annotation and predict movement even when objects are occluded or only partially visible\n\nThis approach lets us combine the skills of expert annotators, who can cleverly correlate what a camera shows with even sparse point clouds, with algorithms to accelerate the manipulation of 3D annotations in space and time. Together, we rapidly produce ground-truth annotations of the highest quality to power 3D perception.\n\nThe bottom line: our clients get to market faster with their latest 3D perception algorithms. Quality and throughput make a difference.\n\nExperience Matters\n\nWe have run many projects through our teams, and consistently produce the quality results made possible only by long-term, dedicated annotation experts. The Sama model allows our agents to concentrate on a single client's data, understanding the nuances and unique requirements of their machine learning team (e.g., how precisely to enclose an object, whether to include side-view mirrors or bicycle racks, how to treat objects that pass out of view, what constitutes sufficient visual contact, and so on).\n\nOne project included side-mounted lidar sensors that produced data rotated by 90 degrees. Rather than expecting our team to annotate with their necks tilted at an angle, our engineering team whipped up a quick \"rotate camera\" tool to quickly reorient the workspace. Between continuous improvements of our Hub annotation studio, iterative refinement of the project objectives and annotation goals, and experience with point cloud data from nearly every lidar sensor out there, there's a bright future in 3D annotation here at Sama.\n\nPartnering with Sama, you can get the most from your 3D lidar projects. We’re incredibly excited about this production-ready 3D annotation tool and the future of high-performance lidar. If you have 3D annotation on your mind and would like to see a demo of our annotation platform in action: Drop us a line! ","slug":{"_type":"slug","current":"training-your-ai-in-3d"},"tags":[{"_key":"C3ND2OEf","label":"Product","value":"Product"},{"_key":"dJfUvcIY","label":"Training Data","value":"Training Data"},{"_key":"Se91A2Xp","label":"Data Annotation","value":"Data Annotation"}],"title":"Training Your AI in 3D"},{"_createdAt":"2018-07-17T00:00:00Z","author":{"_id":"71091c91-664a-44a6-9474-acc40eb12457","avatar":{"_type":"image","asset":{"_ref":"image-bc776336801adf71e2599337e8d6f02186b109d0-500x500-jpg","_type":"reference"}},"bio":"Matthew leads the product team at Sama, responsible for the platform that enables Sama's AI/ML data enrichment teams, internal enterprise operations tools to ensure quality and scalability, and all new product initiatives for the evolution of algorithm development and human-powered automation.","name":"Matthew Landry","slug":{"_type":"slug","current":"matthew-landry"}},"config":{"description":"Announcing object tracking with video annotation","openGraphImage":null,"title":"Introducing Object Tracking with Video Annotation"},"estimatedReadingTime":5,"featured_image":{"_type":"image","asset":{"_ref":"image-af58edf394d9676203b9bc44284f08ccf52125a6-1000x1500-jpg","_type":"reference"}},"plaintextBody":"Today, Sama announces the availability of our latest image annotation toolset for advanced video object tracking. These new tools, in the hands of our expert annotation workforce, level up Sama’s object tracking performance while maintaining the same extreme quality results of our ground truth training data services.\n\nWhat this means for our customers is an even more scalable approach to annotating the growing stream of video object tracking data. Faster training data production speeds your algorithm development and gets you to market faster.\n\nWhy focus on video object tracking?\n\nTesla, as an example, has over 250,000 cars on the road, each packed with high quality cameras to capture the world around them. Video footage collected from a fleet of this size can feed an extremely sophisticated autonomous driving deep learning system. And it's not just Tesla. In the Bay Area, we've become accustomed to seeing data capture vehicles from just about every autonomous driving company out there -- and all of them are collecting video.\n\nAs the computer vision industry progresses from simple object identification (can the algorithm tell what an object is?) to object tracking (can the algorithm follow a specific object over time?), we need tools that can effortlessly annotate this video stream. Sama delivers.\n\nWhat difference does a tool make?\n\nThe traditional approach to an object tracking project is to split the video into individual images and then annotate each image separately, paying careful attention to ensure consistent identifiers for each unique object in sequential images. It's very challenging work, as any Sama agent or quality analyst will tell you. It takes careful attention to detail and often exceeds the capabilities of most annotation services. (We had to build some supporting tools in our platform to make it tractable.)\n\nSama’s introduction of video annotation for object tracking completely changes the game. Now, an entire video sequence can be assessed as a whole, whether the clip contains 2 frames or 2,000 frames. This feature makes it much easier and faster to follow a single object -- even if it's moving -- from beginning to end of a video. If the object disappears from the camera view and reenters later (think: overtaking a cyclist in traffic, only to have them blow past you at the next intersection), we can easily, accurately accommodate it. The whole process is more efficient while maintaining the highest annotation quality, especially as the density of objects increases. And believe me, image complexity at the cutting edge of computer vision is getting up there.\n\nNo, really, why are you so excited?\n\nOne of the coolest aspects of the new tool is how it semi-automatically annotates frames, which makes for a more efficient workflow. If a user starts by drawing a bounding box around an object, the tool automatically estimates the object's location in subsequent or previous frames. Our expert annotation workforce carefully scrutinizes those estimates, and manually tweaks them as needed to get the tracking fully dialed in.\n\nWhen we think about where to focus our platform development, we're always looking for ways to augment the capabilities of our human workforce. We think about how we can make our data services better by using technology to make our team more efficient and more accurate -- with ever more complicated annotation projects. Video annotation is a very visceral demonstration of this approach.\n\n\n\n(By the way, the process couldn't be easier for customers. Hand over camera footage -- color, b\u0026w, high frame rate, low frame, SD, UHD, whatever -- to our project team, and we manage the entire project from start to finish, delivering annotation results that you can immediately route into your training pipeline.)\n\n\nThat’s a wrap!\n\nIt's the leveling up in the speed -- with the highest accuracy -- of our ground truth training data annotation service that really matters. We work with a many clients developing sophisticated vision algorithms, with very aggressive targets for annotation completeness and correctness. Ground truth training data is precious and object tracking video sequences particularly so. Data scientists need confidence in the quality of that training data so that they squeeze the maximum performance out of their deep learning models, focusing on the architecture and hyper-parameter tuning instead of grooming erroneous data.\n\nPartnering with Sama, you can get the most from your object tracking projects. We’re proud of this production-ready video annotation tool, and have big plans for evolving it. If you have object tracking on your mind and would like to see a demo of our annotation platform in action: Drop us a line!","slug":{"_type":"slug","current":"announcing-object-tracking-with-video-annotation"},"tags":[{"_key":"lFK49VoI","label":"Machine Learning","value":"Machine Learning"},{"_key":"uWZdY0Qc","label":"Product","value":"Product"},{"_key":"vbSgnXan","label":"Video Annotation","value":"Video Annotation"},{"_key":"NfB0x55k","label":"Data Annotation","value":"Data Annotation"}],"title":"Introducing Object Tracking with Video Annotation"},{"_createdAt":"2018-03-20T21:00:00Z","author":{"_id":"54800006-1861-42a5-a4bb-0aa2441aef30","avatar":{"_type":"image","asset":{"_ref":"image-ffd42fb1fe492d2a4a5421d537e379ee2ef7850b-299x299-jpg","_type":"reference"}},"bio":"Steve is a Senior Account Executive for Sama focusing on AI applications for the automotive industry.","name":"Steve Allen","slug":{"_type":"slug","current":"steve-allen"}},"config":{"description":"Steve Allen of Sama share's the key questions he gets asked about LiDAR and Point Cloud Annotation.","openGraphImage":null,"title":"What's the Latest with Lidar and Point Cloud Annotation?"},"estimatedReadingTime":6,"featured_image":{"_type":"image","asset":{"_ref":"image-200df2138bcc00249eacdf9d29fa4c78fc756b0a-844x1500-jpg","_type":"reference"}},"plaintextBody":"Sama was recently at the Auto.AI show in San Francisco and over half of the questions asked there were about LiDAR, point cloud data and 3D imagery. Below, we give you the view from our window.\n\n\nWhat is LiDAR and Why is it Such a Hot Topic?\n\nTo answer the first: LiDAR (Light Detection and Ranging) is a laser-based surveying method that builds up a depth-based image of the world by shining out laser lights and then measuring how long it takes for the reflected pulse to bounce back to the sensor. (Read more about LiDAR technology and its uses in this Techcrunch article.)\n\nTo answer the second question; LiDAR is one of the critical technologies in the development and deployment of autonomous vehicles. It is also a part of the development mix in other hot topics like border security, drone mapping and much more.\n\nBecause of the ability to collect three-dimensional measurements, laser scanning systems are used for surveying the built environment (such as buildings, road networks, and railways) as well as creating digital terrain (DTM) and elevation models (DEMs) of specific landscapes.\n\n\n\nEnvironmental applications also benefit from LiDAR – laser scanning is a popular method of map design, including mapping flood risk, carbon stocks in forestry and monitoring coastal erosion.\n\nIn the field of autonomous driving, LiDAR's strength is being able to visualize a 360 view around the vehicle. The only significant company currently not on board is Tesla, who is using camera and radar only at this point. Their abstention from the technology is also widely talked about. And LiDAR is the technology that Waymo/Google and Uber had gone to court over.\n\n\nKey Challenge and Aspirations Around LiDAR and Point Cloud Annotation\n\nMany companies - Sama included - can annotate 2D images, and recognize that LiDAR and 3D (also called Point Cloud) labeling is rapidly growing need. However, labeling 3D data presents some unique challenges.\n\nPoint Cloud Data labeling challenge 1 - navigating/labeling in a 3D space requires a carefully designed UI. Many companies develop proprietary tools - but then need to find a workforce that can be trained to use it. (Sama can help with that.) We are also in the process of developing our own Point Cloud Annotation tool, including a Point Cloud Viewer, that we can make available to clients.\n\nPoint Cloud Data labeling challenge 2 - depending on the sensor you are using, the resolution and the clarity can be miserable - making it hard to differentiate between objects. To get higher resolution, one solution is to use a more sensitive sensor. The top of the line model of these in existence today can cost between $60,000 to $80,000. These systems are primarily used in test and data collection vehicles and are too expensive for production cars.\n\nThe most common solution to this last challenge is that many companies use multiple, but less expensive LiDAR sensors. (We often see test cars from the major brands driving around our neighborhood in San Francisco. They’re usually set up with multiple sensors to build datasets for testing.)\n\nFrom the conversations we had at Auto.AI, we learned that there is a growing need for a broadly available 3D annotation platform that has:\n\nLabeling UI that is intuitive enough for a non-data scientist to use.\n\nFlexibility to be used for more than one customer/industry\n\nAccess to skilled workers who can be trained to provide high quality and consistent annotation (did we mention we help with that?)\n\nThe ability to work on and transfer large files. Large 3D file sizes can make it challenging to share files over the cloud.\n\nThe ability to work on and transfer sensitive files. There is a growing need to ensure the security and privacy of data gathered.\n\nThe ability to produce a top quality data set - the basis of a top quality algorithm.\n\n\nTrends in LiDAR Adoption\n\nAs we have come to expect with technology, investments lead to scale, which leads to price drops. In the past year, GM's Cruise project and Google spinoff Waymo, among many others, made significant investments in LiDAR technology, which is predicted to lead to an order of magnitude cost reduction.\n\nAlong with price drops, we also see considerable gains in capabilities. Tech leader Velodyne's new VLS-128 sensor set a new record by doubling the number of laser beams on its previous top-of-the-line LiDAR system to a massive 128 while shrinking the overall size of the sensor by 70 percent. Velodyne has announced price cuts on other products as well.\n\nOne trend that Sama can speak to personally is LiDAR equipment manufacturers partnering with annotation and labeling partners to develop a full, affordable solution to companies wishing to put this technology to use on their projects. (We help with that.)\n\nWe learned a lot talking to people at the AutoAI show and we look forward to being part of another lively conversation at the Nvidia GPU show later this month. (If you're going, can we buy you a drink?)\n\nPlease drop us a line if you would like to schedule some time to talk about your plans and aspirations in LiDAR or other data-related areas.\n\n","slug":{"_type":"slug","current":"whats-the-latest-with-lidar-and-point-cloud-annotation"},"tags":[{"_key":"DAOlTUFZ","label":"Events","value":"Events"},{"_key":"8LUsgcYe","label":"Data Annotation","value":"Data Annotation"},{"_key":"CvHVKSNz","label":"LiDAR","value":"LiDAR"}],"title":"What's the Latest with Lidar and Point Cloud Annotation?"},{"_createdAt":"2017-05-24T21:33:56Z","author":{"_id":"6dd25391-26af-42ff-a2a2-f0778ce1b002","avatar":{"_type":"image","asset":{"_ref":"image-4e1c47984bb16cb0370855b6272663d6744216d3-450x450-jpg","_type":"reference"}},"bio":"Currently a Senior Product Manager at Sama, Audrey guides cross-functional teams to create thoughtful product solutions. She has guided teams of designers and engineers at HUGE Inc. and NBCUniversal, and monitored user analytics at the Wall Street Journal. With a BA in history from Harvard, an MA in anthropology from Columbia and an MBA from UNC Chapel Hill KFBS, Audrey is passionate a using technology and data analytics facilitate social impact and environmental solutions through technology.","name":"Audrey Boguchwal","slug":{"_type":"slug","current":"audrey-boguchwal"}},"config":{"description":"Better Algorithms, Better Lives: Reducing Poverty Through Training Data","openGraphImage":null,"title":"Better Algorithms, Better Lives: Reducing Poverty Through Training Data"},"estimatedReadingTime":4,"featured_image":{"_type":"image","asset":{"_ref":"image-73eb0ced8bf91c8bd71f48061438e8d61ce0f8b6-597x398-png","_type":"reference"}},"plaintextBody":"Last time, we explained how Sama our online software, to connect low-income people to digital work. Now we’ll spotlight one of our clients to show how our image annotation work can be used in machine learning to train image recognition algorithms.\n\nMarkable came to Sama for help training their state-of-the-art image recognition technology that identifies fashion products in photos and videos. With Markable’s tech in place, viewers can click on clothing they see while watching TV shows and Markable will generate matching product results. Check out Markable’s demo to see how it works.\n\nTo create image recognition technology, engineers develop a machine learning program that learns to identify objects of interest from training data. For Markable, the training data is photographs of fashion products, clearly labeled so when the algorithm encounters an unknown product, it can infer what it is based on trained examples. This process is called “supervised learning” because the algorithm is given examples structured specifically for training. A high-quality training data set for a vision algorithm consists of tens of thousands of images in which objects are outlined and labeled according to the desired classification. The accuracy of the training data is important as an algorithm trained with inconsistently labeled data won’t learn patterns and won’t be able to identify objects.\n\nHow do all those training data images get labeled accurately and quickly enough to get Markable’s tech to customers? That’s where Sama comes in. As part of our image annotation service, we have a trained, scalable workforce and the right tools to label thousands of images for training data, efficiently and accurately. We balance stringent quality requirements with on-time deliveries to ensure clients like Markable can stick to their project timelines.\n\nSama worked closely with Markable to understand the project’s annotation requirements and refine them for ambiguous images. Markable provided Sama with source images and then we trained a dedicated team of workers how to draw boxes around the objects of interest and label them. For Markable, Sama workers learned to identify every minor visible detail, such as heel-length of a shoe, to ensure exceptional data quality. Sama’s on-site quality analysts inspect a sample of annotated images daily to ensure quality, asking workers to redo any images that don’t meet client standards.\n\nHere’s what image annotation work looks like in the Hub. Agents draw bounding boxes (or tight outlines) around each object and then use a menu to add labels. In the for-presentation-only image below, a worker has labeled the shirt, pants and is labeling boots with a multi-level menu. Sama can set up label data entry to meet a variety of project needs: text entry, selection from a dropdown, or search for a label from a list, to name a few.\n\nWith Sama’s workers, Markable has been able to improve their algorithm and better serve their customers. They wrote to us: “With the help of Sama's annotation, we were able to surpass previous state-of-the-art accuracy on the largest open-source fashion e-commerce dataset. We had a great experience in working with Sama and we will continue working with them in future.”\n\nSama’s client relationships are a win-win: clients provide our workers with life-changing job opportunities and workers help clients achieve their business goals, even on the trickiest projects.\n\nIn our third and final post, we’ll show you how Sama’s workers take on a web research project.","slug":{"_type":"slug","current":"better-algorithms-better-lives-reducing-poverty-through-training-data"},"tags":[{"_key":"Bx4Do9xK","label":"Ethical AI","value":"Ethical AI"},{"_key":"Rx6gXgmn","label":"Impact","value":"Impact"},{"_key":"iujrQUVj","label":"Use Cases","value":"Use Cases"},{"_key":"v5mZqxPX","label":"Data Annotation","value":"Data Annotation"}],"title":"Better Algorithms, Better Lives: Reducing Poverty Through Training Data"},{"_createdAt":"2017-05-15T23:51:58Z","author":{"_id":"6dd25391-26af-42ff-a2a2-f0778ce1b002","avatar":{"_type":"image","asset":{"_ref":"image-4e1c47984bb16cb0370855b6272663d6744216d3-450x450-jpg","_type":"reference"}},"bio":"Currently a Senior Product Manager at Sama, Audrey guides cross-functional teams to create thoughtful product solutions. She has guided teams of designers and engineers at HUGE Inc. and NBCUniversal, and monitored user analytics at the Wall Street Journal. With a BA in history from Harvard, an MA in anthropology from Columbia and an MBA from UNC Chapel Hill KFBS, Audrey is passionate a using technology and data analytics facilitate social impact and environmental solutions through technology.","name":"Audrey Boguchwal","slug":{"_type":"slug","current":"audrey-boguchwal"}},"config":{"description":"How exactly does Sama move people out of poverty?","openGraphImage":null,"title":"How Samasource Moves People Out of Poverty with Digital Work"},"estimatedReadingTime":3,"featured_image":{"_type":"image","asset":{"_ref":"image-bde2f64018e50b7615f188245cc00fe516e84936-693x462-jpg","_type":"reference"}},"plaintextBody":"If you’re reading this, you probably already know that Sama’s mission is to connect low-income people to digital work. And, if you’re like the majority of people reading this, the details of how exactly we do that are probably a little fuzzy. This post will dispel the mystery!\n\nSama developed an in-house SaaS (software as a service) platform that enables us to send work from our clients to workers at our delivery centers in Kenya, Uganda and India. Sama is the linchpin in our model, as it allows our project managers, agents, and clients to access the work being done, while also allowing us to closely monitor quality and provide feedback for continuous improvements.\n\nAs a SaaS platform, the Hub can be accessed securely from anywhere - our San Francisco headquarters to our Sama Center in Nairobi, Kenya. To support the Hub, we have a full time, dedicated engineering team that help the Hub evolve to meet changing client needs and continuously improve worker efficiency.\n\nDuring our sales cycle, Sama meets with clients to better understand their data services needs. When a contract is in place, a project manager will partner with the client to design a Hub project workflow that satisfies project requirements, a quality strategy to check work, a training plan and a delivery schedule.\n\nOn Hub, project managers then upload data from clients that our workers will clean, use as the basis for research, annotate or supplement. Next, they custom-design the task layout and workflow that workers will see when they log in to complete tasks. From question types and dependency logic to an easy-to-read layout, the Hub can accommodate a range of project needs.\n\nOnce the project is set up, workers complete specialized training to learn the new workflow. Throughout training, workers are coached by on-site team leaders and evaluated by quality analysts until their work is at or above the client’s desired SLA for quality. The SLA is the service level agreement, that is, the level of quality agreed upon in the client’s contract with Sama. When training is complete, workers begin production tasks on the Hub. During production, coaching continues to maintain quality. QAs use the Hub to check task quality and send back tasks that are below quality standards for rework. Once work is completed, it can be downloaded from the Hub or delivered directly to the client via API.\n\nIn future posts, we’ll go in-depth in two case studies to see how Sama completes work for different verticals: web research and image annotation for machine learning and computer vision applications, and how the Hub is an integral part of our process.\n\nStay tuned!","slug":{"_type":"slug","current":"how-exactly-does-samasource-move-people-out-of-poverty"},"tags":[{"_key":"Z2i2Xm9p","label":"Ethical AI","value":"Ethical AI"},{"_key":"NE08Dk4r","label":"Impact","value":"Impact"},{"_key":"mwOWz5qA","label":"Data Annotation","value":"Data Annotation"}],"title":"How Samasource Moves People Out of Poverty with Digital Work"}],"morePosts":[{"_createdAt":"2016-09-21T18:56:59Z","author":{"_id":"88af0504-c0e4-4479-b961-0d74424c8aff","avatar":{"_type":"image","asset":{"_ref":"image-f1fd7fbcc4633299cdbedddba22cb44e24f17317-518x518-svg","_type":"reference"}},"bio":null,"name":"Andrew Ho","slug":{"_type":"slug","current":"andrew-ho"}},"config":{"description":"Winning Customers with Algorithms: How Teams in Nairobi Help Shape Your Shopping Experience","openGraphImage":null,"title":"Winning Customers with Algorithms:Ãƒâ€šÃ‚Â How Teams in Nairobi Help Shape Your Shopping Experience"},"estimatedReadingTime":5,"featured_image":{"_type":"image","asset":{"_ref":"image-c5d3596e3d13a8462000fb2d564532cb99198bff-290x210-jpg","_type":"reference"}},"plaintextBody":"As a kid, I marveled at the strategy involved in retail layouts. From inviting department store entryways stocked with deals, to the tactical placement of milk \u0026 eggs in the backmost aisle of grocery stores, there’s always a reason why an item is placed where it is. It was a never-ending personal challenge to try and decode these invisible maps.\n\nTermed “planogramming,” visual merchandising is key in retail stores. The best stores find a balance between exciting customers without overwhelming them with deals shouting from every corner.\n\nWalmart is the reigning king of retail. With an average of 120,000 products sold in each store, visual merchandising is critical in maximizing both sales and profits. Yet, as with all traditionally brick-and-mortar markets, technology is re-shaping the consumer experience. Forget 120,000 products — eCommerce sites now place tens (and sometimes hundreds) of millions of products at consumer fingertips.\n\nA new visual experience is emerging, one that starts and ends at a computer screen. Think of Walmart.com as the equivalent 150 Walmart retail stores combined into one super-megastore. Nearly 1 square mile of product (think 2/3 the size of Golden Gate Park!). You land on the homepage and are surrounded by mile-long aisles, each with thousands of virtual shelves — where will you wander today?\n\nWith these massive assortments, the key to success in emerging eCommerce marketplaces is ensuring that the consumer finds what they need. Whoever brings the most searchable shelf to the consumer’s eye wins the virtual planogramming competition.\n\nKeys to navigating this marketplace can be basic — when you search for a white t-shirt, you don’t want results to include a red dress. They can also be more complex and predictive — when you search for firewood, schematic search may also recommend you also buy lighter fluid. As a result, each product needs a significant chunk of metadata.\n\nTake this stroller as an example. Beyond knowing that it’s a baby stroller, what color is it? Fabric material? How much weight can it bear? Is is appropriate for a newborn? Does is require assembly? Is it a multifunction stroller?\n\nWith a rapidly growing product assortment, data scientists at WalmartLabs partnered with Sama to create an easy, user-friendly way for consumers to navigate millions of unique items.\n\nThese data scientists create predictive algorithms that automatically assign attributes to products, so that computers can predict with confidence that this is a pink and black stroller made with polyester fabric that can support up to 50 pounds.\n\nAt its core, this data is inherently human. Behind each prediction, each “1” and “0”, is a judgment that was originally done by humans. Many of Walmart.com’s predictive models “learned” from training data supplied by our Sama team in Nairobi, Kenya.\n\n\n\nIn one instance, Walmart created an algorithm to automatically predict the “gender” associated with products listed on their website. The goal was to sort products that had gender specific words in the title or in the description, such as “Nike Air Women’s tennis shoes” so that these items could be identified as products for women. Walmart partnered with Sama to create data sets for creating the ground truth, and training the algorithm. Sama manually QAed 115,522 products — this included checking to see if the algorithm had correctly predicted the gender, and if not, assigning the correct gender.\n\nAs is typically the case prior to the creation of a training data set that provides sufficient ground truth, at the start, the algorithm’s accuracy was around 59%. After using Sama data sets to train the algorithm, WalmartLabs engineers were able to increase the algorithm accuracy to 93%.\n\nWalmart’s partnership with Sama has resulted in visible enhancements in product descriptions, therefore improving the shopping and purchasing experience of its extensive portfolio of products. Moreover, this project provides work for 12 Sama agents — talented women and youth in Kenya for whom access to dignified data work represents a path out of poverty.\n\nNext time you shop online and over-buy because you keep finding exactly what you want, blame our team in Nairobi! Or blame the internet for creating virtual shopping carts that never get full.","slug":{"_type":"slug","current":"winning-customers-with-algorithms"},"tags":[{"_key":"a60hER5C","label":"Training Data","value":"Training Data"},{"_key":"GJU9hmyY","label":"Data Annotation","value":"Data Annotation"},{"_key":"mMxaxwhQ","label":"Retail","value":"Retail"}],"title":"Winning Customers with Algorithms:Ãƒâ€šÃ‚Â How Teams in Nairobi Help Shape Your Shopping Experience"}],"slug":"data-annotation","tagName":"Data Annotation","pageConfig":{"title":"Sama Blog | Training Data, AI and Impact Sourcing Insights","description":"From machine learning to training data strategy, the Sama blog covers research, news and other AI trends from thought leaders across the globe."}}},"__N_SSG":true},"page":"/blog/tag/[slug]","query":{"slug":"data-annotation"},"buildId":"eTLsVxZNIPc5-poyOFiPZ","isFallback":false,"dynamicIds":[4941,425,3551],"gsp":true,"appGip":true,"scriptLoader":[]}</script></body></html>