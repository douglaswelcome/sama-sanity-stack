{"pageProps":{"config":{"footerNav":{"items":[{"_key":"f255606f8f25","_type":"navDropdownMenu","items":[{"_key":"76389ad94cbb","_type":"navItem","title":"Autonomous Transportation","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"training-data-for-autonomous-driving"}}}},{"_key":"5f64a8d6a69d","_type":"navItem","title":"E-Commerce","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"training-data-for-ecommerce"}}}},{"_key":"f10e54ae04d0","_type":"navItem","title":"AR/VR","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"training-data-for-ar-vr"}}}},{"_key":"fd729b522a77","_type":"navItem","title":"Data Quality","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"data-quality"}}}}],"title":"Guides","url":null},{"_key":"681ef7d8763a","_type":"navDropdownMenu","items":[{"_key":"6238a422b667","_type":"navItem","title":"Our Story","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"our-story"}}}},{"_key":"258985d6d46b","_type":"navItem","title":"Our Team","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"our-team"}}}},{"_key":"e0a76077324a","_type":"navItem","title":"Our Mission","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"mission-vision-values"}}}},{"_key":"239e49661b0d","_type":"navItem","title":"Careers","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"careers"}}}},{"_key":"e005a740cd80","_type":"navItem","title":"Contact","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"company-contact"}}}}],"title":"Company","url":null}]},"logo":{"asset":{"_createdAt":"2021-10-29T18:38:04Z","_id":"image-e20f8cc53e5f74df10ae9a822edb7ec2c4d00f02-2380x636-svg","_rev":"yuZxWYwFNB6KJB4TM9NtaY","_type":"sanity.imageAsset","_updatedAt":"2021-10-29T18:38:04Z","assetId":"e20f8cc53e5f74df10ae9a822edb7ec2c4d00f02","extension":"svg","metadata":{"_type":"sanity.imageMetadata","dimensions":{"_type":"sanity.imageDimensions","aspectRatio":3.742138364779874,"height":636,"width":2380},"hasAlpha":true,"isOpaque":false,"lqip":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAFCAYAAABFA8wzAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAr0lEQVQYlU3QyUoDQBAE0HeIEjeixAVyESRI3BNFJSZxAU/+/wdJQSEehu6Zqa6uKnjBGz7xgTuc4gAj7LWe4LD9cc8Y+9jBUed84QGrEt/gEleYY9Ylr3jGU/tV77fFLirId0nWBYYgoACi+r3D6YPN0vwFm4VxmNlgfkK4qcLUZdVMcdHB+75FzWPfzzCpxXNcFxuOP2uxnhNLASaX5LjbnJJf6jYG2PpXh812/AvSEQ+GGZqgYgAAAABJRU5ErkJggg==","palette":{"_type":"sanity.imagePalette","darkMuted":{"_type":"sanity.imagePaletteSwatch","background":"#040404","foreground":"#fff","population":100.29,"title":"#fff"},"darkVibrant":{"_type":"sanity.imagePaletteSwatch","background":"#424242","foreground":"#fff","population":0,"title":"#fff"},"dominant":{"_type":"sanity.imagePaletteSwatch","background":"#040404","foreground":"#fff","population":100.29,"title":"#fff"},"lightMuted":{"_type":"sanity.imagePaletteSwatch","background":"#4c4c4c","foreground":"#fff","population":0,"title":"#fff"},"lightVibrant":{"_type":"sanity.imagePaletteSwatch","background":"#bcbcbc","foreground":"#000","population":0,"title":"#fff"},"muted":{"_type":"sanity.imagePaletteSwatch","background":"#4c4c4c","foreground":"#fff","population":0,"title":"#fff"},"vibrant":{"_type":"sanity.imagePaletteSwatch","background":"#7f7f7f","foreground":"#fff","population":0,"title":"#fff"}}},"mimeType":"image/svg+xml","originalFilename":"image.svg","path":"images/76e3r62u/production/e20f8cc53e5f74df10ae9a822edb7ec2c4d00f02-2380x636.svg","sha1hash":"ae6a56857a230101a883a9b93974923879775bc9","size":2009,"uploadId":"mtOtmqAQnCEIG5cEqXZ1YAOCuqHJ4X3g","url":"https://cdn.sanity.io/images/76e3r62u/production/e20f8cc53e5f74df10ae9a822edb7ec2c4d00f02-2380x636.svg"}},"mainNav":{"items":[{"_key":"58c18e9aa9ea","_type":"navDropdownMenu","items":[{"_key":"b5b5b8bee78b","_type":"navCat","items":[{"_key":"0e80156a2f1a","_type":"navItem","title":"How it Works","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"how-it-works"}}}},{"_key":"40bacee029b4","_type":"navItem","title":"Video Annotation","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"video-annotation"}}}},{"_key":"32650ef07503","_type":"navItem","title":"Image Annotation","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"image-annotation"}}}},{"_key":"fe9137cd0167","_type":"navItem","title":"3D & LiDAR Annotation","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"3d-lidar"}}}},{"_key":"d9a1316d400a","_type":"navItem","title":"Natural Language Processing","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"natural-language-processing"}}}},{"_key":"ac12c7c5d70a","_type":"navItem","title":"Data Curation (Beta)","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"data-curation"}}}}],"title":"Platform","url":null},{"_key":"37ff4fa913bd","_type":"navCat","items":[{"_key":"6026b1a9314e","_type":"navItem","title":"Semantic Segmentation","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"semantic-segmentation"}}}},{"_key":"f4611b19b406","_type":"navItem","title":"Polygons","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"polygons"}}}},{"_key":"5155d874d6c8","_type":"navItem","title":"Bounding Boxes","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"bounding-boxes"}}}},{"_key":"9ef3c1e21e74","_type":"navItem","title":"Key Points","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"key-points"}}}},{"_key":"314d4c00d351","_type":"navItem","title":"Cuboids","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"cuboids"}}}},{"_key":"8e17a6388d74","_type":"navItem","title":"Lines & Arrows","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"lines-and-arrows"}}}}],"title":"Shapes","url":null}],"title":"Platform","url":null},{"_key":"112867ca4d03","_type":"navDropdownMenu","items":[{"_key":"22699c7e06cb","_type":"navItem","items":null,"title":"Transportation & Navigation","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"transportation-navigation"}}}},{"_key":"122ae5928d6d","_type":"navItem","items":null,"title":"Retail & E-Commerce","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"retail-ecommerce"}}}},{"_key":"7bb234b69fb0","_type":"navItem","items":null,"title":"Consumer & Media","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"consumer-media"}}}},{"_key":"33e6a886b39d","_type":"navItem","items":null,"title":"Biotech & Medtech","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"biotech-medtech"}}}},{"_key":"d095b2619c4e","_type":"navItem","items":null,"title":"Robotics & Manufacturing","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"robotics-and-manufacturing"}}}},{"_key":"2c4b82a94d79","_type":"navItem","items":null,"title":"Food & Agriculture","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"training-data-food-agriculture"}}}}],"title":"Industries","url":null},{"_key":"c47e8763a906","_type":"navDropdownMenu","items":[{"_key":"1d563df30b3f","_type":"navItem","items":null,"title":"Quality","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"quality-training-data"}}}},{"_key":"041725f35d96","_type":"navItem","items":null,"title":"Security","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"security-and-trust"}}}},{"_key":"fd64ede25798","_type":"navItem","items":null,"title":"Ethical AI","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"our-impact"}}}},{"_key":"398dcbb1c95d","_type":"navItem","items":null,"title":"Compare","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"compare"}}}},{"_key":"93bdfdd87879","_type":"navItem","items":null,"title":"Partners","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"partners"}}}}],"title":"Why Sama","url":null},{"_key":"1d38bf63df54","_type":"navDropdownMenu","items":[{"_key":"be81659b38a5","_type":"navItem","items":null,"title":"API Documentation","url":{"_type":"link","externalUrl":"https://docs.sama.com/reference/overview","internalLink":null}},{"_key":"2cec80e94962","_type":"navItem","items":null,"title":"Blog","url":{"_type":"link","internalLink":null,"internalLink_custom":"/blog"}},{"_key":"09e284fcb1d3","_type":"navItem","items":null,"title":"Events","url":{"_type":"link","internalLink":null,"internalLink_custom":"/events"}}],"title":"Resources","url":null},{"_key":"dbee93713c19","_type":"navDropdownMenu","items":[{"_key":"12d594a568bf","_type":"navItem","items":null,"title":"Our Story","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"our-story"}}}},{"_key":"ce36540a102d","_type":"navItem","items":null,"title":"Our Team","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"our-team"}}}},{"_key":"34fc328e8022","_type":"navItem","items":null,"title":"Careers","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"careers"}}}},{"_key":"c1fe2961020a","_type":"navItem","items":null,"title":"Contact","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"company-contact"}}}},{"_key":"ebd81873e538","_type":"navItem","items":null,"title":"Press","url":{"_type":"link","internalLink":null,"internalLink_custom":"/press"}}],"title":"Company","url":null}],"nav_cta":{"_type":"button","link":{"_type":"link","internalLink":{"_ref":"136788cb-06a6-4f27-b75b-07faf403bfa6","_type":"reference"}},"title":"Request a Demo","type":"secondary"}}},"data":{"firstLoad":[{"_createdAt":"2021-07-29T17:14:55Z","author":{"_id":"10ead718-57e1-41a8-b846-da3c81cc323a","avatar":{"_type":"image","asset":{"_ref":"image-a4c79da81bb1e23ce10fba84ea2cba5efe67a2a5-200x200-webp","_type":"reference"}},"bio":"Currently a Director of Product Management at Sama, Saul is passionate about the intersection of technology and social impact. He manages Sama’s data labelling products to ensure high quality training data efficiently and reliably reaches our customers. Experienced in both product and professional services, Saul is a proven leader who takes a data driven approach to expanding Sama’s capabilities and features. When not at work, you can usually find Saul enjoying the outdoors and spending time with his family.","name":"Saul Miller","slug":{"_type":"slug","current":"saul-miller"}},"config":{"description":"ML Assisted Annotation can help you generate high-quality pre-labeled and human-assisted annotations, for predictably higher quality data in half the time.","openGraphImage":null,"title":"ML Assisted Annotation Powered by MicroModels"},"estimatedReadingTime":7,"featured_image":{"_type":"image","asset":{"_ref":"image-65b7fac1b60586a0a7b9ff75006684e2c2467f1e-1800x900-png","_type":"reference"}},"plaintextBody":"Your machine learning model is only as good as the data it’s trained on. And with 80% of AI project time being spent training the large volume of data necessary to train a model, efficiency improvements early on in the process are sure to have compounding effects.\n\nAt Sama, we have a dedicated Machine Learning team working at the forefront of AI research to identify optimization opportunities just like this, so we can develop advanced annotation tools to smooth the path to production for our clients. One of the papers the team presented at last year’s CVPR—Human-Centric Efficiency Improvements in Image Annotation for Autonomous Driving—shared an approach to speeding up polygonal instance segmentation using ML.\n\nToday, this technology has been incorporated into our platform to make our clients’ labeling process more efficient.\n\nWe call it ML Assisted Annotation powered by our MICROMODEL technology, and it’s already helping clients predictably get higher quality training data in half the time.\n\nRead on for an overview of ML Assisted Annotation powered by MICROMODEL technology how it can help you develop models that are more scalable, robust and accurate – and can be brought into production more quickly.\n\nWhat is ML Assisted Annotation powered by MICROMODEL technology?\n\n\nML Assisted Annotation (MAA) powered by MICROMODEL technology is an architecture that allows Sama to expedite the labeling process by drawing from a library of models trained on specific use cases. MAA can be used to generate high-quality pre-labeled annotations, which annotators validate to help them continuously improve over time.\n\nThis powerful combination of skilled annotators and an AI-powered platform allows us to deliver a high standard of label quality to our customers every time, along with efficiency improvements and quicker time to market.\n\nHow it works\n\nIn order to understand how MAA works, we first need to discuss the DEXTR model. DEXTR, or “Deep Extreme Cut,” is a publicly available object segmentation model for images and videos. \n\n\n\nWe’ve outlined the DEXTR model and our approach in detail in this post.\n\n\n\nMany ML methods like DEXTR have been suggested to speed up the process of instance segmentation, but these are not typically tested in a high-scale production environment, nor are ML outputs easily edited by human annotators. This makes it difficult to confidently reach the label quality standards required to run a model in production.\n\nMAA combines the well-known DEXTR approach with a raster-to-polygon algorithm to make results easily editable by a human in the loop. We’ve found that this approach—which pairs skilled annotators with ML-powered automation—significantly increases labeling efficiency and quality.\n\nLet’s see what that looks like in practice, using an example from the Autonomous Vehicle industry.\n\nMachine-Assisted Polygon Annotation\n\n\n\nWhen an annotator logs into the Sama annotation platform, they are presented with this workspace. In this example, the workspace is customized to allow the annotator to draw instance segmentation polygons around each of these vehicles:\n\n\n\nYou’ll notice that there are several vehicles in this image. In a manual context, it could take a human several hours to deliver high-quality annotations of every single vehicle:\n\nWhat the manual annotation process would look like (sped up significantly): several clicks are required to draw a polygon around each of the vehicles.\n\n\n\nThis process is significantly accelerated with Machine-Assisted Polygon Annotation.\n\nThe model allows the annotator to use a crosshair tool to identify only four extreme points: left, right, top and bottom boundaries. These four clicks are the only inputs needed to create a heat map that is then sent to the inference server, returning an accurate prediction of a raster mask.\n\n\n\nWith Machine-Assisted Polygon Annotation, annotators only need to perform four clicks to produce an accurate raster mask prediction.\n\n\n\nMachine-Assisted Polygon Editing\n\n\n\nA polygon prediction can then be further refined by an annotator by switching into editing mode. This enables annotators to label precisely and ensure that high-quality requirements are met without compromise.\n\n\n\nIn this example, the raster mask prediction is edited by the annotator to ensure precise and high-quality labels.\n\n\n\nThis mode also enables annotators to use more than four extreme points in order to produce even more accurate predictions. A fifth user input point can easily be added, with the model immediately incorporating the new input to update its prediction.\n\nIf an ML model struggles to identify specific shapes, annotators can add a few more inference points to help result in a more accurate prediction, and then refine that prediction manually to ensure high-quality labels.\n\nResults from ML Assisted Annotation powered by MICROMODEL technology\n\n\nOur clients are already seeing impressive results from MAA powered by MICROMODEL technology:\n\nPredictably producing 94-98% IOU (Intersection over Union) accuracy\nBecause our models are pre-trained on specific use cases for better performance out of the gate, our clients are seeing a quicker time to accuracy.\n\n2-4x more efficient annotation process\nYou can clearly see above that using MAA over a more manual polygon labeling approach results in significant time savings. But it’s also an iterative process with a human annotator in the loop; modifications to the predictions get fed back into the training data pipeline to retrain the model, enabling it to perform better predictions over time.\n\n\nQuicker time to market\nThe end result for our clients is faster iterations and a quicker time to market. A more efficient annotation process results in more data returned quickly, and ultimately a significantly shorter path to production.\n\nWhat’s more: increasing the efficiency of this labor-intensive manual data annotation process reduces the barrier to entry for more ML teams... and not just those with large R&D budgets. Technology like this can also help democratize data labeling by driving down cost, so we can see even more deserving companies leverage AI to drive value for their business.\n\nSmall teams who are getting started with labeling may not have yet defined what type of annotations they need, or how much data they need to be successful. MAA can help them iterate more quickly, developing models in short increments rather than in large, cumbersome workstreams. The end result is a quicker time to value, and ultimately, to market — for organizations of all shapes and sizes.\n\nLearn more about ML Assisted Annotation powered by MICROMODEL technology by watching our recent webinar here, or reading more about it here.","slug":{"_type":"slug","current":"ml-assisted-annotation-micromodels"},"tags":[{"_key":"giPx682h","label":"Machine Learning","value":"Machine Learning"},{"_key":"JOgmW0AV","label":"Data Annotation","value":"Data Annotation"},{"_key":"45fExC0n","label":"Autonomous Transportation","value":"Autonomous Transportation"}],"title":"ML Assisted Annotation Powered by MicroModels"},{"_createdAt":"2021-03-23T19:00:21Z","author":{"_id":"4e2e7cef-d6eb-4bb7-bd39-375c6299677e","avatar":{"_type":"image","asset":{"_ref":"image-f1a274bcfdb5e70d814f1bab2b6bbd644728e9be-1480x1462-jpg","_type":"reference"}},"bio":"With a background in Computer Science, Abha leads the Customer Success Engineering team at Sama. The team is responsible for managing technical relationships with customers and prospects to understand their business needs, ideate upon them, and manage the implementation and communication of the solutions developed. ","name":"Abha Laddha","slug":{"_type":"slug","current":"abha-laddha"}},"config":{"description":"Sama is an expert in efficiently designing annotation guidelines that enhance data quality. Gold Tasks refer to tasks that have been annotated perfectly.","openGraphImage":null,"title":"Sama's Gold Tasks: ML Training Data with Gold-Standard Quality"},"estimatedReadingTime":4,"featured_image":{"_type":"image","asset":{"_ref":"image-2be5ee7e7ae0847f3bedec01bb88266c371e3eb9-4000x2250-jpg","_type":"reference"}},"plaintextBody":"Several methods exist to help companies define and measure data quality. To define the correct annotation of given data, you want to start by creating annotation guidelines. On top of proposing a multi-level quality checks system, Sama’s experts have built unique know-how in efficiently designing annotation guidelines that enhance data quality.\n\nCue Gold Tasks; referring to tasks that have been annotated “perfectly” or meet the “gold standard”. Such tasks are often used by the client to communicate their expectations around precision and quality used as examples during training. At Sama, we use Gold Tasks in two more ways: During training, to assess annotators and identify those who are ready to move into production and during production to generate an automated metric on quality.\n\nGold Assessments\n\nEvery project launch at Sama is accompanied with a period of training, where annotators focus on requirements for the specific workflow, familiarizing themselves with the taxonomy, accuracy levels needed, and edge cases. Gold tasks come in as they move from classroom training to practice tasks.\n\nA gold set is created that is representative of the overall complexity of the dataset, ensuring a healthy mix of the edge cases. annotators practice on this set of which we already have “gold” answers. As each task is submitted we are able to compare the annotator answers with the gold task, generating custom metrics and error tags. The metrics are determined by the type of workflow and tool used, for e.g. in a semantic segmentation workflow, we focus on IoU calculations per label and depending on the client rubric each label may be weighed differently.\n\nThese metrics are then used to create trends and analyze each annotator’s performance individually, and provide relevant feedback. We are able to analyze trends at an asset label (which type of images are more difficult than others?), annotator level (which annotator is struggling exactly where?), and the impact of time (was today better than yesterday?).\n\nGold assessments, therefore, help us accelerate training by providing customized feedback to each annotator early on and allowing us to track their improvement over time. This enables Sama to quickly identify doubts, find edge cases, and have high confidence that annotators are ready to move on to production. Lastly, this allows us to calibrate and train the manual QAs on the specifics of this particular workflow, ensuring that nothing is missed.\n\n\nGold Metrics\n\nSimilar to gold assessments, gold metrics compare an annotator’s tasks against a known, completed task. These tasks, however, are interspersed within the production queue with the annotators unaware that these are gold tasks. These tasks then act as tests for the annotators, generating similar metrics as mentioned above. This allows the team to report upon the annotator’s performance against the gold tasks that increase insight into quality and help to further tailor training and coaching.\n\nGold metrics are most useful for clients looking to automate the quality loop on their side. Given that their Sama project team consistently samples and approves only high quality tasks, it is a neat way for them to save time and capacity.\n\nBecause no two AI projects are alike, you need to make sure that your quality assurance (QA) process is designed to meet the unique needs of your particular project. Learn more on how to supercharge your data quality with Sama's Automated Quality Accelerators. ","slug":{"_type":"slug","current":"sama-gold-tasks"},"tags":[{"_key":"YSoeJ1Gj","label":"Product","value":"Product"},{"_key":"AHeszSpi","label":"Training Data","value":"Training Data"},{"_key":"9nTBOCjn","label":"Data Annotation","value":"Data Annotation"},{"_key":"ku9OMDnM","label":"Data Quality","value":"Data Quality"}],"title":"Sama's Gold Tasks: ML Training Data with Gold-Standard Quality"},{"_createdAt":"2020-04-23T21:37:48Z","author":{"_id":"26fb3cc4-608d-40e1-bb4d-955bceda232a","avatar":{"_type":"image","asset":{"_ref":"image-e0fe681e594567792ac79048513fe955cc770f54-518x518-svg","_type":"reference"}},"bio":"From self-driving cars to smart hardware, Sama fuels AI. Founded over a decade ago, we’re experts in image, video and sensor data annotation and validation for machine learning algorithms in industries including automotive, navigation, AR/VR, biotech, agriculture, manufacturing, and e-commerce. Our staff are driven by a mission to expand opportunity for low-income people through the digital economy, and our social business model has helped over 50,000 people lift themselves out of poverty.","name":"Sama Team","slug":{"_type":"slug","current":"sama-team"}},"config":{"description":"The traffic light problem for autonomous vehicles is critical for all vehicle safety, and unlike human-drivers, AVs rely solely on computer vision systems to navigate the world around us.","openGraphImage":null,"title":"The Traffic Light Problem for Autonomous Vehicles"},"estimatedReadingTime":5,"featured_image":{"_type":"image","asset":{"_ref":"image-04a6a4a0b08c6631b9ea6592f4d5e29e4ca09ba5-4496x3000-jpg","_type":"reference"}},"plaintextBody":"Solving the traffic light problem for autonomous vehicles is critical for all vehicle safety, not just autonomous vehicles. And unlike human-driven cars, AVs rely solely on their computer vision system and the data used to train them to navigate the world around us.\n\nCurrently, the best self-driving assistance systems incorrectly perceive something in their environment once every tens of thousands of hours. If that object is a traffic light, and the car gets it wrong, passengers, pedestrians, cyclists, etc., are all at risk. Here’s a look at the traffic light problem for autonomous cars from three perspectives. \n\n\n\n1. Traffic lights tell road users when to stop and go, but they only work when everyone follows the rules.\n\nTraffic lights are an engineered system, timed around traffic patterns. There are rules in place that tell cars when to stop and go, but the inherent human behavior of drivers, pedestrians and cyclists sometimes means these rules are loosely interpreted.\n\nThere are no physical barriers forcing road users to abide by traffic signals. They work because drivers follow the rules. AVs must also get the rules right, and that means countless hours of real-world exposure to the unspoken rules (or lack thereof) of the road.\n\nThe quality and accuracy of the data received from sensor packages must be precise, leaving no room for interpretation or inconsistencies. Also, license plates, faces or other personal identifying information (PII) may need to be anonymized to protect the privacy of people who might appear in the raw footage. \n\nEnsuring AVs learn the right rules requires unbiased, appropriately labeled and high-quality training data based on a range of driving scenarios.\n\n\n\n2. Traffic lights challenge both the vision system and the team developing the algorithms.\n\nBecause traffic lights are not a distance detection problem, AVs cannot use lidar or radar to navigate traffic signals. They must rely solely on their computer vision system to understand when to stop and go.\n\nThis can be difficult for both the vision system as well as the team developing the algorithm because the visibility of traffic lights may vary based on weather conditions like bright sunlight, rain, snow or fog. Similarly, not all intersections have traffic lights, so if the AV doesn’t detect one, that could be correct.\n\nThe treatment of out of service traffic signals can also vary. One city might use a plastic bag to indicate an out-of-service light while another city, or even county, might use masking tape or some other method to cover broken traffic signals.\n\nContext clues like head nods or hand signals help human drivers manage low visibility or the lack of a traffic light, but since AVs cannot register this supplemental visual information, machine learning and computer vision engineers must train the AV on such scenarios as they arise.\n\n3. Stock datasets aren’t enough to help AVs navigate traffic lights safely.\n\nThe volume and diversity of data required for autonomous vehicles is vast, and stock datasets cannot cover all use cases.\n\nDuncan Curtis, Sama VP of Product shares approximately 1,000 to 10,000 images are needed to deploy a solution onto a vehicle, and in order to launch a product publicly, approximately 10,000 to 100,000 images are needed per ADAS feature. That’s a lot of data, and the amount of data needed to account for edge cases like traffic lights in unique road conditions is unknown.\n\nSince AVs often need data from a specific camera or sensor package (because that’s what the car will use in production), generic stock data isn’t enough to help AVs navigate traffic lights safely. In addition to this, models need to be refreshed with ongoing training data as road rules, and the world around us changes.\n\nOur team of data labeling experts annotate over 4M tasks per month, and for AV use cases, we’ve achieved 99.5 percent quality SLAs with more than 100 tags per image. We’ve also successfully achieved full productivity within our labeling teams in as little as two to five days.\n\nIf you need ground truth data for your ML model, connect with our team for a virtual consultation and demo of our industry-leading data annotation platform.","slug":{"_type":"slug","current":"the-traffic-light-problem-for-autonomous-vehicles"},"tags":[{"_key":"ivLPY6Ly","label":"AI","value":"AI"},{"_key":"Liw4Inz5","label":"Training Data","value":"Training Data"},{"_key":"3LVFK315","label":"Data Annotation","value":"Data Annotation"},{"_key":"YhUSiZyI","label":"Data Quality","value":"Data Quality"},{"_key":"72HsQzsx","label":"Autonomous Transportation","value":"Autonomous Transportation"}],"title":"The Traffic Light Problem for Autonomous Vehicles"},{"_createdAt":"2020-04-02T20:30:00Z","author":{"_id":"6dd25391-26af-42ff-a2a2-f0778ce1b002","avatar":{"_type":"image","asset":{"_ref":"image-4e1c47984bb16cb0370855b6272663d6744216d3-450x450-jpg","_type":"reference"}},"bio":"Currently a Senior Product Manager at Sama, Audrey guides cross-functional teams to create thoughtful product solutions. She has guided teams of designers and engineers at HUGE Inc. and NBCUniversal, and monitored user analytics at the Wall Street Journal. With a BA in history from Harvard, an MA in anthropology from Columbia and an MBA from UNC Chapel Hill KFBS, Audrey is passionate a using technology and data analytics facilitate social impact and environmental solutions through technology.","name":"Audrey Boguchwal","slug":{"_type":"slug","current":"audrey-boguchwal"}},"config":{"description":"SamaHub's video and 3D object tracking with frame level labeling assists companies in quickly building models that better reflect real-world behavior.","openGraphImage":null,"title":"Object Tracking with Frame Level-Labeling"},"estimatedReadingTime":3,"featured_image":{"_type":"image","asset":{"_ref":"image-7eb4854393afff7be19269b7e8414936e411403f-5506x3671-jpg","_type":"reference"}},"plaintextBody":"We live in an ever-changing world, where AI-enabled technology has become a new normal for society. To assist top organizations in their efforts to build smarter computer vision algorithms, we’ve rolled out a new release for video and 3D object-tracking in our leading data annotation platform.\n\nThere are a number of applications that use computer vision to track how the world, and the objects in it, change overtime. For a self-driving car to navigate safely, it needs to track other moving objects on the road and make predictions about their future movement, so it can plan its driving path.\n\nAR and VR applications like video games need to track the motion of individual people to create a seamless digital experience. These vision applications have something in common: they all seek to understand the change in position, behavior and characteristics of unique objects over time.\n\nObject tracking annotation offers object tracking capabilities for complex scenarios, including path planning, traffic light status, sentiment analysis, etc.\n\nThis frame-level labeling technology allows unique objects to be dynamically tracked across a video or a sequence of 3D point cloud data from a Lidar sensor. Change in position and pose are captured with annotation shapes like cuboids, polygons and bounding boxes.\n\nSama supports custom label taxonomies for both the main object class (person, car, etc.) and dynamic labeling for other object characteristics that change over time, such as visibility percentage of an object or a specific set of characteristics like emotions. Sama’s built-in automated interpolation between video frames helps ensure efficient, high quality labeled training data for a variety of object tracking use cases.\n\nFor over 10 years, Sama has delivered turnkey, high-quality training data and validation to train the world's leading AI technologies. Video and 3D object tracking are no exception, and this update for video object tracking annotation in 2D RGB video and 3D Lidar data will continue to assist organizations in quickly building models that better reflect real-world behavior.\n\nSama has deep expertise working with training data for object tracking use cases across a variety of industries including autonomous vehicles, AR/VR, retail and e-commerce, communications, media and entertainment, to name just a few.\n\nDownload our solution brief to learn more about our secure training data annotation platform, or contact our team here.","slug":{"_type":"slug","current":"object-tracking-in-samahub-with-frame-level-labeling"},"tags":[{"_key":"OjIX5QKU","label":"Product","value":"Product"},{"_key":"2ZGxEWq0","label":"Video Annotation","value":"Video Annotation"},{"_key":"iBmdS1VE","label":"Training Data","value":"Training Data"},{"_key":"uZYhm6jx","label":"Data Annotation","value":"Data Annotation"}],"title":"Object Tracking with Frame Level-Labeling"},{"_createdAt":"2020-02-28T21:00:00Z","author":{"_id":"b4d4d096-8187-41fa-a386-87ce949c9915","avatar":{"_type":"image","asset":{"_ref":"image-45005425be54d2d450b1aa53f7d9435477531eb3-1664x1758-jpg","_type":"reference"}},"bio":"Liliosa is the Impact and Marketing Manager at Sama.","name":"Liliosa Mbirimi Muturi","slug":{"_type":"slug","current":"liliosa-mbirimi-muturi"}},"config":{"description":"As AI adoption expands, untapped communities are finding work at the cutting edge of AI. Here are 4 ways AI makes a positive impact on communities in East Africa.","openGraphImage":null,"title":"4 Ways AI Makes a Positive Impact on Communities in East Africa"},"estimatedReadingTime":6,"featured_image":{"_type":"image","asset":{"_ref":"image-7e68dc7b894694d301fe5feff694b3df88f8beda-2048x1365-jpg","_type":"reference"}},"plaintextBody":"In a small town in Northern Uganda, Ocem turns on his lights at 6am to get ready for work. It’s his second year working at Sama, and before becoming an AI trainer, he was a student at the local university, moonlighting as a farmer.\n\nWorking in AI was not a path Ocem imagined until he was recruited to work at Sama. In fact, like many students, his greatest uncertainty was whether he could put his degree to good use at all, post graduation.\n\nOcem’s story is like many other young professionals in East Africa, but as AI adoption explodes, these talented, untapped communities are finding work they never thought possible, at the cutting edge of AI.\n\nHere are four ways AI makes a positive impact on communities in East Africa.\n\nGainful Employment\n\nA report by the World Bank shared that 60 percent of the unemployed in Africa are youth, who after graduating from University will take on average, one year to find employment in their field of study. \n\nI believe this is due to the lack of formal career opportunities within the region, making it difficult for the employment industry to support the volume of college graduates entering the workforce.\n\nAs a result, talented graduates find work in menial jobs that often don’t provide a sustainable income, but Sama and other tech companies are working to fix that.\n\nThe explosion of AI has birthed the demand for high-quality, ground truth data for artificial intelligence, and with training, recent graduates like Ocem can find meaningful work in the growing digital economy.\n\nDriven by a mission to expand opportunity for low-income people, Sama's social impact business model has helped over 50,000 people move themselves out of poverty through digital work.\n\nWorkers in our East Africa centers are paid a living wage and competitive benefits. They also receive ongoing digital skills training and have access to professional development opportunities that further help increase the purchasing power of their communities, potentially ending a long cycle of poverty.\n\nSkills and Knowledge\n\nAs more tech companies expand operations to Africa, and more people start gaining on-the-job experience working in machine learning and AI, the positive impact AI makes on communities in East Africa will only expand.\n\nI’ve personally witnessed how energized the youth in East Africa are about the intricacies of enriching data for AI. For many, this budding tech boom has motivated them to enroll in courses centered around data sciences and AI.\n\nAlso, the Deep Learning Indaba, an annual meeting for AI enthusiasts in Africa has brought together a growing number of youths for week long discussions, debates and research on machine learning and AI.\n\nWith a reliable IT infrastructure, paired with increased trust in the skill level of youth workers in the region, East Africa has the potential to become a major hub for AI training data.\n\nAt Sama, our AI training classes fill up weekly, and this level of investment in skills development for young professionals is what’s needed to continue developing the talent pool. \n\nDiversity and Inclusion\n\nUnesco reported that 30% of the tech-workforce in sub-Saharan Africa are women, and despite the challenges of gender discrimination in the technology sector, the percentage of female workers is steadily growing. \n\nWomen in Africa have often been set apart from the tech industry because culturally, they were considered primary caretakers, expected to focus on their family, not their career. Now, the vast opportunities in AI are helping to change that.\n\nWe actively recruit women and youth to work in our East Africa centers, providing in-depth technical training, so underserved communities can pursue a career in AI. From scholarship programs to nursing rooms, we’ve also made it a priority to establish an office culture conducive to the success of women workers.\n\nOur baseline survey data found that women earned 70 cents for every dollar earned by men, before joining Sama. By ensuring we achieve gender parity in pay, while simultaneously paying a living wage, our workers can support themselves and their families sustainably.\n\nData Privacy and Protection\n\nAs more populations in Africa adopt mobile technology and digital apps, their exposure to AI-enabled technologies that collect data to improve experiences will only increase. These interactions can be as simple as product recommendations from an online store, or interactions with an automated messenger bot.\n\nGiven the rapid advancement of AI and the Internet of Things (IoT) , the Kenyan government recently passed a data protection law which complies with the European Union’s General Data Protection Regulation (GDPR). \n\nSimilar to GDPR, this legislation outlines restrictions on data handling and sharing, in an effort to regulate the processing of personal data and information in the country.\n\nThis commitment to increased digital security will no doubt have a positive impact on the citizens of Kenya, and in many ways, it sets the standard for data privacy and protection across the continent.\n\nMcKinsey Global Institute predicts that AI has the potential to deliver an additional economic output of $13 trillion by 2030. This economic growth, alongside the digital transformation of sub-Saharan Africa will have enormous positive impact on communities in East Africa.\n\nIf you’re interested in learning more about how Sama connects people to dignified digital work, you can read about our social business model here.","slug":{"_type":"slug","current":"4-ways-ai-makes-a-positive-impact-on-communities-in-east-africa"},"tags":[{"_key":"AEPEpJXp","label":"Ethical AI","value":"Ethical AI"},{"_key":"tc6Hsfbm","label":"AI","value":"AI"},{"_key":"VmY4sBKG","label":"Impact","value":"Impact"},{"_key":"iLYfly0n","label":"Data Annotation","value":"Data Annotation"}],"title":"4 Ways AI Makes a Positive Impact on Communities in East Africa"},{"_createdAt":"2019-07-11T22:00:00Z","author":{"_id":"6dd25391-26af-42ff-a2a2-f0778ce1b002","avatar":{"_type":"image","asset":{"_ref":"image-4e1c47984bb16cb0370855b6272663d6744216d3-450x450-jpg","_type":"reference"}},"bio":"Currently a Senior Product Manager at Sama, Audrey guides cross-functional teams to create thoughtful product solutions. She has guided teams of designers and engineers at HUGE Inc. and NBCUniversal, and monitored user analytics at the Wall Street Journal. With a BA in history from Harvard, an MA in anthropology from Columbia and an MBA from UNC Chapel Hill KFBS, Audrey is passionate a using technology and data analytics facilitate social impact and environmental solutions through technology.","name":"Audrey Boguchwal","slug":{"_type":"slug","current":"audrey-boguchwal"}},"config":{"description":"Samasource's revamped toolset for 2D image vector segmentation is ideal for computer vision projects using vector shapes to structure training data.","openGraphImage":null,"title":"Revamped 2D Vector Segmentation"},"estimatedReadingTime":null,"featured_image":{"_type":"image","asset":{"_ref":"image-31269a4f01b8e495d1ec30a473f063a4957254c6-5760x3840-jpg","_type":"reference"}},"plaintextBody":null,"slug":null,"tags":[{"_key":"oc26gThR","label":"Product","value":"Product"},{"_key":"8Uhzi08N","label":"Vector Annotation","value":"Vector Annotation"},{"_key":"W46d0x8z","label":"Training Data","value":"Training Data"},{"_key":"NwhOkizM","label":"Data Annotation","value":"Data Annotation"},{"_key":"46Go0q8m","label":"Data Quality","value":"Data Quality"}],"title":"Revamped 2D Vector Segmentation"},{"_createdAt":"2019-06-12T20:00:00Z","author":{"_id":"97dc2368-fffb-4c41-82aa-5a9cbe2ec670","avatar":{"_type":"image","asset":{"_ref":"image-af58425525bb33d8cffdc1f1b10f02bf1e4faf57-1916x2028-jpg","_type":"reference"}},"bio":"Sharon is the Content Marketing Manager at Sama where she's responsible for telling the story behind the company's impact sourcing mission and human-powered training data solutions. Sharon holds a MS in Integrated Marketing Communications and is passionate about helping social enterprises transform abstract concepts into results-driven marketing.","name":"Sharon L. Hadden","slug":{"_type":"slug","current":"sharon-l-hadden"}},"config":{"description":"Here are 7 organizations attending CVPR 2019 who are leading the way in computer vision, plus 3 noteworthy companies from around the web.","openGraphImage":null,"title":"10 Organizations Leading the Way in Computer Vision"},"estimatedReadingTime":null,"featured_image":{"_type":"image","asset":{"_ref":"image-a49d2248810c7d2141277314bfa9b78a95c2a9a3-2250x1500-jpg","_type":"reference"}},"plaintextBody":null,"slug":null,"tags":[{"_key":"pm9yH2PJ","label":"Best of","value":"Best of"},{"_key":"YwMfUUBk","label":"Data Annotation","value":"Data Annotation"},{"_key":"NsG61NKf","label":"Computer Vision","value":"Computer Vision"}],"title":"10 Organizations Leading the Way in Computer Vision"},{"_createdAt":"2018-12-14T19:54:00Z","author":{"_id":"71091c91-664a-44a6-9474-acc40eb12457","avatar":{"_type":"image","asset":{"_ref":"image-bc776336801adf71e2599337e8d6f02186b109d0-500x500-jpg","_type":"reference"}},"bio":"Matthew leads the product team at Sama, responsible for the platform that enables Sama's AI/ML data enrichment teams, internal enterprise operations tools to ensure quality and scalability, and all new product initiatives for the evolution of algorithm development and human-powered automation.","name":"Matthew Landry","slug":{"_type":"slug","current":"matthew-landry"}},"config":{"description":"Training your AI in 3D","openGraphImage":null,"title":"Training Your AI in 3D"},"estimatedReadingTime":null,"featured_image":{"_type":"image","asset":{"_ref":"image-9a175242c5b6d9d34ab8d001420280c4279ffe11-1125x1500-jpg","_type":"reference"}},"plaintextBody":null,"slug":null,"tags":[{"_key":"C3ND2OEf","label":"Product","value":"Product"},{"_key":"dJfUvcIY","label":"Training Data","value":"Training Data"},{"_key":"Se91A2Xp","label":"Data Annotation","value":"Data Annotation"}],"title":"Training Your AI in 3D"},{"_createdAt":"2018-07-17T00:00:00Z","author":{"_id":"71091c91-664a-44a6-9474-acc40eb12457","avatar":{"_type":"image","asset":{"_ref":"image-bc776336801adf71e2599337e8d6f02186b109d0-500x500-jpg","_type":"reference"}},"bio":"Matthew leads the product team at Sama, responsible for the platform that enables Sama's AI/ML data enrichment teams, internal enterprise operations tools to ensure quality and scalability, and all new product initiatives for the evolution of algorithm development and human-powered automation.","name":"Matthew Landry","slug":{"_type":"slug","current":"matthew-landry"}},"config":{"description":"Announcing object tracking with video annotation","openGraphImage":null,"title":"Introducing Object Tracking with Video Annotation"},"estimatedReadingTime":null,"featured_image":{"_type":"image","asset":{"_ref":"image-af58edf394d9676203b9bc44284f08ccf52125a6-1000x1500-jpg","_type":"reference"}},"plaintextBody":null,"slug":null,"tags":[{"_key":"lFK49VoI","label":"Machine Learning","value":"Machine Learning"},{"_key":"uWZdY0Qc","label":"Product","value":"Product"},{"_key":"vbSgnXan","label":"Video Annotation","value":"Video Annotation"},{"_key":"NfB0x55k","label":"Data Annotation","value":"Data Annotation"}],"title":"Introducing Object Tracking with Video Annotation"},{"_createdAt":"2018-03-20T21:00:00Z","author":{"_id":"54800006-1861-42a5-a4bb-0aa2441aef30","avatar":{"_type":"image","asset":{"_ref":"image-ffd42fb1fe492d2a4a5421d537e379ee2ef7850b-299x299-jpg","_type":"reference"}},"bio":"Steve is a Senior Account Executive for Sama focusing on AI applications for the automotive industry.","name":"Steve Allen","slug":{"_type":"slug","current":"steve-allen"}},"config":{"description":"Steve Allen of Sama share's the key questions he gets asked about LiDAR and Point Cloud Annotation.","openGraphImage":null,"title":"What's the Latest with Lidar and Point Cloud Annotation?"},"estimatedReadingTime":null,"featured_image":{"_type":"image","asset":{"_ref":"image-200df2138bcc00249eacdf9d29fa4c78fc756b0a-844x1500-jpg","_type":"reference"}},"plaintextBody":null,"slug":null,"tags":[{"_key":"DAOlTUFZ","label":"Events","value":"Events"},{"_key":"8LUsgcYe","label":"Data Annotation","value":"Data Annotation"},{"_key":"CvHVKSNz","label":"LiDAR","value":"LiDAR"}],"title":"What's the Latest with Lidar and Point Cloud Annotation?"},{"_createdAt":"2017-05-24T21:33:56Z","author":{"_id":"6dd25391-26af-42ff-a2a2-f0778ce1b002","avatar":{"_type":"image","asset":{"_ref":"image-4e1c47984bb16cb0370855b6272663d6744216d3-450x450-jpg","_type":"reference"}},"bio":"Currently a Senior Product Manager at Sama, Audrey guides cross-functional teams to create thoughtful product solutions. She has guided teams of designers and engineers at HUGE Inc. and NBCUniversal, and monitored user analytics at the Wall Street Journal. With a BA in history from Harvard, an MA in anthropology from Columbia and an MBA from UNC Chapel Hill KFBS, Audrey is passionate a using technology and data analytics facilitate social impact and environmental solutions through technology.","name":"Audrey Boguchwal","slug":{"_type":"slug","current":"audrey-boguchwal"}},"config":{"description":"Better Algorithms, Better Lives: Reducing Poverty Through Training Data","openGraphImage":null,"title":"Better Algorithms, Better Lives: Reducing Poverty Through Training Data"},"estimatedReadingTime":null,"featured_image":"","plaintextBody":null,"slug":null,"tags":[{"_key":"Bx4Do9xK","label":"Ethical AI","value":"Ethical AI"},{"_key":"Rx6gXgmn","label":"Impact","value":"Impact"},{"_key":"iujrQUVj","label":"Use Cases","value":"Use Cases"},{"_key":"v5mZqxPX","label":"Data Annotation","value":"Data Annotation"}],"title":"Better Algorithms, Better Lives: Reducing Poverty Through Training Data"},{"_createdAt":"2017-05-15T23:51:58Z","author":{"_id":"6dd25391-26af-42ff-a2a2-f0778ce1b002","avatar":{"_type":"image","asset":{"_ref":"image-4e1c47984bb16cb0370855b6272663d6744216d3-450x450-jpg","_type":"reference"}},"bio":"Currently a Senior Product Manager at Sama, Audrey guides cross-functional teams to create thoughtful product solutions. She has guided teams of designers and engineers at HUGE Inc. and NBCUniversal, and monitored user analytics at the Wall Street Journal. With a BA in history from Harvard, an MA in anthropology from Columbia and an MBA from UNC Chapel Hill KFBS, Audrey is passionate a using technology and data analytics facilitate social impact and environmental solutions through technology.","name":"Audrey Boguchwal","slug":{"_type":"slug","current":"audrey-boguchwal"}},"config":{"description":"How exactly does Sama move people out of poverty?","openGraphImage":null,"title":"How Samasource Moves People Out of Poverty with Digital Work"},"estimatedReadingTime":null,"featured_image":{"_type":"image","asset":{"_ref":"image-bde2f64018e50b7615f188245cc00fe516e84936-693x462-jpg","_type":"reference"}},"plaintextBody":null,"slug":null,"tags":[{"_key":"Z2i2Xm9p","label":"Ethical AI","value":"Ethical AI"},{"_key":"NE08Dk4r","label":"Impact","value":"Impact"},{"_key":"mwOWz5qA","label":"Data Annotation","value":"Data Annotation"}],"title":"How Samasource Moves People Out of Poverty with Digital Work"}],"morePosts":[{"_createdAt":"2016-09-21T18:56:59Z","author":{"_id":"88af0504-c0e4-4479-b961-0d74424c8aff","avatar":{"_type":"image","asset":{"_ref":"image-e0fe681e594567792ac79048513fe955cc770f54-518x518-svg","_type":"reference"}},"bio":null,"name":"Andrew Ho","slug":{"_type":"slug","current":"andrew-ho"}},"config":{"description":"Winning Customers with Algorithms:Ãƒâ€šÃ‚Â How Teams in Nairobi Help Shape Your Shopping Experience","openGraphImage":null,"title":"Winning Customers with Algorithms:Ãƒâ€šÃ‚Â How Teams in Nairobi Help Shape Your Shopping Experience"},"estimatedReadingTime":null,"featured_image":{"_type":"image","asset":{"_ref":"image-c5d3596e3d13a8462000fb2d564532cb99198bff-290x210-jpg","_type":"reference"}},"plaintextBody":null,"slug":null,"tags":[{"_key":"a60hER5C","label":"Training Data","value":"Training Data"},{"_key":"GJU9hmyY","label":"Data Annotation","value":"Data Annotation"},{"_key":"mMxaxwhQ","label":"Retail","value":"Retail"}],"title":"Winning Customers with Algorithms:Ãƒâ€šÃ‚Â How Teams in Nairobi Help Shape Your Shopping Experience"}],"slug":"data-annotation","tagName":"Data Annotation","pageConfig":{"title":"Sama Blog | Training Data, AI and Impact Sourcing Insights","description":"From machine learning to training data strategy, the Sama blog covers research, news and other AI trends from thought leaders across the globe."}}},"__N_SSG":true}