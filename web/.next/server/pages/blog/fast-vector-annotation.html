<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"/><meta name="viewport" content="initial-scale=1.0, width=device-width, viewport-fit=cover"/><title>Fast Vector Annotation with Machine Learning Assisted Annotation</title><link rel="canonical" href="https://sama.com/blog/fast-vector-annotation"/><meta name="description" content="In this article we summarize an approach that we have developed to speed up polygonal instance segmentation using machine learning."/><meta property="og:type" content="website"/><meta property="og:description" content="In this article we summarize an approach that we have developed to speed up polygonal instance segmentation using machine learning."/><meta property="og:title" content="Fast Vector Annotation with Machine Learning Assisted Annotation"/><meta property="og:url" content="https://sama.com/blog/fast-vector-annotation"/><meta name="twitter:card" content="summary"/><meta property="twitter:description" content="In this article we summarize an approach that we have developed to speed up polygonal instance segmentation using machine learning."/><meta property="twitter:title" content="Fast Vector Annotation with Machine Learning Assisted Annotation"/><meta name="msapplication-TileColor" content="#28282a"/><meta name="theme-color" content="#ffffff"/><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"/><link rel="shortcut icon" href="/favicon.ico"/><meta name="next-head-count" content="17"/><link rel="preload" href="/_next/static/css/bd60e2be2420db639f1f.css" as="style"/><link rel="stylesheet" href="/_next/static/css/bd60e2be2420db639f1f.css" data-n-g=""/><link rel="preload" href="/_next/static/css/ee44a66de7e204f53221.css" as="style"/><link rel="stylesheet" href="/_next/static/css/ee44a66de7e204f53221.css" data-n-p=""/><link rel="preload" href="/_next/static/css/0d4bd6b9e4f2c8d7d433.css" as="style"/><link rel="stylesheet" href="/_next/static/css/0d4bd6b9e4f2c8d7d433.css"/><link rel="preload" href="/_next/static/css/3b03d00d40d80e105549.css" as="style"/><link rel="stylesheet" href="/_next/static/css/3b03d00d40d80e105549.css"/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-a40ef1678bae11e696dba45124eadd70.js"></script><script defer="" src="/_next/static/chunks/1776.90811ba0e7bd8502bb5d.js"></script><script defer="" src="/_next/static/chunks/4934.37651fffe4e244d39030.js"></script><script defer="" src="/_next/static/chunks/1952.c48d8556ec1dc7f6b94b.js"></script><script defer="" src="/_next/static/chunks/3551.489d89a23d509b64ee09.js"></script><script src="/_next/static/chunks/webpack-56c8daaf4d8705ab0c8a.js" defer=""></script><script src="/_next/static/chunks/framework-bdc1b4e5e48979e16d36.js" defer=""></script><script src="/_next/static/chunks/main-6409a04df91a58e5134b.js" defer=""></script><script src="/_next/static/chunks/pages/_app-8b37b83a9f5e9f7f90ae.js" defer=""></script><script src="/_next/static/chunks/commons-15b6d00d8a6970926aed.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-c110057df969a5c6e40e.js" defer=""></script><script src="/_next/static/fpyn7Yiasmv-BShm3_n8o/_buildManifest.js" defer=""></script><script src="/_next/static/fpyn7Yiasmv-BShm3_n8o/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="container"><header class="header_outer__yu9q7 "><nav class="umoja-l-grid--12 header_wrapper__3Ghzm"><a class="header_logo__eiLSq" href="/"></a><button class="header_hamburger__1ZcbZ" type="button"><span class="header_hamburger_box__RZ7CY"><span class="header_hamburger_box_inner__1PmWZ"></span></span></button><ul class="header_navBar__37eSJ"><li class="header_navItem__1HiGN"><p class="header_navItem_label__fN3Cy">Platform</p><div class="header_dropDown__6NxUb"><div class="header_dropDown_group__2BZXC"><p class="header_dropdown_group_label__tmND8">Platform</p><a class="header_navLink__1ARu5" href="/how-it-works">How it Works</a><a class="header_navLink__1ARu5" href="/video-annotation">Video Annotation</a><a class="header_navLink__1ARu5" href="/image-annotation">Image Annotation</a><a class="header_navLink__1ARu5" href="/3d-lidar">3D &amp; LiDAR Annotation</a><a class="header_navLink__1ARu5" href="/natural-language-processing">Natural Language Processing</a><a class="header_navLink__1ARu5" href="/data-curation">Data Curation (Beta)</a></div><div class="header_dropDown_group__2BZXC"><p class="header_dropdown_group_label__tmND8">Shapes</p><a class="header_navLink__1ARu5" href="/semantic-segmentation">Semantic Segmentation</a><a class="header_navLink__1ARu5" href="/polygons">Polygons</a><a class="header_navLink__1ARu5" href="/bounding-boxes">Bounding Boxes</a><a class="header_navLink__1ARu5" href="/key-points">Key Points</a><a class="header_navLink__1ARu5" href="/cuboids">Cuboids</a><a class="header_navLink__1ARu5" href="/lines-and-arrows">Lines &amp; Arrows</a></div></div></li><li class="header_navItem__1HiGN"><p class="header_navItem_label__fN3Cy">Industries</p><div class="header_dropDown__6NxUb"><a class="header_navLink__1ARu5" href="/transportation-navigation">Transportation &amp; Navigation</a><a class="header_navLink__1ARu5" href="/retail-ecommerce">Retail &amp; E-Commerce</a><a class="header_navLink__1ARu5" href="/consumer-media">Consumer &amp; Media</a><a class="header_navLink__1ARu5" href="/biotech-medtech">Biotech &amp; Medtech</a><a class="header_navLink__1ARu5" href="/robotics-and-manufacturing">Robotics &amp; Manufacturing</a><a class="header_navLink__1ARu5" href="/training-data-food-agriculture">Food &amp; Agriculture</a></div></li><li class="header_navItem__1HiGN"><p class="header_navItem_label__fN3Cy">Why Sama</p><div class="header_dropDown__6NxUb"><a class="header_navLink__1ARu5" href="/quality-training-data">Quality</a><a class="header_navLink__1ARu5" href="/security-and-trust">Security</a><a class="header_navLink__1ARu5" href="/our-impact">Ethical AI</a><a class="header_navLink__1ARu5" href="/compare">Compare</a><a class="header_navLink__1ARu5" href="/partners">Partners</a></div></li><li class="header_navItem__1HiGN"><p class="header_navItem_label__fN3Cy">Resources</p><div class="header_dropDown__6NxUb"><a href="https://docs.sama.com/reference/overview" class="header_navLink__1ARu5" target="_blank">API Documentation</a><a class="header_navLink__1ARu5" href="/blog">Blog</a><a class="header_navLink__1ARu5" href="/events">Events</a></div></li><li class="header_navItem__1HiGN"><p class="header_navItem_label__fN3Cy">Company</p><div class="header_dropDown__6NxUb"><a class="header_navLink__1ARu5" href="/our-story">Our Story</a><a class="header_navLink__1ARu5" href="/our-team">Our Team</a><a class="header_navLink__1ARu5" href="/careers">Careers</a><a class="header_navLink__1ARu5" href="/company-contact">Contact</a><a class="header_navLink__1ARu5" href="/press">Press</a></div></li></ul><div class="header_cta__3J8I7"><a class="button_wrapper__3lRbv button__secondary__1pZ5q button__small__2kIwW" href="/[object%20Object]"><button class="button_btn__1qxP1"><h3 class="button_text__3_sCS">Request a Demo</h3></button></a></div></nav></header><main class="content"><section class="umoja-l-grid-section umoja-u-bg--white"><div class="umoja-l-grid--12 umoja-l-grid-align--center"><div class="blog-hero-post_wrap__1KbfB"><div class="blog-hero-post_left__2zn39"><h1>Fast Vector Annotation with Machine Learning Assisted Annotation</h1><a class="blog-hero-post_author__28uii" href="/[object%20Object]">Frederic Ratle</a><p class="blog-hero-post_date__hVvVU">December 15, 2020<!-- --> | <!-- -->8 Min Read</p></div><div class="blog-hero-post_right__AAryF"><div style="display:block;overflow:hidden;position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;margin:0"><img alt="" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="fill" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover"/><noscript><img alt="" sizes="100vw" srcSet="https://cdn.sanity.io/images/76e3r62u/production/6abd2f846119ba50a8325787f99ec3aeec68ffff-1076x605.png?w=640&amp;q=75&amp;fit=clip&amp;auto=format 640w, https://cdn.sanity.io/images/76e3r62u/production/6abd2f846119ba50a8325787f99ec3aeec68ffff-1076x605.png?w=750&amp;q=75&amp;fit=clip&amp;auto=format 750w, https://cdn.sanity.io/images/76e3r62u/production/6abd2f846119ba50a8325787f99ec3aeec68ffff-1076x605.png?w=828&amp;q=75&amp;fit=clip&amp;auto=format 828w, https://cdn.sanity.io/images/76e3r62u/production/6abd2f846119ba50a8325787f99ec3aeec68ffff-1076x605.png?w=1080&amp;q=75&amp;fit=clip&amp;auto=format 1080w, https://cdn.sanity.io/images/76e3r62u/production/6abd2f846119ba50a8325787f99ec3aeec68ffff-1076x605.png?w=1200&amp;q=75&amp;fit=clip&amp;auto=format 1200w, https://cdn.sanity.io/images/76e3r62u/production/6abd2f846119ba50a8325787f99ec3aeec68ffff-1076x605.png?w=1920&amp;q=75&amp;fit=clip&amp;auto=format 1920w, https://cdn.sanity.io/images/76e3r62u/production/6abd2f846119ba50a8325787f99ec3aeec68ffff-1076x605.png?w=2048&amp;q=75&amp;fit=clip&amp;auto=format 2048w, https://cdn.sanity.io/images/76e3r62u/production/6abd2f846119ba50a8325787f99ec3aeec68ffff-1076x605.png?w=3840&amp;q=75&amp;fit=clip&amp;auto=format 3840w" src="https://cdn.sanity.io/images/76e3r62u/production/6abd2f846119ba50a8325787f99ec3aeec68ffff-1076x605.png?w=3840&amp;q=75&amp;fit=clip&amp;auto=format" decoding="async" data-nimg="fill" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover" loading="lazy"/></noscript></div></div></div></div></section><section class="umoja-l-grid-section umoja-u-bg--white"><div class="umoja-l-grid--12 blog-post_body__2CyGF"><div class="blog-post_share__2sAqL"><div class="blog-post_share_track__3nibH"><a class="blog-post_share_button__2ZGXI blog-post_share_button__facebook__2V_7U" href="https://www.facebook.com/sharer/sharer.php?u=https://www.sama.com//blog/fast-vector-annotation"></a><a class="blog-post_share_button__2ZGXI blog-post_share_button__twitter__3lhXm" href="https://twitter.com/intent/tweet?text=Check%20out%20this%20great%20blog%20post%20I%20just%20read&amp;url=https://www.sama.com//blog/fast-vector-annotation"></a><a class="blog-post_share_button__2ZGXI blog-post_share_button__linkedin__34so0" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://www.sama.com//blog/fast-vector-annotation&amp;title=Fast Vector Annotation with Machine Learning Assisted Annotation&amp;source=https://www.sama.com/&amp;summary=Check%20out%20this%20great%20blog%20post%20I%20just%20read"></a></div></div><div class="blog-post_content__1Q3Gg richText_inner__37NFR undefined"><p>At Sama, Vector Annotation of objects using polygons is a task that our expert annotators spend a great deal of time on. This is especially true for projects involving autonomous vehicles, where it is typical to apply instance segmentation to label scenes comprising hundreds of frames, each with multiple objects (vehicles, pedestrians, traffic signs, etc.) like you see in Figure 1.</p><img src="https://cdn.sanity.io/images/76e3r62u/production/3a4b80bf2e9cb5cebab901de01ab7f5b4fb8c5c9-1475x758.png?w=1475&amp;q=75&amp;fit=clip&amp;auto=format" alt=""/><h6><strong>Figure 1</strong>. Example of Polygonal Annotation in Sama.</h6><p></p><p>In this post, we summarize an approach that we have developed to speed up polygonal instance segmentation using machine learning. This approach was presented earlier this year at the <a href="http://cvpr2020.wad.vision/" target="_blank" rel="noopener">CVPR Workshop on Scalability in Autonomous Driving</a>, and the <a href="https://icml.cc/" target="_blank" rel="noopener">ICML Workshop on Human-in-the-Loop Learning</a><em>.</em></p><h4>Few-Click Annotation</h4><p>Building instance segmentation Deep Learning (DL) models for autonomous vehicles requires a significant amount of labeled data. The use of Machine Learning (ML) for producing pre-annotations to be reviewed by human annotators, whether in an interactive setting or as a pre-processing procedure, is a very popular approach for scaling up labeling while controlling the costs.</p><p>Multiple approaches have been suggested for machine-assisted instance segmentation. These typically consist of a DL-based segmentation of the object(s) integrated into a human-in-the-loop system. The human can interact with the system by correcting the model output, initializing the model with one or several clicks, or a combination of those steps. Examples of such systems include Polygon-RNN++ [1], DELSE [7], DEXTR [3], and CurveGCN [2]. Those systems all present good results, but some open questions remain:</p><ul><li>Do these methods perform well when a production-level accuracy is required, as when working for a customer project?</li><li>Does the choice of annotation tool influence the results? The gains to be made by using ML depend on how difficult it is for humans to draw polygons in the provided UI. Here we used our optimized drawing tool for polygons, which is part of our labeling platform.</li><li>ML integration is not usually approached from a human-centric perspective. Beyond the optimization of traditional metrics like IoU, what interactions are most desirable and how should we present the output of the model to annotators?</li></ul><p>Our method relies on combining the well-known DEXTR [3] approach with a raster-to-polygon algorithm, to make the result more easily editable. This is not unlike what other tools (such as CVAT) have implemented, though we have optimized this approach for our specific use cases using A/B testing.</p><h4>The Model</h4><p>Our instance segmentation model is based on the well-known Deep Extreme Cut (DEXTR) approach [3], along with a raster-to-polygon conversion algorithm that yields high quality polygons whose vertices are sampled in a way that reproduces human drawing patterns. The model uses the few clicks provided by human annotators at inference time. The steps are described in Figure 2.</p><img src="https://cdn.sanity.io/images/76e3r62u/production/2385ed2f2128573a154324bb5fbbded2e611e9fa-1424x638.png?w=1424&amp;q=75&amp;fit=clip&amp;auto=format" alt=""/><h6><strong>Figure 2</strong>. An overview of the approach.</h6><p></p><p>Regarding the model itself, we adopted a custom version of the UNet [5] along with an EfficientNet backbone [6] (instead of the ResNet backbone used in the paper).</p><p>In our experience, for human annotators to produce good instance segmentation masks efficiently, a polygon annotation tool should be used. As such, we needed to convert the raster masks produced by our model to high quality polygons. To add to the challenge, humans tend to produce sparse polygons, adding vertices only when necessary. We therefore adopted a raster-to-polygon procedure that minimizes the number of output vertices.</p><h4>A/B Testing the Approach</h4><p>At Sama, we use A/B testing as much as possible to systematically refine and improve our new features. To this end, we have developed a flexible testing infrastructure that can ingest and aggregate data from multiple internal processes and is made available to anyone within the organization.</p><p>This framework measures the statistical impact of proposed changes on our efficiency metrics (such as drawing time or shape adjustment time). The significance of observed differences on a given efficiency metric is evaluated using statistical tests.</p><h5>Toy A/B Tests</h5><p>We conducted an A/B test of the method using a synthetic automotive dataset called SYNTHIA-AL [8]. The dataset&#x27;s images and corresponding annotations were generated from video streams at 25 frames per second (FPS). Figure 3 shows SYNTHIA image examples, along with their segmentation (done manually and with the Few-Click tool).</p><img src="https://cdn.sanity.io/images/76e3r62u/production/ad6b4a09778cefac7979292c68c15ba0dcfa5ddd-512x231.png?w=512&amp;q=75&amp;fit=clip&amp;auto=format" alt=""/><h6><strong>Figure 3</strong>. SYNTHIA example images, along with their manual and semi-automated annotations.</h6><p></p><p>The test, applied only to motor vehicles, reproduced realistic annotation guidelines, namely:</p><ul><li>The drawn polygon needs to be within 2 pixels of the edge of the vehicle.</li><li>All vehicles down to 10 pixels (height or width) need to be annotated.</li></ul><p>Following this test, we found a nearly 3-fold reduction in annotation time. On the other hand, we also found that on some of the more complex shapes, annotators were spending quite some time manually adjusting the ML output. DEXTR&#x27;s authors originally showed that the segmentation can be improved with additional clicks beyond the four initial ones. We therefore extended our few-click tool to allow online refinement of the polygons by considering modifications to their vertices as extra clicks. At train time we simulated the corrective clicks by considering the point of greatest deviation between predicted mask and ground truth as illustrated in Figure 4.</p><ul><li><strong>Problem</strong>: DEXTR’s 4 extreme clicks are not always sufficient.</li><li><strong>Observation</strong>: DEXTR trained on 4 clicks benefits from additional clicks.</li><li><strong>Solution</strong>: Fine-tune DEXTR model with additional clicks for hard samples as established by IoU at train time.</li></ul><img src="https://cdn.sanity.io/images/76e3r62u/production/9761c7350f514786633b12b6f17058b6b67d6677-627x427.png?w=627&amp;q=75&amp;fit=clip&amp;auto=format" alt=""/><h6><strong>Figure 4</strong>. Integrating additional clicks in the training process.<br/></h6><p><br/>Using this method, annotators are able to re-trigger the model inference with an additional click, instead of manually adjusting the output. We proceeded to a second toy A/B test, and results showed that we could obtain a theoretical efficiency gain of up to 3.5x on vehicles using the improved method.<br/><br/><a href="https://www.sama.com/hubfs/Research%20Papers/Human-Centric%20Efficiency%20Improvements%20in%20Image%20Annotation%20for%20Autonomous%20Driving.pdf" target="_blank" rel="noopener">Download our paper on Human-Centric Efficiency Improvements in Image Annotation for Autonomous Driving here</a> and stay tuned to hear more about our latest advances!</p><p></p><p><strong>References</strong></p><ol><li>Acuna, D., Ling, H., Kar, A., and Fidler, S. Efficient annotation of segmentation datasets with polygon-rnn++. In CVPR, 2018.</li><li>Ling, H., Gao, J., Kar, A., Chen, W., and Fidler, S. Fast interactive object annotation with curve-gcn. In CVPR, 2019.</li><li>Maninis, K.-K., Caelles, S., Pont-Tuset, J., and Van Gool, L. Deep extreme cut: From extreme points to object segmentation. In Computer Vision and Pattern Recognition (CVPR), 2018.</li><li>Papadopoulos, D., Uijlings, J., Keller, F., and Ferrari, V. Extreme clicking for efficient object annotation. In ICCV, 2017.</li><li>Ronneberger, O., Fischer, P., and Brox, T. U-net: Convolutional networks for biomedical image segmentation. CoRR, abs/1505.04597, 2015. URL http://arxiv. org/abs/1505.04597.</li><li>Tan, M. and Le, Q. V. Efficientnet: Rethinking model scaling for convolutional neural networks. CoRR, abs/1905.11946, 2019. URL http://arxiv.org/ abs/1905.11946.</li><li>Wang, Z., Acuna, D., Ling, H., Kar, A., and Fidler, S. Object instance annotation with deep extreme level set evolution. In CVPR, 2019.</li><li>Zolfaghari Bengar, J., Gonzalez-Garcia, A., Villalonga, G., Raducanu, B., Aghdam, H. H., Mozerov, M., Lopez, A. M., and van de Weijer, J. Temporal coherence for active learning in videos. arXiv preprint arXiv:1908.11757, 2019.</li></ol><p>This post was written by Frederic Ratle and Martine Bertrand.</p></div></div></section><section class="umoja-l-grid-section umoja-u-bg--white"><div class="umoja-l-grid--12"><div class="blog-post-footer_tags__yEMl0"><h3 class="blog-post-footer_intro__1dGYr">Filed Under:</h3><a class="blog-post-footer_tag__UlZZ0" href="/[object%20Object]">Vector Annotation</a><a class="blog-post-footer_tag__UlZZ0" href="/[object%20Object]">Polygons</a><a class="blog-post-footer_tag__UlZZ0" href="/[object%20Object]">Sama Engineering</a><a class="blog-post-footer_tag__UlZZ0" href="/[object%20Object]">Featured</a></div><div class="blog-post-footer_author__1Qqrq"><h3 class="blog-post-footer_intro__1dGYr">Words by:</h3><div class="blog-post-footer_author_inner__1tFQc"><div class="blog-post-footer_author_headshot__d-1KT"><div style="display:block;overflow:hidden;position:relative;box-sizing:border-box;margin:0"><div style="display:block;box-sizing:border-box;padding-top:100%"></div><img alt="" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="responsive" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover"/><noscript><img alt="" sizes="100vw" srcSet="https://cdn.sanity.io/images/76e3r62u/production/6643136d6c33f77b8e49366c166642ca5dafba8d-500x500.webp?w=640&amp;q=75&amp;fit=clip&amp;auto=format 640w, https://cdn.sanity.io/images/76e3r62u/production/6643136d6c33f77b8e49366c166642ca5dafba8d-500x500.webp?w=750&amp;q=75&amp;fit=clip&amp;auto=format 750w, https://cdn.sanity.io/images/76e3r62u/production/6643136d6c33f77b8e49366c166642ca5dafba8d-500x500.webp?w=828&amp;q=75&amp;fit=clip&amp;auto=format 828w, https://cdn.sanity.io/images/76e3r62u/production/6643136d6c33f77b8e49366c166642ca5dafba8d-500x500.webp?w=1080&amp;q=75&amp;fit=clip&amp;auto=format 1080w, https://cdn.sanity.io/images/76e3r62u/production/6643136d6c33f77b8e49366c166642ca5dafba8d-500x500.webp?w=1200&amp;q=75&amp;fit=clip&amp;auto=format 1200w, https://cdn.sanity.io/images/76e3r62u/production/6643136d6c33f77b8e49366c166642ca5dafba8d-500x500.webp?w=1920&amp;q=75&amp;fit=clip&amp;auto=format 1920w, https://cdn.sanity.io/images/76e3r62u/production/6643136d6c33f77b8e49366c166642ca5dafba8d-500x500.webp?w=2048&amp;q=75&amp;fit=clip&amp;auto=format 2048w, https://cdn.sanity.io/images/76e3r62u/production/6643136d6c33f77b8e49366c166642ca5dafba8d-500x500.webp?w=3840&amp;q=75&amp;fit=clip&amp;auto=format 3840w" src="https://cdn.sanity.io/images/76e3r62u/production/6643136d6c33f77b8e49366c166642ca5dafba8d-500x500.webp?w=3840&amp;q=75&amp;fit=clip&amp;auto=format" decoding="async" data-nimg="responsive" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover" loading="lazy"/></noscript></div></div><div class="blog-post-footer_author_bio__2z8Kp"><a href="/[object%20Object]"><h4>Frederic Ratle</h4></a><p>Frédéric is a researcher and team leader with over 15 years of R&amp;D experience in machine learning, AI, NLP, speech recognition, and computer vision. Currently Head of AI at Sama, he has worked on building ML-based products in multiple industries from Healthcare to Retail, in large companies and startups. He cares about the impact of technology, and outside of work, you can often see him on a bicycle or on skis.</p></div></div></div></div></section><section class="umoja-l-grid-section umoja-l-grid-section--flat-top umoja-u-bg--white"><div class="umoja-l-grid--12"><h3 class="blog-post_relatedPosts__19WKQ">Related Posts:</h3><div class="blog-smallCard-row_post__in_Mm"><a class="blog-smallCard-row_imgWrap__36jhQ" href="/[object%20Object]"><div style="display:block;overflow:hidden;position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;margin:0"><img alt="" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="fill" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover"/><noscript><img alt="" sizes="100vw" srcSet="https://cdn.sanity.io/images/76e3r62u/production/ae9f62c210539de6ed2b60b71efa4be6b90021c6-1920x960.png?w=640&amp;q=75&amp;fit=clip&amp;auto=format 640w, https://cdn.sanity.io/images/76e3r62u/production/ae9f62c210539de6ed2b60b71efa4be6b90021c6-1920x960.png?w=750&amp;q=75&amp;fit=clip&amp;auto=format 750w, https://cdn.sanity.io/images/76e3r62u/production/ae9f62c210539de6ed2b60b71efa4be6b90021c6-1920x960.png?w=828&amp;q=75&amp;fit=clip&amp;auto=format 828w, https://cdn.sanity.io/images/76e3r62u/production/ae9f62c210539de6ed2b60b71efa4be6b90021c6-1920x960.png?w=1080&amp;q=75&amp;fit=clip&amp;auto=format 1080w, https://cdn.sanity.io/images/76e3r62u/production/ae9f62c210539de6ed2b60b71efa4be6b90021c6-1920x960.png?w=1200&amp;q=75&amp;fit=clip&amp;auto=format 1200w, https://cdn.sanity.io/images/76e3r62u/production/ae9f62c210539de6ed2b60b71efa4be6b90021c6-1920x960.png?w=1920&amp;q=75&amp;fit=clip&amp;auto=format 1920w, https://cdn.sanity.io/images/76e3r62u/production/ae9f62c210539de6ed2b60b71efa4be6b90021c6-1920x960.png?w=2048&amp;q=75&amp;fit=clip&amp;auto=format 2048w, https://cdn.sanity.io/images/76e3r62u/production/ae9f62c210539de6ed2b60b71efa4be6b90021c6-1920x960.png?w=3840&amp;q=75&amp;fit=clip&amp;auto=format 3840w" src="https://cdn.sanity.io/images/76e3r62u/production/ae9f62c210539de6ed2b60b71efa4be6b90021c6-1920x960.png?w=3840&amp;q=75&amp;fit=clip&amp;auto=format" decoding="async" data-nimg="fill" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover" loading="lazy"/></noscript></div></a><div class="blog-smallCard-row_post_info__3V7Qy"><a href="/[object%20Object]"><h4>Accurate Data Labeling Powers the Volumental Shoe Recommendation App — Helping Retailers Convert Mobile Customers</h4></a><a class="blog-smallCard-row_tag__33FrA" href="/[object%20Object]">Case Studies</a></div></div><div class="blog-smallCard-row_post__in_Mm"><a class="blog-smallCard-row_imgWrap__36jhQ" href="/[object%20Object]"><div style="display:block;overflow:hidden;position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;margin:0"><img alt="" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="fill" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover"/><noscript><img alt="" sizes="100vw" srcSet="https://cdn.sanity.io/images/76e3r62u/production/0434b713b7dcba1cdf98eff0eb50d7cb3d5fb008-1500x908.jpg?w=640&amp;q=75&amp;fit=clip&amp;auto=format 640w, https://cdn.sanity.io/images/76e3r62u/production/0434b713b7dcba1cdf98eff0eb50d7cb3d5fb008-1500x908.jpg?w=750&amp;q=75&amp;fit=clip&amp;auto=format 750w, https://cdn.sanity.io/images/76e3r62u/production/0434b713b7dcba1cdf98eff0eb50d7cb3d5fb008-1500x908.jpg?w=828&amp;q=75&amp;fit=clip&amp;auto=format 828w, https://cdn.sanity.io/images/76e3r62u/production/0434b713b7dcba1cdf98eff0eb50d7cb3d5fb008-1500x908.jpg?w=1080&amp;q=75&amp;fit=clip&amp;auto=format 1080w, https://cdn.sanity.io/images/76e3r62u/production/0434b713b7dcba1cdf98eff0eb50d7cb3d5fb008-1500x908.jpg?w=1200&amp;q=75&amp;fit=clip&amp;auto=format 1200w, https://cdn.sanity.io/images/76e3r62u/production/0434b713b7dcba1cdf98eff0eb50d7cb3d5fb008-1500x908.jpg?w=1920&amp;q=75&amp;fit=clip&amp;auto=format 1920w, https://cdn.sanity.io/images/76e3r62u/production/0434b713b7dcba1cdf98eff0eb50d7cb3d5fb008-1500x908.jpg?w=2048&amp;q=75&amp;fit=clip&amp;auto=format 2048w, https://cdn.sanity.io/images/76e3r62u/production/0434b713b7dcba1cdf98eff0eb50d7cb3d5fb008-1500x908.jpg?w=3840&amp;q=75&amp;fit=clip&amp;auto=format 3840w" src="https://cdn.sanity.io/images/76e3r62u/production/0434b713b7dcba1cdf98eff0eb50d7cb3d5fb008-1500x908.jpg?w=3840&amp;q=75&amp;fit=clip&amp;auto=format" decoding="async" data-nimg="fill" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover" loading="lazy"/></noscript></div></a><div class="blog-smallCard-row_post_info__3V7Qy"><a href="/[object%20Object]"><h4>Orbisk’s Sama-Powered Food Waste Solution Named to Fast Company’s First-Ever List of the Next Big Things in Tech</h4></a><a class="blog-smallCard-row_tag__33FrA" href="/[object%20Object]">Awards</a></div></div><div class="blog-smallCard-row_post__in_Mm"><a class="blog-smallCard-row_imgWrap__36jhQ" href="/[object%20Object]"><div style="display:block;overflow:hidden;position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;margin:0"><img alt="" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="fill" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover"/><noscript><img alt="" sizes="100vw" srcSet="https://cdn.sanity.io/images/76e3r62u/production/1502043edcf701cb4267cb2f6d7c6edf8e0e0cad-2500x1313.png?w=640&amp;q=75&amp;fit=clip&amp;auto=format 640w, https://cdn.sanity.io/images/76e3r62u/production/1502043edcf701cb4267cb2f6d7c6edf8e0e0cad-2500x1313.png?w=750&amp;q=75&amp;fit=clip&amp;auto=format 750w, https://cdn.sanity.io/images/76e3r62u/production/1502043edcf701cb4267cb2f6d7c6edf8e0e0cad-2500x1313.png?w=828&amp;q=75&amp;fit=clip&amp;auto=format 828w, https://cdn.sanity.io/images/76e3r62u/production/1502043edcf701cb4267cb2f6d7c6edf8e0e0cad-2500x1313.png?w=1080&amp;q=75&amp;fit=clip&amp;auto=format 1080w, https://cdn.sanity.io/images/76e3r62u/production/1502043edcf701cb4267cb2f6d7c6edf8e0e0cad-2500x1313.png?w=1200&amp;q=75&amp;fit=clip&amp;auto=format 1200w, https://cdn.sanity.io/images/76e3r62u/production/1502043edcf701cb4267cb2f6d7c6edf8e0e0cad-2500x1313.png?w=1920&amp;q=75&amp;fit=clip&amp;auto=format 1920w, https://cdn.sanity.io/images/76e3r62u/production/1502043edcf701cb4267cb2f6d7c6edf8e0e0cad-2500x1313.png?w=2048&amp;q=75&amp;fit=clip&amp;auto=format 2048w, https://cdn.sanity.io/images/76e3r62u/production/1502043edcf701cb4267cb2f6d7c6edf8e0e0cad-2500x1313.png?w=3840&amp;q=75&amp;fit=clip&amp;auto=format 3840w" src="https://cdn.sanity.io/images/76e3r62u/production/1502043edcf701cb4267cb2f6d7c6edf8e0e0cad-2500x1313.png?w=3840&amp;q=75&amp;fit=clip&amp;auto=format" decoding="async" data-nimg="fill" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover" loading="lazy"/></noscript></div></a><div class="blog-smallCard-row_post_info__3V7Qy"><a href="/[object%20Object]"><h4>New Podcast Episode: Facebook&#x27;s Manohar Paluri Makes Machines See</h4></a><a class="blog-smallCard-row_tag__33FrA" href="/[object%20Object]">Podcast</a></div></div></div></section></main><footer class="footer_wrapper__2VAfJ"><div class="umoja-l-grid--12"><div class="footer_upper__2a6XG"><div><h4>Newsletter</h4><p>Subscribe today and be the first to receive the latest from Sama.</p></div><div class="footer_upper_right__cpliC"><div><p class="footer_nav_head__1keQK">Guides</p><a class="footer_nav_link__X1RNI" href="/training-data-for-autonomous-driving">Autonomous Transportation</a><a class="footer_nav_link__X1RNI" href="/training-data-for-ecommerce">E-Commerce</a><a class="footer_nav_link__X1RNI" href="/training-data-for-ar-vr">AR/VR</a><a class="footer_nav_link__X1RNI" href="/data-quality">Data Quality</a></div><div><p class="footer_nav_head__1keQK">Company</p><a class="footer_nav_link__X1RNI" href="/our-story">Our Story</a><a class="footer_nav_link__X1RNI" href="/our-team">Our Team</a><a class="footer_nav_link__X1RNI" href="/mission-vision-values">Our Mission</a><a class="footer_nav_link__X1RNI" href="/careers">Careers</a><a class="footer_nav_link__X1RNI" href="/company-contact">Contact</a></div></div></div><div class="footer_middle__iiTSJ"><div class="footer_middle_left__3ff78"><a href="/"></a></div><div class="footer_middle_right__2b-lC"><div class="footer_social__1NFfV"><a href="https://www.facebook.com/samaartificialintelligence" class="footer_social_icon__wI2OK" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 35.1 64.89"><title>facebook</title><g id="fb3209c8-23d4-4288-984d-a2b8f32b7f0c" data-name="Layer 2"><g id="ba1a815d-13f1-45f9-9321-18e9a3cfa5db" data-name="Layer 1"><path d="M35.1,11.26V1.36A1.35,1.35,0,0,0,33.76,0H25.35A15.34,15.34,0,0,0,14,4.35C11.24,7.2,9.78,11.22,9.78,16v7.34H1.34A1.34,1.34,0,0,0,0,24.66V35.32a1.35,1.35,0,0,0,1.34,1.35H9.78V63.55a1.34,1.34,0,0,0,1.34,1.34h11a1.34,1.34,0,0,0,1.34-1.34V36.67h9.87a1.35,1.35,0,0,0,1.34-1.35V24.66a1.37,1.37,0,0,0-.7-1.18,1.47,1.47,0,0,0-.67-.16H23.49V17.1c0-1.72.25-2.69.84-3.37s1.88-1.13,3.77-1.13h5.66A1.34,1.34,0,0,0,35.1,11.26Z"></path></g></g></svg></a><a href="https://www.instagram.com/sama_ai_" class="footer_social_icon__wI2OK" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 57 57"><title>insta</title><g id="ef02a3ef-c0d3-4be7-9d4e-2c42263777a3" data-name="Layer 2"><g id="a0a92778-6c5c-4e06-a08f-641546580dd0" data-name="Layer 1"><circle cx="28.5" cy="28.5" r="9.24"></circle><path d="M41.57,0H15.43A15.45,15.45,0,0,0,0,15.43V41.57A15.45,15.45,0,0,0,15.43,57H41.57A15.45,15.45,0,0,0,57,41.57V15.43A15.45,15.45,0,0,0,41.57,0ZM28.5,42.74A14.24,14.24,0,1,1,42.74,28.5,14.26,14.26,0,0,1,28.5,42.74ZM44.46,17a5,5,0,1,1,5-5A5,5,0,0,1,44.46,17Z"></path></g></g></svg></a><a href="https://twitter.com/SamaAI" class="footer_social_icon__wI2OK" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 62 51.19"><title>twitter</title><g id="e7743e55-1863-47ad-a995-4b7f1f924f07" data-name="Layer 2"><g id="ec8c6fd9-2f52-4507-9a24-804bc60dbc33" data-name="Layer 1"><path d="M23.59,51.19c-10.35,0-18.53-1.81-22.44-5l-.07-.06L1,46.1a3.19,3.19,0,0,1-.84-3.35l0-.1a3.24,3.24,0,0,1,3-2,26.57,26.57,0,0,0,7.06-1,13.45,13.45,0,0,1-7.07-8.16,2.92,2.92,0,0,1,1-3.38,3.06,3.06,0,0,1,.88-.45,19.52,19.52,0,0,1-4-7.18l0-.08,0-.09a3,3,0,0,1,1.4-3.23,3,3,0,0,1,1.43-.4,15.15,15.15,0,0,1-1.14-3.49A14.59,14.59,0,0,1,4.24,3.47l.38-.77a2.15,2.15,0,0,1,3.44-.56l.7.7c5.53,5.81,10.49,8.56,19.06,10.44a15.17,15.17,0,0,1,4.1-8.75A14.39,14.39,0,0,1,42.19,0h0c2.84,0,6.36,1.62,8.49,2.77,1.83-.6,4-1.53,6.32-2.51a2.88,2.88,0,0,1,3.22.57,2.85,2.85,0,0,1,.62,3.11c-.17.47-.36.92-.57,1.36a3.07,3.07,0,0,1,.84.58,3.13,3.13,0,0,1,.78,2.92l0,.1a11.92,11.92,0,0,1-4.78,6.56C56.73,35.23,41.84,51.19,23.59,51.19Z"></path></g></g></svg></a><a href="https://www.linkedin.com/company/sama-ai/" class="footer_social_icon__wI2OK" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 59.71 60.79"><title>linkedin</title><g id="bd073dff-b6ea-4cf0-bfeb-c3ce392ee6a5" data-name="Layer 2"><g id="f6b6eb77-7c10-4e27-a3d0-6f8b14e97f2e" data-name="Layer 1"><path d="M59.65,60.79l-12.35,0,0-19.36c0-4.62-.07-10.56-6.41-10.58s-7.44,5-7.45,10.21l0,19.7-12.36,0,.09-40.95,11.87,0v6.57h.16c1.66-3.13,5.7-6.42,11.73-6.41,12.51,0,14.81,8.28,14.79,19l-.05,21.85Z"></path><path d="M7.17,14.35a7.18,7.18,0,1,1,7.18-7.18A7.17,7.17,0,0,1,7.17,14.35Z"></path><rect x="0.98" y="19.8" width="12.39" height="40.95"></rect></g></g></svg></a><a href="https://www.youtube.com/c/SamaAI" class="footer_social_icon__wI2OK" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 65 44.63"><title>youtube</title><g id="b03ac260-8029-40d4-832d-1f2d757db2d5" data-name="Layer 2"><g id="eca1bfe4-f0ec-4c24-9169-be6d2f8b24b5" data-name="Layer 1"><path d="M55,0H10A10,10,0,0,0,0,10V34.63a10,10,0,0,0,10,10H55a10,10,0,0,0,10-10V10A10,10,0,0,0,55,0ZM40.89,24.41,28.3,31.18a2.31,2.31,0,0,1-3.41-2V15.48a2.3,2.3,0,0,1,3.42-2l12.6,6.89a2.31,2.31,0,0,1,0,4.06Z"></path></g></g></svg></a></div></div></div><div class="footer_lower__1z3Av"><div class="footer_lower_left__141hE"><a class="footer_nav_link__X1RNI" href="/terms-of-service">Terms</a><a class="footer_nav_link__X1RNI" href="/privacy-policy">Privacy</a><a class="footer_nav_link__X1RNI" href="/quality-and-information-policy">Quality &amp; Information</a></div><div class="footer_lower_right__22vMw"><h6>Copyright © <!-- -->0<!-- --> Sama Inc.</h6><h6>All rights reserved.</h6></div></div></div></footer></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"config":{"footerNav":{"items":[{"_key":"f255606f8f25","_type":"navDropdownMenu","items":[{"_key":"76389ad94cbb","_type":"navItem","title":"Autonomous Transportation","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"training-data-for-autonomous-driving"}}}},{"_key":"5f64a8d6a69d","_type":"navItem","title":"E-Commerce","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"training-data-for-ecommerce"}}}},{"_key":"f10e54ae04d0","_type":"navItem","title":"AR/VR","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"training-data-for-ar-vr"}}}},{"_key":"fd729b522a77","_type":"navItem","title":"Data Quality","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"data-quality"}}}}],"title":"Guides","url":null},{"_key":"681ef7d8763a","_type":"navDropdownMenu","items":[{"_key":"6238a422b667","_type":"navItem","title":"Our Story","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"our-story"}}}},{"_key":"258985d6d46b","_type":"navItem","title":"Our Team","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"our-team"}}}},{"_key":"e0a76077324a","_type":"navItem","title":"Our Mission","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"mission-vision-values"}}}},{"_key":"239e49661b0d","_type":"navItem","title":"Careers","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"careers"}}}},{"_key":"e005a740cd80","_type":"navItem","title":"Contact","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"company-contact"}}}}],"title":"Company","url":null}]},"logo":{"asset":{"_createdAt":"2021-10-29T18:38:04Z","_id":"image-e20f8cc53e5f74df10ae9a822edb7ec2c4d00f02-2380x636-svg","_rev":"yuZxWYwFNB6KJB4TM9NtaY","_type":"sanity.imageAsset","_updatedAt":"2021-10-29T18:38:04Z","assetId":"e20f8cc53e5f74df10ae9a822edb7ec2c4d00f02","extension":"svg","metadata":{"_type":"sanity.imageMetadata","dimensions":{"_type":"sanity.imageDimensions","aspectRatio":3.742138364779874,"height":636,"width":2380},"hasAlpha":true,"isOpaque":false,"lqip":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAFCAYAAABFA8wzAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAr0lEQVQYlU3QyUoDQBAE0HeIEjeixAVyESRI3BNFJSZxAU/+/wdJQSEehu6Zqa6uKnjBGz7xgTuc4gAj7LWe4LD9cc8Y+9jBUed84QGrEt/gEleYY9Ylr3jGU/tV77fFLirId0nWBYYgoACi+r3D6YPN0vwFm4VxmNlgfkK4qcLUZdVMcdHB+75FzWPfzzCpxXNcFxuOP2uxnhNLASaX5LjbnJJf6jYG2PpXh812/AvSEQ+GGZqgYgAAAABJRU5ErkJggg==","palette":{"_type":"sanity.imagePalette","darkMuted":{"_type":"sanity.imagePaletteSwatch","background":"#040404","foreground":"#fff","population":100.29,"title":"#fff"},"darkVibrant":{"_type":"sanity.imagePaletteSwatch","background":"#424242","foreground":"#fff","population":0,"title":"#fff"},"dominant":{"_type":"sanity.imagePaletteSwatch","background":"#040404","foreground":"#fff","population":100.29,"title":"#fff"},"lightMuted":{"_type":"sanity.imagePaletteSwatch","background":"#4c4c4c","foreground":"#fff","population":0,"title":"#fff"},"lightVibrant":{"_type":"sanity.imagePaletteSwatch","background":"#bcbcbc","foreground":"#000","population":0,"title":"#fff"},"muted":{"_type":"sanity.imagePaletteSwatch","background":"#4c4c4c","foreground":"#fff","population":0,"title":"#fff"},"vibrant":{"_type":"sanity.imagePaletteSwatch","background":"#7f7f7f","foreground":"#fff","population":0,"title":"#fff"}}},"mimeType":"image/svg+xml","originalFilename":"image.svg","path":"images/76e3r62u/production/e20f8cc53e5f74df10ae9a822edb7ec2c4d00f02-2380x636.svg","sha1hash":"ae6a56857a230101a883a9b93974923879775bc9","size":2009,"uploadId":"mtOtmqAQnCEIG5cEqXZ1YAOCuqHJ4X3g","url":"https://cdn.sanity.io/images/76e3r62u/production/e20f8cc53e5f74df10ae9a822edb7ec2c4d00f02-2380x636.svg"}},"mainNav":{"items":[{"_key":"58c18e9aa9ea","_type":"navDropdownMenu","items":[{"_key":"b5b5b8bee78b","_type":"navCat","items":[{"_key":"0e80156a2f1a","_type":"navItem","title":"How it Works","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"how-it-works"}}}},{"_key":"40bacee029b4","_type":"navItem","title":"Video Annotation","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"video-annotation"}}}},{"_key":"32650ef07503","_type":"navItem","title":"Image Annotation","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"image-annotation"}}}},{"_key":"fe9137cd0167","_type":"navItem","title":"3D \u0026 LiDAR Annotation","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"3d-lidar"}}}},{"_key":"d9a1316d400a","_type":"navItem","title":"Natural Language Processing","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"natural-language-processing"}}}},{"_key":"ac12c7c5d70a","_type":"navItem","title":"Data Curation (Beta)","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"data-curation"}}}}],"title":"Platform","url":null},{"_key":"37ff4fa913bd","_type":"navCat","items":[{"_key":"6026b1a9314e","_type":"navItem","title":"Semantic Segmentation","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"semantic-segmentation"}}}},{"_key":"f4611b19b406","_type":"navItem","title":"Polygons","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"polygons"}}}},{"_key":"5155d874d6c8","_type":"navItem","title":"Bounding Boxes","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"bounding-boxes"}}}},{"_key":"9ef3c1e21e74","_type":"navItem","title":"Key Points","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"key-points"}}}},{"_key":"314d4c00d351","_type":"navItem","title":"Cuboids","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"cuboids"}}}},{"_key":"8e17a6388d74","_type":"navItem","title":"Lines \u0026 Arrows","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"lines-and-arrows"}}}}],"title":"Shapes","url":null}],"title":"Platform","url":null},{"_key":"112867ca4d03","_type":"navDropdownMenu","items":[{"_key":"22699c7e06cb","_type":"navItem","items":null,"title":"Transportation \u0026 Navigation","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"transportation-navigation"}}}},{"_key":"122ae5928d6d","_type":"navItem","items":null,"title":"Retail \u0026 E-Commerce","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"retail-ecommerce"}}}},{"_key":"7bb234b69fb0","_type":"navItem","items":null,"title":"Consumer \u0026 Media","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"consumer-media"}}}},{"_key":"33e6a886b39d","_type":"navItem","items":null,"title":"Biotech \u0026 Medtech","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"biotech-medtech"}}}},{"_key":"d095b2619c4e","_type":"navItem","items":null,"title":"Robotics \u0026 Manufacturing","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"robotics-and-manufacturing"}}}},{"_key":"2c4b82a94d79","_type":"navItem","items":null,"title":"Food \u0026 Agriculture","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"training-data-food-agriculture"}}}}],"title":"Industries","url":null},{"_key":"c47e8763a906","_type":"navDropdownMenu","items":[{"_key":"1d563df30b3f","_type":"navItem","items":null,"title":"Quality","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"quality-training-data"}}}},{"_key":"041725f35d96","_type":"navItem","items":null,"title":"Security","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"security-and-trust"}}}},{"_key":"fd64ede25798","_type":"navItem","items":null,"title":"Ethical AI","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"our-impact"}}}},{"_key":"398dcbb1c95d","_type":"navItem","items":null,"title":"Compare","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"compare"}}}},{"_key":"93bdfdd87879","_type":"navItem","items":null,"title":"Partners","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"partners"}}}}],"title":"Why Sama","url":null},{"_key":"1d38bf63df54","_type":"navDropdownMenu","items":[{"_key":"be81659b38a5","_type":"navItem","items":null,"title":"API Documentation","url":{"_type":"link","externalUrl":"https://docs.sama.com/reference/overview","internalLink":null}},{"_key":"2cec80e94962","_type":"navItem","items":null,"title":"Blog","url":{"_type":"link","internalLink":null,"internalLink_custom":"/blog"}},{"_key":"09e284fcb1d3","_type":"navItem","items":null,"title":"Events","url":{"_type":"link","internalLink":null,"internalLink_custom":"/events"}}],"title":"Resources","url":null},{"_key":"dbee93713c19","_type":"navDropdownMenu","items":[{"_key":"12d594a568bf","_type":"navItem","items":null,"title":"Our Story","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"our-story"}}}},{"_key":"ce36540a102d","_type":"navItem","items":null,"title":"Our Team","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"our-team"}}}},{"_key":"34fc328e8022","_type":"navItem","items":null,"title":"Careers","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"careers"}}}},{"_key":"c1fe2961020a","_type":"navItem","items":null,"title":"Contact","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"company-contact"}}}},{"_key":"ebd81873e538","_type":"navItem","items":null,"title":"Press","url":{"_type":"link","internalLink":null,"internalLink_custom":"/press"}}],"title":"Company","url":null}],"nav_cta":{"_type":"button","link":{"_type":"link","internalLink":{"_ref":"136788cb-06a6-4f27-b75b-07faf403bfa6","_type":"reference"}},"title":"Request a Demo","type":"secondary"}}},"data":{"post":{"_createdAt":"2020-12-15T21:43:21Z","author":{"_id":"a3099d34-9595-4978-b517-e508196414c1","avatar":{"_type":"image","asset":{"_ref":"image-6643136d6c33f77b8e49366c166642ca5dafba8d-500x500-webp","_type":"reference"}},"bio":"Frédéric is a researcher and team leader with over 15 years of R\u0026D experience in machine learning, AI, NLP, speech recognition, and computer vision. Currently Head of AI at Sama, he has worked on building ML-based products in multiple industries from Healthcare to Retail, in large companies and startups. He cares about the impact of technology, and outside of work, you can often see him on a bicycle or on skis.","name":"Frederic Ratle","slug":{"_type":"slug","current":"frederic-ratle"}},"body":[{"_key":"eebc5c4b61c8","_type":"block","children":[{"_key":"63071f4b9b3b","_type":"span","marks":[],"text":"At Sama, Vector Annotation of objects using polygons is a task that our expert annotators spend a great deal of time on. This is especially true for projects involving autonomous vehicles, where it is typical to apply instance segmentation to label scenes comprising hundreds of frames, each with multiple objects (vehicles, pedestrians, traffic signs, etc.) like you see in Figure 1."}],"markDefs":[],"style":"normal"},{"_key":"e4803d20524c","_type":"image","asset":{"_ref":"image-3a4b80bf2e9cb5cebab901de01ab7f5b4fb8c5c9-1475x758-png","_type":"reference"}},{"_key":"b354b6fba58c","_type":"block","children":[{"_key":"42ffa496c529","_type":"span","marks":["strong"],"text":"Figure 1"},{"_key":"40b0f9d11a28","_type":"span","marks":[],"text":". Example of Polygonal Annotation in Sama."}],"markDefs":[],"style":"h6"},{"_key":"cbe433c0d8ad","_type":"block","children":[{"_key":"419d4d3c71bb0","_type":"span","marks":[],"text":""}],"markDefs":[],"style":"normal"},{"_key":"31a034e0f206","_type":"block","children":[{"_key":"7497db35e5e10","_type":"span","marks":[],"text":"In this post, we summarize an approach that we have developed to speed up polygonal instance segmentation using machine learning. This approach was presented earlier this year at the "},{"_key":"68bf56458f0e","_type":"span","marks":["594f0769bd80"],"text":"CVPR Workshop on Scalability in Autonomous Driving"},{"_key":"0a349de379b0","_type":"span","marks":[],"text":", and the "},{"_key":"cf700d61e8da","_type":"span","marks":["a94fd912ecfb"],"text":"ICML Workshop on Human-in-the-Loop Learning"},{"_key":"7497db35e5e11","_type":"span","marks":["em"],"text":"."}],"markDefs":[{"_key":"594f0769bd80","_type":"button_link","externalUrl":"http://cvpr2020.wad.vision/"},{"_key":"a94fd912ecfb","_type":"button_link","externalUrl":"https://icml.cc/"}],"style":"normal"},{"_key":"179a41949f08","_type":"block","children":[{"_key":"00b877c28ca30","_type":"span","marks":[],"text":"Few-Click Annotation"}],"markDefs":[],"style":"h4"},{"_key":"cecc97391bc6","_type":"block","children":[{"_key":"e4a08f9c20d80","_type":"span","marks":[],"text":"Building instance segmentation Deep Learning (DL) models for autonomous vehicles requires a significant amount of labeled data. The use of Machine Learning (ML) for producing pre-annotations to be reviewed by human annotators, whether in an interactive setting or as a pre-processing procedure, is a very popular approach for scaling up labeling while controlling the costs."}],"markDefs":[],"style":"normal"},{"_key":"4b3a24ca8cba","_type":"block","children":[{"_key":"539f877a007d0","_type":"span","marks":[],"text":"Multiple approaches have been suggested for machine-assisted instance segmentation. These typically consist of a DL-based segmentation of the object(s) integrated into a human-in-the-loop system. The human can interact with the system by correcting the model output, initializing the model with one or several clicks, or a combination of those steps. Examples of such systems include Polygon-RNN++ [1], DELSE [7], DEXTR [3], and CurveGCN [2]. Those systems all present good results, but some open questions remain:"}],"markDefs":[],"style":"normal"},{"_key":"c92f55769104","_type":"block","children":[{"_key":"5f69d213a5bd0","_type":"span","marks":[],"text":"Do these methods perform well when a production-level accuracy is required, as when working for a customer project?"}],"level":1,"listItem":"bullet","markDefs":[],"style":"normal"},{"_key":"36decf65e948","_type":"block","children":[{"_key":"ff0f0393404c0","_type":"span","marks":[],"text":"Does the choice of annotation tool influence the results? The gains to be made by using ML depend on how difficult it is for humans to draw polygons in the provided UI. Here we used our optimized drawing tool for polygons, which is part of our labeling platform."}],"level":1,"listItem":"bullet","markDefs":[],"style":"normal"},{"_key":"16106c03b772","_type":"block","children":[{"_key":"e181c4c8f4bc0","_type":"span","marks":[],"text":"ML integration is not usually approached from a human-centric perspective. Beyond the optimization of traditional metrics like IoU, what interactions are most desirable and how should we present the output of the model to annotators?"}],"level":1,"listItem":"bullet","markDefs":[],"style":"normal"},{"_key":"d64fd39a61f4","_type":"block","children":[{"_key":"01ee6afc1a0d0","_type":"span","marks":[],"text":"Our method relies on combining the well-known DEXTR [3] approach with a raster-to-polygon algorithm, to make the result more easily editable. This is not unlike what other tools (such as CVAT) have implemented, though we have optimized this approach for our specific use cases using A/B testing."}],"markDefs":[],"style":"normal"},{"_key":"3ac01570dffd","_type":"block","children":[{"_key":"59eaa44a4c260","_type":"span","marks":[],"text":"The Model"}],"markDefs":[],"style":"h4"},{"_key":"54d3bffc72f6","_type":"block","children":[{"_key":"1462d2b372cd0","_type":"span","marks":[],"text":"Our instance segmentation model is based on the well-known Deep Extreme Cut (DEXTR) approach [3], along with a raster-to-polygon conversion algorithm that yields high quality polygons whose vertices are sampled in a way that reproduces human drawing patterns. The model uses the few clicks provided by human annotators at inference time. The steps are described in Figure 2."}],"markDefs":[],"style":"normal"},{"_key":"f1a9600d373f","_type":"image","asset":{"_ref":"image-2385ed2f2128573a154324bb5fbbded2e611e9fa-1424x638-png","_type":"reference"}},{"_key":"e911991f8b70","_type":"block","children":[{"_key":"4bf3633cbab3","_type":"span","marks":["strong"],"text":"Figure 2"},{"_key":"17324ee29aa9","_type":"span","marks":[],"text":". An overview of the approach."}],"markDefs":[],"style":"h6"},{"_key":"ae73a6896af9","_type":"block","children":[{"_key":"cb111cf90433","_type":"span","marks":[],"text":""}],"markDefs":[],"style":"normal"},{"_key":"8c802d2aaab5","_type":"block","children":[{"_key":"5f359a2eba5a0","_type":"span","marks":[],"text":"Regarding the model itself, we adopted a custom version of the UNet [5] along with an EfficientNet backbone [6] (instead of the ResNet backbone used in the paper)."}],"markDefs":[],"style":"normal"},{"_key":"19f0856e9d21","_type":"block","children":[{"_key":"2a36d4c4c0180","_type":"span","marks":[],"text":"In our experience, for human annotators to produce good instance segmentation masks efficiently, a polygon annotation tool should be used. As such, we needed to convert the raster masks produced by our model to high quality polygons. To add to the challenge, humans tend to produce sparse polygons, adding vertices only when necessary. We therefore adopted a raster-to-polygon procedure that minimizes the number of output vertices."}],"markDefs":[],"style":"normal"},{"_key":"3303f1c4fc82","_type":"block","children":[{"_key":"8838a493180a0","_type":"span","marks":[],"text":"A/B Testing the Approach"}],"markDefs":[],"style":"h4"},{"_key":"44a37c3a6108","_type":"block","children":[{"_key":"0da12b52688d0","_type":"span","marks":[],"text":"At Sama, we use A/B testing as much as possible to systematically refine and improve our new features. To this end, we have developed a flexible testing infrastructure that can ingest and aggregate data from multiple internal processes and is made available to anyone within the organization."}],"markDefs":[],"style":"normal"},{"_key":"17d662e807d5","_type":"block","children":[{"_key":"69b1f49269f20","_type":"span","marks":[],"text":"This framework measures the statistical impact of proposed changes on our efficiency metrics (such as drawing time or shape adjustment time). The significance of observed differences on a given efficiency metric is evaluated using statistical tests."}],"markDefs":[],"style":"normal"},{"_key":"1180ca13ba2f","_type":"block","children":[{"_key":"ea083eb414500","_type":"span","marks":[],"text":"Toy A/B Tests"}],"markDefs":[],"style":"h5"},{"_key":"23882d62cd08","_type":"block","children":[{"_key":"0acaf35e0ba60","_type":"span","marks":[],"text":"We conducted an A/B test of the method using a synthetic automotive dataset called SYNTHIA-AL [8]. The dataset's images and corresponding annotations were generated from video streams at 25 frames per second (FPS). Figure 3 shows SYNTHIA image examples, along with their segmentation (done manually and with the Few-Click tool)."}],"markDefs":[],"style":"normal"},{"_key":"f1a4316ae458","_type":"image","asset":{"_ref":"image-ad6b4a09778cefac7979292c68c15ba0dcfa5ddd-512x231-png","_type":"reference"}},{"_key":"808d376bf982","_type":"block","children":[{"_key":"3a1413f967fe","_type":"span","marks":["strong"],"text":"Figure 3"},{"_key":"8601872f2fc4","_type":"span","marks":[],"text":". SYNTHIA example images, along with their manual and semi-automated annotations."}],"markDefs":[],"style":"h6"},{"_key":"526d134806e9","_type":"block","children":[{"_key":"ca49d87a7f95","_type":"span","marks":[],"text":""}],"markDefs":[],"style":"normal"},{"_key":"bae3a97f7e6f","_type":"block","children":[{"_key":"d9ee99bb3525","_type":"span","marks":[],"text":"The test, applied only to motor vehicles, reproduced realistic annotation guidelines, namely:"}],"markDefs":[],"style":"normal"},{"_key":"ecf8e8ea82b1","_type":"block","children":[{"_key":"49e2d5ac21660","_type":"span","marks":[],"text":"The drawn polygon needs to be within 2 pixels of the edge of the vehicle."}],"level":1,"listItem":"bullet","markDefs":[],"style":"normal"},{"_key":"dab9087621d0","_type":"block","children":[{"_key":"ba3c3b0ac1c40","_type":"span","marks":[],"text":"All vehicles down to 10 pixels (height or width) need to be annotated."}],"level":1,"listItem":"bullet","markDefs":[],"style":"normal"},{"_key":"f9b86647ddeb","_type":"block","children":[{"_key":"5cbfbf2c7af60","_type":"span","marks":[],"text":"Following this test, we found a nearly 3-fold reduction in annotation time. On the other hand, we also found that on some of the more complex shapes, annotators were spending quite some time manually adjusting the ML output. DEXTR's authors originally showed that the segmentation can be improved with additional clicks beyond the four initial ones. We therefore extended our few-click tool to allow online refinement of the polygons by considering modifications to their vertices as extra clicks. At train time we simulated the corrective clicks by considering the point of greatest deviation between predicted mask and ground truth as illustrated in Figure 4."}],"markDefs":[],"style":"normal"},{"_key":"71c13a4d8416","_type":"block","children":[{"_key":"057816f9b3890","_type":"span","marks":["strong"],"text":"Problem"},{"_key":"057816f9b3891","_type":"span","marks":[],"text":": DEXTR’s 4 extreme clicks are not always sufficient."}],"level":1,"listItem":"bullet","markDefs":[],"style":"normal"},{"_key":"68de997cd6fe","_type":"block","children":[{"_key":"9d1bcff9fd4c0","_type":"span","marks":["strong"],"text":"Observation"},{"_key":"9d1bcff9fd4c1","_type":"span","marks":[],"text":": DEXTR trained on 4 clicks benefits from additional clicks."}],"level":1,"listItem":"bullet","markDefs":[],"style":"normal"},{"_key":"1b1455f60e96","_type":"block","children":[{"_key":"cc55125cc4340","_type":"span","marks":["strong"],"text":"Solution"},{"_key":"cc55125cc4341","_type":"span","marks":[],"text":": Fine-tune DEXTR model with additional clicks for hard samples as established by IoU at train time."}],"level":1,"listItem":"bullet","markDefs":[],"style":"normal"},{"_key":"8e6a368385b1","_type":"image","asset":{"_ref":"image-9761c7350f514786633b12b6f17058b6b67d6677-627x427-png","_type":"reference"}},{"_key":"0802009112d6","_type":"block","children":[{"_key":"7b5b2d61f4d8","_type":"span","marks":["strong"],"text":"Figure 4"},{"_key":"2ed23d83d06b","_type":"span","marks":[],"text":". Integrating additional clicks in the training process.\n"}],"markDefs":[],"style":"h6"},{"_key":"361f9f2ea9ff","_type":"block","children":[{"_key":"35e7aac5405c0","_type":"span","marks":[],"text":"\nUsing this method, annotators are able to re-trigger the model inference with an additional click, instead of manually adjusting the output. We proceeded to a second toy A/B test, and results showed that we could obtain a theoretical efficiency gain of up to 3.5x on vehicles using the improved method.\n\n"},{"_key":"8aaf238574a9","_type":"span","marks":["4c5ab5c85862"],"text":"Download our paper on Human-Centric Efficiency Improvements in Image Annotation for Autonomous Driving here"},{"_key":"45b4ebd5c155","_type":"span","marks":[],"text":" and stay tuned to hear more about our latest advances!"}],"markDefs":[{"_key":"4c5ab5c85862","_type":"button_link","externalUrl":"https://www.sama.com/hubfs/Research%20Papers/Human-Centric%20Efficiency%20Improvements%20in%20Image%20Annotation%20for%20Autonomous%20Driving.pdf"}],"style":"normal"},{"_key":"5781b73520e3","_type":"block","children":[{"_key":"81c33d4ca3f7","_type":"span","marks":[],"text":""}],"markDefs":[],"style":"normal"},{"_key":"b94fb50cbcb5","_type":"block","children":[{"_key":"ef9bee866628","_type":"span","marks":["strong"],"text":"References"}],"markDefs":[],"style":"normal"},{"_key":"e03bf7bdcf2c","_type":"block","children":[{"_key":"8554d7938d6a0","_type":"span","marks":[],"text":"Acuna, D., Ling, H., Kar, A., and Fidler, S. Efficient annotation of segmentation datasets with polygon-rnn++. In CVPR, 2018."}],"level":1,"listItem":"number","markDefs":[],"style":"normal"},{"_key":"68d19051c1d7","_type":"block","children":[{"_key":"8114c10f973d0","_type":"span","marks":[],"text":"Ling, H., Gao, J., Kar, A., Chen, W., and Fidler, S. Fast interactive object annotation with curve-gcn. In CVPR, 2019."}],"level":1,"listItem":"number","markDefs":[],"style":"normal"},{"_key":"d319ee43a89e","_type":"block","children":[{"_key":"52a5f1d1ca4b0","_type":"span","marks":[],"text":"Maninis, K.-K., Caelles, S., Pont-Tuset, J., and Van Gool, L. Deep extreme cut: From extreme points to object segmentation. In Computer Vision and Pattern Recognition (CVPR), 2018."}],"level":1,"listItem":"number","markDefs":[],"style":"normal"},{"_key":"b75b2ef61c86","_type":"block","children":[{"_key":"2654c54524690","_type":"span","marks":[],"text":"Papadopoulos, D., Uijlings, J., Keller, F., and Ferrari, V. Extreme clicking for efficient object annotation. In ICCV, 2017."}],"level":1,"listItem":"number","markDefs":[],"style":"normal"},{"_key":"7e687f6aaa7a","_type":"block","children":[{"_key":"6d23551b69520","_type":"span","marks":[],"text":"Ronneberger, O., Fischer, P., and Brox, T. U-net: Convolutional networks for biomedical image segmentation. CoRR, abs/1505.04597, 2015. URL http://arxiv. org/abs/1505.04597."}],"level":1,"listItem":"number","markDefs":[],"style":"normal"},{"_key":"8b32ad5b452b","_type":"block","children":[{"_key":"52f98de763ee0","_type":"span","marks":[],"text":"Tan, M. and Le, Q. V. Efficientnet: Rethinking model scaling for convolutional neural networks. CoRR, abs/1905.11946, 2019. URL http://arxiv.org/ abs/1905.11946."}],"level":1,"listItem":"number","markDefs":[],"style":"normal"},{"_key":"fc2fc1479556","_type":"block","children":[{"_key":"165a0bce0b9f0","_type":"span","marks":[],"text":"Wang, Z., Acuna, D., Ling, H., Kar, A., and Fidler, S. Object instance annotation with deep extreme level set evolution. In CVPR, 2019."}],"level":1,"listItem":"number","markDefs":[],"style":"normal"},{"_key":"818e48f068d7","_type":"block","children":[{"_key":"4b5e7645a5b40","_type":"span","marks":[],"text":"Zolfaghari Bengar, J., Gonzalez-Garcia, A., Villalonga, G., Raducanu, B., Aghdam, H. H., Mozerov, M., Lopez, A. M., and van de Weijer, J. Temporal coherence for active learning in videos. arXiv preprint arXiv:1908.11757, 2019."}],"level":1,"listItem":"number","markDefs":[],"style":"normal"},{"_key":"1af2aede9358","_type":"block","children":[{"_key":"94aa7f1782210","_type":"span","marks":[],"text":"This post was written by Frederic Ratle and Martine Bertrand."}],"markDefs":[],"style":"normal"}],"config":{"description":"In this article we summarize an approach that we have developed to speed up polygonal instance segmentation using machine learning.","openGraphImage":null,"title":"Fast Vector Annotation with Machine Learning Assisted Annotation"},"estimatedReadingTime":8,"featured_image":{"_type":"image","asset":{"_ref":"image-6abd2f846119ba50a8325787f99ec3aeec68ffff-1076x605-png","_type":"reference"}},"plaintextBody":"At Sama, Vector Annotation of objects using polygons is a task that our expert annotators spend a great deal of time on. This is especially true for projects involving autonomous vehicles, where it is typical to apply instance segmentation to label scenes comprising hundreds of frames, each with multiple objects (vehicles, pedestrians, traffic signs, etc.) like you see in Figure 1.\n\nFigure 1. Example of Polygonal Annotation in Sama.\n\n\n\nIn this post, we summarize an approach that we have developed to speed up polygonal instance segmentation using machine learning. This approach was presented earlier this year at the CVPR Workshop on Scalability in Autonomous Driving, and the ICML Workshop on Human-in-the-Loop Learning.\n\nFew-Click Annotation\n\nBuilding instance segmentation Deep Learning (DL) models for autonomous vehicles requires a significant amount of labeled data. The use of Machine Learning (ML) for producing pre-annotations to be reviewed by human annotators, whether in an interactive setting or as a pre-processing procedure, is a very popular approach for scaling up labeling while controlling the costs.\n\nMultiple approaches have been suggested for machine-assisted instance segmentation. These typically consist of a DL-based segmentation of the object(s) integrated into a human-in-the-loop system. The human can interact with the system by correcting the model output, initializing the model with one or several clicks, or a combination of those steps. Examples of such systems include Polygon-RNN++ [1], DELSE [7], DEXTR [3], and CurveGCN [2]. Those systems all present good results, but some open questions remain:\n\nDo these methods perform well when a production-level accuracy is required, as when working for a customer project?\n\nDoes the choice of annotation tool influence the results? The gains to be made by using ML depend on how difficult it is for humans to draw polygons in the provided UI. Here we used our optimized drawing tool for polygons, which is part of our labeling platform.\n\nML integration is not usually approached from a human-centric perspective. Beyond the optimization of traditional metrics like IoU, what interactions are most desirable and how should we present the output of the model to annotators?\n\nOur method relies on combining the well-known DEXTR [3] approach with a raster-to-polygon algorithm, to make the result more easily editable. This is not unlike what other tools (such as CVAT) have implemented, though we have optimized this approach for our specific use cases using A/B testing.\n\nThe Model\n\nOur instance segmentation model is based on the well-known Deep Extreme Cut (DEXTR) approach [3], along with a raster-to-polygon conversion algorithm that yields high quality polygons whose vertices are sampled in a way that reproduces human drawing patterns. The model uses the few clicks provided by human annotators at inference time. The steps are described in Figure 2.\n\nFigure 2. An overview of the approach.\n\n\n\nRegarding the model itself, we adopted a custom version of the UNet [5] along with an EfficientNet backbone [6] (instead of the ResNet backbone used in the paper).\n\nIn our experience, for human annotators to produce good instance segmentation masks efficiently, a polygon annotation tool should be used. As such, we needed to convert the raster masks produced by our model to high quality polygons. To add to the challenge, humans tend to produce sparse polygons, adding vertices only when necessary. We therefore adopted a raster-to-polygon procedure that minimizes the number of output vertices.\n\nA/B Testing the Approach\n\nAt Sama, we use A/B testing as much as possible to systematically refine and improve our new features. To this end, we have developed a flexible testing infrastructure that can ingest and aggregate data from multiple internal processes and is made available to anyone within the organization.\n\nThis framework measures the statistical impact of proposed changes on our efficiency metrics (such as drawing time or shape adjustment time). The significance of observed differences on a given efficiency metric is evaluated using statistical tests.\n\nToy A/B Tests\n\nWe conducted an A/B test of the method using a synthetic automotive dataset called SYNTHIA-AL [8]. The dataset's images and corresponding annotations were generated from video streams at 25 frames per second (FPS). Figure 3 shows SYNTHIA image examples, along with their segmentation (done manually and with the Few-Click tool).\n\nFigure 3. SYNTHIA example images, along with their manual and semi-automated annotations.\n\n\n\nThe test, applied only to motor vehicles, reproduced realistic annotation guidelines, namely:\n\nThe drawn polygon needs to be within 2 pixels of the edge of the vehicle.\n\nAll vehicles down to 10 pixels (height or width) need to be annotated.\n\nFollowing this test, we found a nearly 3-fold reduction in annotation time. On the other hand, we also found that on some of the more complex shapes, annotators were spending quite some time manually adjusting the ML output. DEXTR's authors originally showed that the segmentation can be improved with additional clicks beyond the four initial ones. We therefore extended our few-click tool to allow online refinement of the polygons by considering modifications to their vertices as extra clicks. At train time we simulated the corrective clicks by considering the point of greatest deviation between predicted mask and ground truth as illustrated in Figure 4.\n\nProblem: DEXTR’s 4 extreme clicks are not always sufficient.\n\nObservation: DEXTR trained on 4 clicks benefits from additional clicks.\n\nSolution: Fine-tune DEXTR model with additional clicks for hard samples as established by IoU at train time.\n\nFigure 4. Integrating additional clicks in the training process.\n\n\n\nUsing this method, annotators are able to re-trigger the model inference with an additional click, instead of manually adjusting the output. We proceeded to a second toy A/B test, and results showed that we could obtain a theoretical efficiency gain of up to 3.5x on vehicles using the improved method.\n\nDownload our paper on Human-Centric Efficiency Improvements in Image Annotation for Autonomous Driving here and stay tuned to hear more about our latest advances!\n\n\n\nReferences\n\nAcuna, D., Ling, H., Kar, A., and Fidler, S. Efficient annotation of segmentation datasets with polygon-rnn++. In CVPR, 2018.\n\nLing, H., Gao, J., Kar, A., Chen, W., and Fidler, S. Fast interactive object annotation with curve-gcn. In CVPR, 2019.\n\nManinis, K.-K., Caelles, S., Pont-Tuset, J., and Van Gool, L. Deep extreme cut: From extreme points to object segmentation. In Computer Vision and Pattern Recognition (CVPR), 2018.\n\nPapadopoulos, D., Uijlings, J., Keller, F., and Ferrari, V. Extreme clicking for efficient object annotation. In ICCV, 2017.\n\nRonneberger, O., Fischer, P., and Brox, T. U-net: Convolutional networks for biomedical image segmentation. CoRR, abs/1505.04597, 2015. URL http://arxiv. org/abs/1505.04597.\n\nTan, M. and Le, Q. V. Efficientnet: Rethinking model scaling for convolutional neural networks. CoRR, abs/1905.11946, 2019. URL http://arxiv.org/ abs/1905.11946.\n\nWang, Z., Acuna, D., Ling, H., Kar, A., and Fidler, S. Object instance annotation with deep extreme level set evolution. In CVPR, 2019.\n\nZolfaghari Bengar, J., Gonzalez-Garcia, A., Villalonga, G., Raducanu, B., Aghdam, H. H., Mozerov, M., Lopez, A. M., and van de Weijer, J. Temporal coherence for active learning in videos. arXiv preprint arXiv:1908.11757, 2019.\n\nThis post was written by Frederic Ratle and Martine Bertrand.","relatedPosts":[{"_id":"97fe94a6-283f-49a2-9311-96738e00e5c4","featured_image":{"_type":"image","asset":{"_ref":"image-ae9f62c210539de6ed2b60b71efa4be6b90021c6-1920x960-png","_type":"reference"}},"slug":{"_type":"slug","current":"volumental-shoe-sizing-app"},"tags":[{"label":"Case Studies","value":"Case Studies"}],"title":"Accurate Data Labeling Powers the Volumental Shoe Recommendation App — Helping Retailers Convert Mobile Customers"},{"_id":"6aa16e9a-75c4-44f5-8dec-6f1f06fb709e","featured_image":{"_type":"image","asset":{"_ref":"image-0434b713b7dcba1cdf98eff0eb50d7cb3d5fb008-1500x908-jpg","_type":"reference"}},"slug":{"_type":"slug","current":"fast-company-next-best-things-tech-2021"},"tags":[{"label":"Company News","value":"Company News"},{"label":"Awards","value":"Awards"}],"title":"Orbisk’s Sama-Powered Food Waste Solution Named to Fast Company’s First-Ever List of the Next Big Things in Tech"},{"_id":"728400d2-d453-42f7-b20c-47f753bc4583","featured_image":{"_type":"image","asset":{"_ref":"image-1502043edcf701cb4267cb2f6d7c6edf8e0e0cad-2500x1313-png","_type":"reference"}},"slug":{"_type":"slug","current":"podcast-episode-facebook-manohar-paluri"},"tags":[{"label":"Podcast","value":"Podcast"}],"title":"New Podcast Episode: Facebook's Manohar Paluri Makes Machines See"}],"slug":{"_type":"slug","current":"fast-vector-annotation"},"tags":[{"_key":"fJGSmFCx","label":"Vector Annotation","value":"Vector Annotation"},{"_key":"21OOfbwx","label":"Polygons","value":"Polygons"},{"_key":"SPABaoXN","label":"Sama Engineering","value":"Sama Engineering"},{"_key":"O5nmmFbm","label":"Featured","value":"Featured"}],"title":"Fast Vector Annotation with Machine Learning Assisted Annotation"}}},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"fast-vector-annotation"},"buildId":"fpyn7Yiasmv-BShm3_n8o","isFallback":false,"dynamicIds":[4941,425,3551],"gsp":true,"appGip":true,"scriptLoader":[]}</script></body></html>