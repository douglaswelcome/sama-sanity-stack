<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"/><meta name="viewport" content="initial-scale=1.0, width=device-width, viewport-fit=cover"/><title>Sama Blog | Training Data, AI and Impact Sourcing Insights</title><link rel="canonical" href="https://sama-next.netlify.app//blog"/><meta name="description" content="From machine learning to training data strategy, the Sama blog covers research, news and other AI trends from thought leaders across the globe."/><meta property="og:type" content="website"/><meta property="og:description" content="From machine learning to training data strategy, the Sama blog covers research, news and other AI trends from thought leaders across the globe."/><meta property="og:title" content="Sama Blog | Training Data, AI and Impact Sourcing Insights"/><meta property="og:url" content="https://sama-next.netlify.app//blog"/><meta name="twitter:card" content="summary"/><meta property="twitter:description" content="From machine learning to training data strategy, the Sama blog covers research, news and other AI trends from thought leaders across the globe."/><meta property="twitter:title" content="Sama Blog | Training Data, AI and Impact Sourcing Insights"/><meta name="twitter:domain" content="www.sama.com"/><meta name="msapplication-TileColor" content="#28282a"/><link rel="manifest" href="/manifest.json"/><meta name="theme-color" content="#ffffff"/><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"/><link rel="shortcut icon" href="/favicon.ico"/><meta name="next-head-count" content="19"/><link rel="preload" href="/_next/static/css/435d0bf9db9751276ebf.css" as="style"/><link rel="stylesheet" href="/_next/static/css/435d0bf9db9751276ebf.css" data-n-g=""/><link rel="preload" href="/_next/static/css/230f921851e8f4c090d2.css" as="style"/><link rel="stylesheet" href="/_next/static/css/230f921851e8f4c090d2.css" data-n-p=""/><link rel="preload" href="/_next/static/css/5478fb8820b37cca7a22.css" as="style"/><link rel="stylesheet" href="/_next/static/css/5478fb8820b37cca7a22.css"/><link rel="preload" href="/_next/static/css/f9f8c0d06ba77c772815.css" as="style"/><link rel="stylesheet" href="/_next/static/css/f9f8c0d06ba77c772815.css"/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-a40ef1678bae11e696dba45124eadd70.js"></script><script defer="" src="/_next/static/chunks/4941.f7b4b29e03c1b6f7dee2.js"></script><script defer="" src="/_next/static/chunks/4934.37651fffe4e244d39030.js"></script><script defer="" src="/_next/static/chunks/1952.b946b9426ec92537afc7.js"></script><script defer="" src="/_next/static/chunks/3551.369166f7a3aa0696cada.js"></script><script src="/_next/static/chunks/webpack-82b57159936545fe5ab3.js" defer=""></script><script src="/_next/static/chunks/framework-bdc1b4e5e48979e16d36.js" defer=""></script><script src="/_next/static/chunks/main-04be5b8b279d67174a77.js" defer=""></script><script src="/_next/static/chunks/pages/_app-ab0a399274566aad1c00.js" defer=""></script><script src="/_next/static/chunks/commons-29272ea044d5a317a0a9.js" defer=""></script><script src="/_next/static/chunks/pages/blog-05768f615dcccc011a72.js" defer=""></script><script src="/_next/static/202899f50319881aba98bacba98f348defc1a075/_buildManifest.js" defer=""></script><script src="/_next/static/202899f50319881aba98bacba98f348defc1a075/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="container"><header class="header_outer__yu9q7 "><nav class="umoja-l-grid--12 header_wrapper__3Ghzm"><a class="header_logo__eiLSq" href="/"><span class="visually-hidden">Sama</span></a><button class="header_hamburger__1ZcbZ" type="button"><span class="header_hamburger_box__RZ7CY"><span class="header_hamburger_box_inner__1PmWZ"></span></span></button><ul class="header_navBar__37eSJ"><li class="header_navItem__1HiGN"><p class="header_navItem_label__fN3Cy">Platform</p><div class="header_dropDown__6NxUb"><div class="header_dropDown_group__2BZXC"><p class="header_dropdown_group_label__tmND8">Platform</p><a class="header_navLink__1ARu5" href="/how-it-works">How it Works</a><a class="header_navLink__1ARu5" href="/video-annotation">Video Annotation</a><a class="header_navLink__1ARu5" href="/image-annotation">Image Annotation</a><a class="header_navLink__1ARu5" href="/3d-lidar">3D &amp; LiDAR Annotation</a><a class="header_navLink__1ARu5" href="/natural-language-processing">Natural Language Processing</a><a class="header_navLink__1ARu5" href="/data-curation">Data Curation (Beta)</a></div><div class="header_dropDown_group__2BZXC"><p class="header_dropdown_group_label__tmND8">Shapes</p><a class="header_navLink__1ARu5" href="/semantic-segmentation">Semantic Segmentation</a><a class="header_navLink__1ARu5" href="/polygons">Polygons</a><a class="header_navLink__1ARu5" href="/bounding-boxes">Bounding Boxes</a><a class="header_navLink__1ARu5" href="/key-points">Key Points</a><a class="header_navLink__1ARu5" href="/cuboids">Cuboids</a><a class="header_navLink__1ARu5" href="/lines-and-arrows">Lines &amp; Arrows</a></div></div></li><li class="header_navItem__1HiGN"><p class="header_navItem_label__fN3Cy">Industries</p><div class="header_dropDown__6NxUb"><a class="header_navLink__1ARu5" href="/transportation-navigation">Transportation &amp; Navigation</a><a class="header_navLink__1ARu5" href="/retail-ecommerce">Retail &amp; E-Commerce</a><a class="header_navLink__1ARu5" href="/consumer-media">Consumer &amp; Media</a><a class="header_navLink__1ARu5" href="/biotech-medtech">Biotech &amp; Medtech</a><a class="header_navLink__1ARu5" href="/robotics-and-manufacturing">Robotics &amp; Manufacturing</a><a class="header_navLink__1ARu5" href="/training-data-food-agriculture">Food &amp; Agriculture</a></div></li><li class="header_navItem__1HiGN"><p class="header_navItem_label__fN3Cy">Why Sama</p><div class="header_dropDown__6NxUb"><a class="header_navLink__1ARu5" href="/quality-training-data">Quality</a><a class="header_navLink__1ARu5" href="/security-and-trust">Security</a><a class="header_navLink__1ARu5" href="/our-impact">Ethical AI</a><a class="header_navLink__1ARu5" href="/compare">Compare</a><a class="header_navLink__1ARu5" href="/partners">Partners</a></div></li><li class="header_navItem__1HiGN"><p class="header_navItem_label__fN3Cy">Resources</p><div class="header_dropDown__6NxUb"><a href="https://docs.sama.com/reference/overview" class="header_navLink__1ARu5" target="_blank" rel="noopener">API Documentation</a><a class="header_navLink__1ARu5 header_navLink__active__3cW9s" href="/blog">Blog</a><a class="header_navLink__1ARu5" href="/events">Events</a></div></li><li class="header_navItem__1HiGN"><p class="header_navItem_label__fN3Cy">Company</p><div class="header_dropDown__6NxUb"><a class="header_navLink__1ARu5" href="/our-story">Our Story</a><a class="header_navLink__1ARu5" href="/our-team">Our Team</a><a class="header_navLink__1ARu5" href="/careers">Careers</a><a class="header_navLink__1ARu5" href="/company-contact">Contact</a><a class="header_navLink__1ARu5" href="/press">Press</a></div></li></ul><div class="header_cta__3J8I7"><a class="button_wrapper__3lRbv button__secondary__1pZ5q button__small__2kIwW" href="/[object%20Object]"><button class="button_btn__1qxP1"><h3 class="button_text__3_sCS">Request a Demo</h3></button></a></div></nav></header><main class="content"><section class="umoja-l-grid-section umoja-u-bg--white"><div class="umoja-l-grid--12 umoja-l-grid-align--center"><div class="blog-heroHome_left__3EMpN"><a href="/[object%20Object]"><div style="display:block;overflow:hidden;position:relative;box-sizing:border-box;margin:0"><div style="display:block;box-sizing:border-box;padding-top:50%"></div><img alt="" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="responsive" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover"/><noscript><img alt="" sizes="100vw" srcSet="https://cdn.sanity.io/images/76e3r62u/production/30d0eaccd1e57322b31a4e16b84576ef1f8db57e-1920x960.png?w=640&amp;q=75&amp;fit=clip&amp;auto=format 640w, https://cdn.sanity.io/images/76e3r62u/production/30d0eaccd1e57322b31a4e16b84576ef1f8db57e-1920x960.png?w=750&amp;q=75&amp;fit=clip&amp;auto=format 750w, https://cdn.sanity.io/images/76e3r62u/production/30d0eaccd1e57322b31a4e16b84576ef1f8db57e-1920x960.png?w=828&amp;q=75&amp;fit=clip&amp;auto=format 828w, https://cdn.sanity.io/images/76e3r62u/production/30d0eaccd1e57322b31a4e16b84576ef1f8db57e-1920x960.png?w=1080&amp;q=75&amp;fit=clip&amp;auto=format 1080w, https://cdn.sanity.io/images/76e3r62u/production/30d0eaccd1e57322b31a4e16b84576ef1f8db57e-1920x960.png?w=1200&amp;q=75&amp;fit=clip&amp;auto=format 1200w, https://cdn.sanity.io/images/76e3r62u/production/30d0eaccd1e57322b31a4e16b84576ef1f8db57e-1920x960.png?w=1920&amp;q=75&amp;fit=clip&amp;auto=format 1920w, https://cdn.sanity.io/images/76e3r62u/production/30d0eaccd1e57322b31a4e16b84576ef1f8db57e-1920x960.png?w=2048&amp;q=75&amp;fit=clip&amp;auto=format 2048w, https://cdn.sanity.io/images/76e3r62u/production/30d0eaccd1e57322b31a4e16b84576ef1f8db57e-1920x960.png?w=3840&amp;q=75&amp;fit=clip&amp;auto=format 3840w" src="https://cdn.sanity.io/images/76e3r62u/production/30d0eaccd1e57322b31a4e16b84576ef1f8db57e-1920x960.png?w=3840&amp;q=75&amp;fit=clip&amp;auto=format" decoding="async" data-nimg="responsive" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover" loading="lazy"/></noscript></div></a></div><div class="blog-heroHome_right__bxPqk"><a class="blog-heroHome_tag__1aSq3" href="/[object%20Object]">Case Studies</a><a href="/[object%20Object]"><h1>High-Quality Labeled Data Fuels zeroG’s Mission to Optimize Aircraft Turnaround Management</h1></a><p class="blog-heroHome_excerpt__ts8bV">zeroG, a Lufthansa Systems company, uses AI, data science, and analytics to help airports and airlines optimize operational efficiency. zeroG partnered with Sama to power their Deep Turnaround...</p><a class="blog-heroHome_author__3Cy_h" href="/[object%20Object]">Amanda Durepos</a><p>December 8, 2021<!-- --> | <!-- -->7 Min Read</p></div></div></section><section class="umoja-l-grid-section umoja-u-bg--charcoal1"><div class="umoja-l-grid--12"><div class="umoja-l-grid-span--full"><h2>Featured Posts</h2><p>See some of our recent posts and exciting news:</p></div><div class="blog-smallCard-row_post__in_Mm"><a class="blog-smallCard-row_imgWrap__36jhQ" href="/[object%20Object]"><div style="display:block;overflow:hidden;position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;margin:0"><img alt="" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="fill" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover"/><noscript><img alt="" sizes="100vw" srcSet="https://cdn.sanity.io/images/76e3r62u/production/19ef3f125b8ee47727e9f94dc3aff7ed08a4d78c-720x360.png?w=640&amp;q=75&amp;fit=clip&amp;auto=format 640w, https://cdn.sanity.io/images/76e3r62u/production/19ef3f125b8ee47727e9f94dc3aff7ed08a4d78c-720x360.png?w=750&amp;q=75&amp;fit=clip&amp;auto=format 750w, https://cdn.sanity.io/images/76e3r62u/production/19ef3f125b8ee47727e9f94dc3aff7ed08a4d78c-720x360.png?w=828&amp;q=75&amp;fit=clip&amp;auto=format 828w, https://cdn.sanity.io/images/76e3r62u/production/19ef3f125b8ee47727e9f94dc3aff7ed08a4d78c-720x360.png?w=1080&amp;q=75&amp;fit=clip&amp;auto=format 1080w, https://cdn.sanity.io/images/76e3r62u/production/19ef3f125b8ee47727e9f94dc3aff7ed08a4d78c-720x360.png?w=1200&amp;q=75&amp;fit=clip&amp;auto=format 1200w, https://cdn.sanity.io/images/76e3r62u/production/19ef3f125b8ee47727e9f94dc3aff7ed08a4d78c-720x360.png?w=1920&amp;q=75&amp;fit=clip&amp;auto=format 1920w, https://cdn.sanity.io/images/76e3r62u/production/19ef3f125b8ee47727e9f94dc3aff7ed08a4d78c-720x360.png?w=2048&amp;q=75&amp;fit=clip&amp;auto=format 2048w, https://cdn.sanity.io/images/76e3r62u/production/19ef3f125b8ee47727e9f94dc3aff7ed08a4d78c-720x360.png?w=3840&amp;q=75&amp;fit=clip&amp;auto=format 3840w" src="https://cdn.sanity.io/images/76e3r62u/production/19ef3f125b8ee47727e9f94dc3aff7ed08a4d78c-720x360.png?w=3840&amp;q=75&amp;fit=clip&amp;auto=format" decoding="async" data-nimg="fill" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover" loading="lazy"/></noscript></div></a><div class="blog-smallCard-row_post_info__3V7Qy"><a href="/[object%20Object]"><h4>Factotum: Containerizing DevOps Tools for Cloud Native Engineering and CI/CD</h4></a><a class="blog-smallCard-row_tag__33FrA" href="/[object%20Object]">Sama Engineering</a></div></div><div class="blog-smallCard-row_post__in_Mm"><a class="blog-smallCard-row_imgWrap__36jhQ" href="/[object%20Object]"><div style="display:block;overflow:hidden;position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;margin:0"><img alt="" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="fill" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover"/><noscript><img alt="" sizes="100vw" srcSet="https://cdn.sanity.io/images/76e3r62u/production/04641081798c41caebebefa44685828d80b7434f-1200x675.png?w=640&amp;q=75&amp;fit=clip&amp;auto=format 640w, https://cdn.sanity.io/images/76e3r62u/production/04641081798c41caebebefa44685828d80b7434f-1200x675.png?w=750&amp;q=75&amp;fit=clip&amp;auto=format 750w, https://cdn.sanity.io/images/76e3r62u/production/04641081798c41caebebefa44685828d80b7434f-1200x675.png?w=828&amp;q=75&amp;fit=clip&amp;auto=format 828w, https://cdn.sanity.io/images/76e3r62u/production/04641081798c41caebebefa44685828d80b7434f-1200x675.png?w=1080&amp;q=75&amp;fit=clip&amp;auto=format 1080w, https://cdn.sanity.io/images/76e3r62u/production/04641081798c41caebebefa44685828d80b7434f-1200x675.png?w=1200&amp;q=75&amp;fit=clip&amp;auto=format 1200w, https://cdn.sanity.io/images/76e3r62u/production/04641081798c41caebebefa44685828d80b7434f-1200x675.png?w=1920&amp;q=75&amp;fit=clip&amp;auto=format 1920w, https://cdn.sanity.io/images/76e3r62u/production/04641081798c41caebebefa44685828d80b7434f-1200x675.png?w=2048&amp;q=75&amp;fit=clip&amp;auto=format 2048w, https://cdn.sanity.io/images/76e3r62u/production/04641081798c41caebebefa44685828d80b7434f-1200x675.png?w=3840&amp;q=75&amp;fit=clip&amp;auto=format 3840w" src="https://cdn.sanity.io/images/76e3r62u/production/04641081798c41caebebefa44685828d80b7434f-1200x675.png?w=3840&amp;q=75&amp;fit=clip&amp;auto=format" decoding="async" data-nimg="fill" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover" loading="lazy"/></noscript></div></a><div class="blog-smallCard-row_post_info__3V7Qy"><a href="/[object%20Object]"><h4>The Sama MLOps Pipeline: Automating Model Training on the Cloud</h4></a><a class="blog-smallCard-row_tag__33FrA" href="/[object%20Object]">Training Data</a></div></div><div class="blog-smallCard-row_post__in_Mm"><a class="blog-smallCard-row_imgWrap__36jhQ" href="/[object%20Object]"><div style="display:block;overflow:hidden;position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;margin:0"><img alt="" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="fill" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover"/><noscript><img alt="" sizes="100vw" srcSet="https://cdn.sanity.io/images/76e3r62u/production/6abd2f846119ba50a8325787f99ec3aeec68ffff-1076x605.png?w=640&amp;q=75&amp;fit=clip&amp;auto=format 640w, https://cdn.sanity.io/images/76e3r62u/production/6abd2f846119ba50a8325787f99ec3aeec68ffff-1076x605.png?w=750&amp;q=75&amp;fit=clip&amp;auto=format 750w, https://cdn.sanity.io/images/76e3r62u/production/6abd2f846119ba50a8325787f99ec3aeec68ffff-1076x605.png?w=828&amp;q=75&amp;fit=clip&amp;auto=format 828w, https://cdn.sanity.io/images/76e3r62u/production/6abd2f846119ba50a8325787f99ec3aeec68ffff-1076x605.png?w=1080&amp;q=75&amp;fit=clip&amp;auto=format 1080w, https://cdn.sanity.io/images/76e3r62u/production/6abd2f846119ba50a8325787f99ec3aeec68ffff-1076x605.png?w=1200&amp;q=75&amp;fit=clip&amp;auto=format 1200w, https://cdn.sanity.io/images/76e3r62u/production/6abd2f846119ba50a8325787f99ec3aeec68ffff-1076x605.png?w=1920&amp;q=75&amp;fit=clip&amp;auto=format 1920w, https://cdn.sanity.io/images/76e3r62u/production/6abd2f846119ba50a8325787f99ec3aeec68ffff-1076x605.png?w=2048&amp;q=75&amp;fit=clip&amp;auto=format 2048w, https://cdn.sanity.io/images/76e3r62u/production/6abd2f846119ba50a8325787f99ec3aeec68ffff-1076x605.png?w=3840&amp;q=75&amp;fit=clip&amp;auto=format 3840w" src="https://cdn.sanity.io/images/76e3r62u/production/6abd2f846119ba50a8325787f99ec3aeec68ffff-1076x605.png?w=3840&amp;q=75&amp;fit=clip&amp;auto=format" decoding="async" data-nimg="fill" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover" loading="lazy"/></noscript></div></a><div class="blog-smallCard-row_post_info__3V7Qy"><a href="/[object%20Object]"><h4>Fast Vector Annotation with Machine Learning Assisted Annotation</h4></a><a class="blog-smallCard-row_tag__33FrA" href="/[object%20Object]">Polygons</a></div></div></div></section><section class="umoja-l-grid-section umoja-u-bg--white"><div class="umoja-l-grid--12 umoja-l-grid-gap--row-1"><div class="blog-post_postCard__i0O-O"><a class="blog-post_postCard_image__3k8eA" href="/[object%20Object]"><div style="display:block;overflow:hidden;position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;margin:0"><img alt="" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="fill" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover"/><noscript><img alt="" sizes="100vw" srcSet="https://cdn.sanity.io/images/76e3r62u/production/f80b5e83a9927bafc61285d9e7a16b07070f53c1-1200x630.png?w=640&amp;q=75&amp;fit=clip&amp;auto=format 640w, https://cdn.sanity.io/images/76e3r62u/production/f80b5e83a9927bafc61285d9e7a16b07070f53c1-1200x630.png?w=750&amp;q=75&amp;fit=clip&amp;auto=format 750w, https://cdn.sanity.io/images/76e3r62u/production/f80b5e83a9927bafc61285d9e7a16b07070f53c1-1200x630.png?w=828&amp;q=75&amp;fit=clip&amp;auto=format 828w, https://cdn.sanity.io/images/76e3r62u/production/f80b5e83a9927bafc61285d9e7a16b07070f53c1-1200x630.png?w=1080&amp;q=75&amp;fit=clip&amp;auto=format 1080w, https://cdn.sanity.io/images/76e3r62u/production/f80b5e83a9927bafc61285d9e7a16b07070f53c1-1200x630.png?w=1200&amp;q=75&amp;fit=clip&amp;auto=format 1200w, https://cdn.sanity.io/images/76e3r62u/production/f80b5e83a9927bafc61285d9e7a16b07070f53c1-1200x630.png?w=1920&amp;q=75&amp;fit=clip&amp;auto=format 1920w, https://cdn.sanity.io/images/76e3r62u/production/f80b5e83a9927bafc61285d9e7a16b07070f53c1-1200x630.png?w=2048&amp;q=75&amp;fit=clip&amp;auto=format 2048w, https://cdn.sanity.io/images/76e3r62u/production/f80b5e83a9927bafc61285d9e7a16b07070f53c1-1200x630.png?w=3840&amp;q=75&amp;fit=clip&amp;auto=format 3840w" src="https://cdn.sanity.io/images/76e3r62u/production/f80b5e83a9927bafc61285d9e7a16b07070f53c1-1200x630.png?w=3840&amp;q=75&amp;fit=clip&amp;auto=format" decoding="async" data-nimg="fill" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover" loading="lazy"/></noscript></div></a><div class="blog-post_postCard_data__2vfn3"><a class="blog-post_postCard_tag__9RTi-" href="/[object%20Object]">Company News</a><a href="/[object%20Object]"><h3>Sama Partners with Mila to Solve Key Problems in AI Development</h3></a><a class="blog-post_postCard_author__Y7RjO" href="/[object%20Object]">Alex Shee</a><p class="blog-post_postCard_date__hrDMA ">December 2, 2021<!-- --> | 3 Min Read</p></div></div><div class="blog-post_postCard__i0O-O"><a class="blog-post_postCard_image__3k8eA" href="/[object%20Object]"><div style="display:block;overflow:hidden;position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;margin:0"><img alt="" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="fill" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover"/><noscript><img alt="" sizes="100vw" srcSet="https://cdn.sanity.io/images/76e3r62u/production/1502043edcf701cb4267cb2f6d7c6edf8e0e0cad-2500x1313.png?w=640&amp;q=75&amp;fit=clip&amp;auto=format 640w, https://cdn.sanity.io/images/76e3r62u/production/1502043edcf701cb4267cb2f6d7c6edf8e0e0cad-2500x1313.png?w=750&amp;q=75&amp;fit=clip&amp;auto=format 750w, https://cdn.sanity.io/images/76e3r62u/production/1502043edcf701cb4267cb2f6d7c6edf8e0e0cad-2500x1313.png?w=828&amp;q=75&amp;fit=clip&amp;auto=format 828w, https://cdn.sanity.io/images/76e3r62u/production/1502043edcf701cb4267cb2f6d7c6edf8e0e0cad-2500x1313.png?w=1080&amp;q=75&amp;fit=clip&amp;auto=format 1080w, https://cdn.sanity.io/images/76e3r62u/production/1502043edcf701cb4267cb2f6d7c6edf8e0e0cad-2500x1313.png?w=1200&amp;q=75&amp;fit=clip&amp;auto=format 1200w, https://cdn.sanity.io/images/76e3r62u/production/1502043edcf701cb4267cb2f6d7c6edf8e0e0cad-2500x1313.png?w=1920&amp;q=75&amp;fit=clip&amp;auto=format 1920w, https://cdn.sanity.io/images/76e3r62u/production/1502043edcf701cb4267cb2f6d7c6edf8e0e0cad-2500x1313.png?w=2048&amp;q=75&amp;fit=clip&amp;auto=format 2048w, https://cdn.sanity.io/images/76e3r62u/production/1502043edcf701cb4267cb2f6d7c6edf8e0e0cad-2500x1313.png?w=3840&amp;q=75&amp;fit=clip&amp;auto=format 3840w" src="https://cdn.sanity.io/images/76e3r62u/production/1502043edcf701cb4267cb2f6d7c6edf8e0e0cad-2500x1313.png?w=3840&amp;q=75&amp;fit=clip&amp;auto=format" decoding="async" data-nimg="fill" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover" loading="lazy"/></noscript></div></a><div class="blog-post_postCard_data__2vfn3"><a class="blog-post_postCard_tag__9RTi-" href="/[object%20Object]">Podcast</a><a href="/[object%20Object]"><h3>New Podcast Episode: Facebook&#x27;s Manohar Paluri Makes Machines See</h3></a><a class="blog-post_postCard_author__Y7RjO" href="/[object%20Object]">Rob Stevenson</a><p class="blog-post_postCard_date__hrDMA ">November 23, 2021<!-- --> | 2 Min Read</p></div></div><div class="blog-post_postCard__i0O-O"><a class="blog-post_postCard_image__3k8eA" href="/[object%20Object]"><div style="display:block;overflow:hidden;position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;margin:0"><img alt="" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="fill" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover"/><noscript><img alt="" sizes="100vw" srcSet="https://cdn.sanity.io/images/76e3r62u/production/0434b713b7dcba1cdf98eff0eb50d7cb3d5fb008-1500x908.jpg?w=640&amp;q=75&amp;fit=clip&amp;auto=format 640w, https://cdn.sanity.io/images/76e3r62u/production/0434b713b7dcba1cdf98eff0eb50d7cb3d5fb008-1500x908.jpg?w=750&amp;q=75&amp;fit=clip&amp;auto=format 750w, https://cdn.sanity.io/images/76e3r62u/production/0434b713b7dcba1cdf98eff0eb50d7cb3d5fb008-1500x908.jpg?w=828&amp;q=75&amp;fit=clip&amp;auto=format 828w, https://cdn.sanity.io/images/76e3r62u/production/0434b713b7dcba1cdf98eff0eb50d7cb3d5fb008-1500x908.jpg?w=1080&amp;q=75&amp;fit=clip&amp;auto=format 1080w, https://cdn.sanity.io/images/76e3r62u/production/0434b713b7dcba1cdf98eff0eb50d7cb3d5fb008-1500x908.jpg?w=1200&amp;q=75&amp;fit=clip&amp;auto=format 1200w, https://cdn.sanity.io/images/76e3r62u/production/0434b713b7dcba1cdf98eff0eb50d7cb3d5fb008-1500x908.jpg?w=1920&amp;q=75&amp;fit=clip&amp;auto=format 1920w, https://cdn.sanity.io/images/76e3r62u/production/0434b713b7dcba1cdf98eff0eb50d7cb3d5fb008-1500x908.jpg?w=2048&amp;q=75&amp;fit=clip&amp;auto=format 2048w, https://cdn.sanity.io/images/76e3r62u/production/0434b713b7dcba1cdf98eff0eb50d7cb3d5fb008-1500x908.jpg?w=3840&amp;q=75&amp;fit=clip&amp;auto=format 3840w" src="https://cdn.sanity.io/images/76e3r62u/production/0434b713b7dcba1cdf98eff0eb50d7cb3d5fb008-1500x908.jpg?w=3840&amp;q=75&amp;fit=clip&amp;auto=format" decoding="async" data-nimg="fill" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover" loading="lazy"/></noscript></div></a><div class="blog-post_postCard_data__2vfn3"><a class="blog-post_postCard_tag__9RTi-" href="/[object%20Object]">Company News</a><a href="/[object%20Object]"><h3>Orbisk’s Sama-Powered Food Waste Solution Named to Fast Company’s First-Ever List of the Next Big Things in Tech</h3></a><a class="blog-post_postCard_author__Y7RjO" href="/[object%20Object]">Suzin Wold</a><p class="blog-post_postCard_date__hrDMA ">November 18, 2021<!-- --> | 3 Min Read</p></div></div><div class="blog-post_postCard__i0O-O"><a class="blog-post_postCard_image__3k8eA" href="/[object%20Object]"><div style="display:block;overflow:hidden;position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;margin:0"><img alt="" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="fill" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover"/><noscript><img alt="" sizes="100vw" srcSet="https://cdn.sanity.io/images/76e3r62u/production/ae9f62c210539de6ed2b60b71efa4be6b90021c6-1920x960.png?w=640&amp;q=75&amp;fit=clip&amp;auto=format 640w, https://cdn.sanity.io/images/76e3r62u/production/ae9f62c210539de6ed2b60b71efa4be6b90021c6-1920x960.png?w=750&amp;q=75&amp;fit=clip&amp;auto=format 750w, https://cdn.sanity.io/images/76e3r62u/production/ae9f62c210539de6ed2b60b71efa4be6b90021c6-1920x960.png?w=828&amp;q=75&amp;fit=clip&amp;auto=format 828w, https://cdn.sanity.io/images/76e3r62u/production/ae9f62c210539de6ed2b60b71efa4be6b90021c6-1920x960.png?w=1080&amp;q=75&amp;fit=clip&amp;auto=format 1080w, https://cdn.sanity.io/images/76e3r62u/production/ae9f62c210539de6ed2b60b71efa4be6b90021c6-1920x960.png?w=1200&amp;q=75&amp;fit=clip&amp;auto=format 1200w, https://cdn.sanity.io/images/76e3r62u/production/ae9f62c210539de6ed2b60b71efa4be6b90021c6-1920x960.png?w=1920&amp;q=75&amp;fit=clip&amp;auto=format 1920w, https://cdn.sanity.io/images/76e3r62u/production/ae9f62c210539de6ed2b60b71efa4be6b90021c6-1920x960.png?w=2048&amp;q=75&amp;fit=clip&amp;auto=format 2048w, https://cdn.sanity.io/images/76e3r62u/production/ae9f62c210539de6ed2b60b71efa4be6b90021c6-1920x960.png?w=3840&amp;q=75&amp;fit=clip&amp;auto=format 3840w" src="https://cdn.sanity.io/images/76e3r62u/production/ae9f62c210539de6ed2b60b71efa4be6b90021c6-1920x960.png?w=3840&amp;q=75&amp;fit=clip&amp;auto=format" decoding="async" data-nimg="fill" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover" loading="lazy"/></noscript></div></a><div class="blog-post_postCard_data__2vfn3"><a class="blog-post_postCard_tag__9RTi-" href="/[object%20Object]">Case Studies</a><a href="/[object%20Object]"><h3>Accurate Data Labeling Powers the Volumental Shoe Recommendation App — Helping Retailers Convert Mobile Customers</h3></a><a class="blog-post_postCard_author__Y7RjO" href="/[object%20Object]">Amanda Durepos</a><p class="blog-post_postCard_date__hrDMA ">November 16, 2021<!-- --> | 7 Min Read</p></div></div><div class="blog-post_postCard__i0O-O"><a class="blog-post_postCard_image__3k8eA" href="/[object%20Object]"><div style="display:block;overflow:hidden;position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;margin:0"><img alt="" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="fill" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover"/><noscript><img alt="" sizes="100vw" srcSet="https://cdn.sanity.io/images/76e3r62u/production/e7a31b38bf862680df4a21015cebc9ab8232c733-5472x3648.jpg?w=640&amp;q=75&amp;fit=clip&amp;auto=format 640w, https://cdn.sanity.io/images/76e3r62u/production/e7a31b38bf862680df4a21015cebc9ab8232c733-5472x3648.jpg?w=750&amp;q=75&amp;fit=clip&amp;auto=format 750w, https://cdn.sanity.io/images/76e3r62u/production/e7a31b38bf862680df4a21015cebc9ab8232c733-5472x3648.jpg?w=828&amp;q=75&amp;fit=clip&amp;auto=format 828w, https://cdn.sanity.io/images/76e3r62u/production/e7a31b38bf862680df4a21015cebc9ab8232c733-5472x3648.jpg?w=1080&amp;q=75&amp;fit=clip&amp;auto=format 1080w, https://cdn.sanity.io/images/76e3r62u/production/e7a31b38bf862680df4a21015cebc9ab8232c733-5472x3648.jpg?w=1200&amp;q=75&amp;fit=clip&amp;auto=format 1200w, https://cdn.sanity.io/images/76e3r62u/production/e7a31b38bf862680df4a21015cebc9ab8232c733-5472x3648.jpg?w=1920&amp;q=75&amp;fit=clip&amp;auto=format 1920w, https://cdn.sanity.io/images/76e3r62u/production/e7a31b38bf862680df4a21015cebc9ab8232c733-5472x3648.jpg?w=2048&amp;q=75&amp;fit=clip&amp;auto=format 2048w, https://cdn.sanity.io/images/76e3r62u/production/e7a31b38bf862680df4a21015cebc9ab8232c733-5472x3648.jpg?w=3840&amp;q=75&amp;fit=clip&amp;auto=format 3840w" src="https://cdn.sanity.io/images/76e3r62u/production/e7a31b38bf862680df4a21015cebc9ab8232c733-5472x3648.jpg?w=3840&amp;q=75&amp;fit=clip&amp;auto=format" decoding="async" data-nimg="fill" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover" loading="lazy"/></noscript></div></a><div class="blog-post_postCard_data__2vfn3"><a class="blog-post_postCard_tag__9RTi-" href="/[object%20Object]">Company News</a><a href="/[object%20Object]"><h3>Getting to Series B: How Sama is Proving Impact is a Strategy for Business Success</h3></a><a class="blog-post_postCard_author__Y7RjO" href="/[object%20Object]">Amanda Durepos</a><p class="blog-post_postCard_date__hrDMA ">November 10, 2021<!-- --> | 9 Min Read</p></div></div><div class="blog-post_postCard__i0O-O"><a class="blog-post_postCard_image__3k8eA" href="/[object%20Object]"><div style="display:block;overflow:hidden;position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;margin:0"><img alt="" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="fill" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover"/><noscript><img alt="" sizes="100vw" srcSet="https://cdn.sanity.io/images/76e3r62u/production/5129846575d1f7d8aa08a3784a1ff9eced8ba307-1000x500.png?w=640&amp;q=75&amp;fit=clip&amp;auto=format 640w, https://cdn.sanity.io/images/76e3r62u/production/5129846575d1f7d8aa08a3784a1ff9eced8ba307-1000x500.png?w=750&amp;q=75&amp;fit=clip&amp;auto=format 750w, https://cdn.sanity.io/images/76e3r62u/production/5129846575d1f7d8aa08a3784a1ff9eced8ba307-1000x500.png?w=828&amp;q=75&amp;fit=clip&amp;auto=format 828w, https://cdn.sanity.io/images/76e3r62u/production/5129846575d1f7d8aa08a3784a1ff9eced8ba307-1000x500.png?w=1080&amp;q=75&amp;fit=clip&amp;auto=format 1080w, https://cdn.sanity.io/images/76e3r62u/production/5129846575d1f7d8aa08a3784a1ff9eced8ba307-1000x500.png?w=1200&amp;q=75&amp;fit=clip&amp;auto=format 1200w, https://cdn.sanity.io/images/76e3r62u/production/5129846575d1f7d8aa08a3784a1ff9eced8ba307-1000x500.png?w=1920&amp;q=75&amp;fit=clip&amp;auto=format 1920w, https://cdn.sanity.io/images/76e3r62u/production/5129846575d1f7d8aa08a3784a1ff9eced8ba307-1000x500.png?w=2048&amp;q=75&amp;fit=clip&amp;auto=format 2048w, https://cdn.sanity.io/images/76e3r62u/production/5129846575d1f7d8aa08a3784a1ff9eced8ba307-1000x500.png?w=3840&amp;q=75&amp;fit=clip&amp;auto=format 3840w" src="https://cdn.sanity.io/images/76e3r62u/production/5129846575d1f7d8aa08a3784a1ff9eced8ba307-1000x500.png?w=3840&amp;q=75&amp;fit=clip&amp;auto=format" decoding="async" data-nimg="fill" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover" loading="lazy"/></noscript></div></a><div class="blog-post_postCard_data__2vfn3"><a class="blog-post_postCard_tag__9RTi-" href="/[object%20Object]">Company News</a><a href="/[object%20Object]"><h3>Sama Raises $70M Series B to Build a More Accurate, More Ethical, End-to-End AI Development Pipeline</h3></a><a class="blog-post_postCard_author__Y7RjO" href="/[object%20Object]">Wendy Gonzalez</a><p class="blog-post_postCard_date__hrDMA ">November 4, 2021<!-- --> | 4 Min Read</p></div></div><div class="blog-post_postCard__i0O-O"><a class="blog-post_postCard_image__3k8eA" href="/[object%20Object]"><div style="display:block;overflow:hidden;position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;margin:0"><img alt="" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="fill" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover"/><noscript><img alt="" sizes="100vw" srcSet="https://cdn.sanity.io/images/76e3r62u/production/1502043edcf701cb4267cb2f6d7c6edf8e0e0cad-2500x1313.png?w=640&amp;q=75&amp;fit=clip&amp;auto=format 640w, https://cdn.sanity.io/images/76e3r62u/production/1502043edcf701cb4267cb2f6d7c6edf8e0e0cad-2500x1313.png?w=750&amp;q=75&amp;fit=clip&amp;auto=format 750w, https://cdn.sanity.io/images/76e3r62u/production/1502043edcf701cb4267cb2f6d7c6edf8e0e0cad-2500x1313.png?w=828&amp;q=75&amp;fit=clip&amp;auto=format 828w, https://cdn.sanity.io/images/76e3r62u/production/1502043edcf701cb4267cb2f6d7c6edf8e0e0cad-2500x1313.png?w=1080&amp;q=75&amp;fit=clip&amp;auto=format 1080w, https://cdn.sanity.io/images/76e3r62u/production/1502043edcf701cb4267cb2f6d7c6edf8e0e0cad-2500x1313.png?w=1200&amp;q=75&amp;fit=clip&amp;auto=format 1200w, https://cdn.sanity.io/images/76e3r62u/production/1502043edcf701cb4267cb2f6d7c6edf8e0e0cad-2500x1313.png?w=1920&amp;q=75&amp;fit=clip&amp;auto=format 1920w, https://cdn.sanity.io/images/76e3r62u/production/1502043edcf701cb4267cb2f6d7c6edf8e0e0cad-2500x1313.png?w=2048&amp;q=75&amp;fit=clip&amp;auto=format 2048w, https://cdn.sanity.io/images/76e3r62u/production/1502043edcf701cb4267cb2f6d7c6edf8e0e0cad-2500x1313.png?w=3840&amp;q=75&amp;fit=clip&amp;auto=format 3840w" src="https://cdn.sanity.io/images/76e3r62u/production/1502043edcf701cb4267cb2f6d7c6edf8e0e0cad-2500x1313.png?w=3840&amp;q=75&amp;fit=clip&amp;auto=format" decoding="async" data-nimg="fill" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover" loading="lazy"/></noscript></div></a><div class="blog-post_postCard_data__2vfn3"><a class="blog-post_postCard_tag__9RTi-" href="/[object%20Object]">Podcast</a><a href="/[object%20Object]"><h3>New Podcast Episode: Moxie the Conversational AI Robot Teaches Children Kindness</h3></a><a class="blog-post_postCard_author__Y7RjO" href="/[object%20Object]">Rob Stevenson</a><p class="blog-post_postCard_date__hrDMA ">October 25, 2021<!-- --> | 2 Min Read</p></div></div><div class="blog-post_postCard__i0O-O"><a class="blog-post_postCard_image__3k8eA" href="/[object%20Object]"><div style="display:block;overflow:hidden;position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;margin:0"><img alt="" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="fill" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover"/><noscript><img alt="" sizes="100vw" srcSet="https://cdn.sanity.io/images/76e3r62u/production/5b774da87e817474525a389529a9ea674aa54a11-4167x2084.png?w=640&amp;q=75&amp;fit=clip&amp;auto=format 640w, https://cdn.sanity.io/images/76e3r62u/production/5b774da87e817474525a389529a9ea674aa54a11-4167x2084.png?w=750&amp;q=75&amp;fit=clip&amp;auto=format 750w, https://cdn.sanity.io/images/76e3r62u/production/5b774da87e817474525a389529a9ea674aa54a11-4167x2084.png?w=828&amp;q=75&amp;fit=clip&amp;auto=format 828w, https://cdn.sanity.io/images/76e3r62u/production/5b774da87e817474525a389529a9ea674aa54a11-4167x2084.png?w=1080&amp;q=75&amp;fit=clip&amp;auto=format 1080w, https://cdn.sanity.io/images/76e3r62u/production/5b774da87e817474525a389529a9ea674aa54a11-4167x2084.png?w=1200&amp;q=75&amp;fit=clip&amp;auto=format 1200w, https://cdn.sanity.io/images/76e3r62u/production/5b774da87e817474525a389529a9ea674aa54a11-4167x2084.png?w=1920&amp;q=75&amp;fit=clip&amp;auto=format 1920w, https://cdn.sanity.io/images/76e3r62u/production/5b774da87e817474525a389529a9ea674aa54a11-4167x2084.png?w=2048&amp;q=75&amp;fit=clip&amp;auto=format 2048w, https://cdn.sanity.io/images/76e3r62u/production/5b774da87e817474525a389529a9ea674aa54a11-4167x2084.png?w=3840&amp;q=75&amp;fit=clip&amp;auto=format 3840w" src="https://cdn.sanity.io/images/76e3r62u/production/5b774da87e817474525a389529a9ea674aa54a11-4167x2084.png?w=3840&amp;q=75&amp;fit=clip&amp;auto=format" decoding="async" data-nimg="fill" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover" loading="lazy"/></noscript></div></a><div class="blog-post_postCard_data__2vfn3"><a class="blog-post_postCard_tag__9RTi-" href="/[object%20Object]">Case Studies</a><a href="/[object%20Object]"><h3>How More Accurate Data Labeling is Helping PolyPerception Advocate for Responsible Waste Management</h3></a><a class="blog-post_postCard_author__Y7RjO" href="/[object%20Object]">Amanda Durepos</a><p class="blog-post_postCard_date__hrDMA ">October 13, 2021<!-- --> | 8 Min Read</p></div></div><div class="blog-post_postCard__i0O-O"><a class="blog-post_postCard_image__3k8eA" href="/[object%20Object]"><div style="display:block;overflow:hidden;position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;margin:0"><img alt="" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="fill" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover"/><noscript><img alt="" sizes="100vw" srcSet="https://cdn.sanity.io/images/76e3r62u/production/0434b713b7dcba1cdf98eff0eb50d7cb3d5fb008-1500x908.jpg?w=640&amp;q=75&amp;fit=clip&amp;auto=format 640w, https://cdn.sanity.io/images/76e3r62u/production/0434b713b7dcba1cdf98eff0eb50d7cb3d5fb008-1500x908.jpg?w=750&amp;q=75&amp;fit=clip&amp;auto=format 750w, https://cdn.sanity.io/images/76e3r62u/production/0434b713b7dcba1cdf98eff0eb50d7cb3d5fb008-1500x908.jpg?w=828&amp;q=75&amp;fit=clip&amp;auto=format 828w, https://cdn.sanity.io/images/76e3r62u/production/0434b713b7dcba1cdf98eff0eb50d7cb3d5fb008-1500x908.jpg?w=1080&amp;q=75&amp;fit=clip&amp;auto=format 1080w, https://cdn.sanity.io/images/76e3r62u/production/0434b713b7dcba1cdf98eff0eb50d7cb3d5fb008-1500x908.jpg?w=1200&amp;q=75&amp;fit=clip&amp;auto=format 1200w, https://cdn.sanity.io/images/76e3r62u/production/0434b713b7dcba1cdf98eff0eb50d7cb3d5fb008-1500x908.jpg?w=1920&amp;q=75&amp;fit=clip&amp;auto=format 1920w, https://cdn.sanity.io/images/76e3r62u/production/0434b713b7dcba1cdf98eff0eb50d7cb3d5fb008-1500x908.jpg?w=2048&amp;q=75&amp;fit=clip&amp;auto=format 2048w, https://cdn.sanity.io/images/76e3r62u/production/0434b713b7dcba1cdf98eff0eb50d7cb3d5fb008-1500x908.jpg?w=3840&amp;q=75&amp;fit=clip&amp;auto=format 3840w" src="https://cdn.sanity.io/images/76e3r62u/production/0434b713b7dcba1cdf98eff0eb50d7cb3d5fb008-1500x908.jpg?w=3840&amp;q=75&amp;fit=clip&amp;auto=format" decoding="async" data-nimg="fill" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover" loading="lazy"/></noscript></div></a><div class="blog-post_postCard_data__2vfn3"><a class="blog-post_postCard_tag__9RTi-" href="/[object%20Object]">Company News</a><a href="/[object%20Object]"><h3>Sama Made the Inc. 5000 List (For the Second Year in a Row!)</h3></a><a class="blog-post_postCard_author__Y7RjO" href="/[object%20Object]">Sama Team</a><p class="blog-post_postCard_date__hrDMA ">August 17, 2021<!-- --> | 2 Min Read</p></div></div><div class="blog-post_postCard__i0O-O"><a class="blog-post_postCard_image__3k8eA" href="/[object%20Object]"><div style="display:block;overflow:hidden;position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;margin:0"><img alt="" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="fill" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover"/><noscript><img alt="" sizes="100vw" srcSet="https://cdn.sanity.io/images/76e3r62u/production/1502043edcf701cb4267cb2f6d7c6edf8e0e0cad-2500x1313.png?w=640&amp;q=75&amp;fit=clip&amp;auto=format 640w, https://cdn.sanity.io/images/76e3r62u/production/1502043edcf701cb4267cb2f6d7c6edf8e0e0cad-2500x1313.png?w=750&amp;q=75&amp;fit=clip&amp;auto=format 750w, https://cdn.sanity.io/images/76e3r62u/production/1502043edcf701cb4267cb2f6d7c6edf8e0e0cad-2500x1313.png?w=828&amp;q=75&amp;fit=clip&amp;auto=format 828w, https://cdn.sanity.io/images/76e3r62u/production/1502043edcf701cb4267cb2f6d7c6edf8e0e0cad-2500x1313.png?w=1080&amp;q=75&amp;fit=clip&amp;auto=format 1080w, https://cdn.sanity.io/images/76e3r62u/production/1502043edcf701cb4267cb2f6d7c6edf8e0e0cad-2500x1313.png?w=1200&amp;q=75&amp;fit=clip&amp;auto=format 1200w, https://cdn.sanity.io/images/76e3r62u/production/1502043edcf701cb4267cb2f6d7c6edf8e0e0cad-2500x1313.png?w=1920&amp;q=75&amp;fit=clip&amp;auto=format 1920w, https://cdn.sanity.io/images/76e3r62u/production/1502043edcf701cb4267cb2f6d7c6edf8e0e0cad-2500x1313.png?w=2048&amp;q=75&amp;fit=clip&amp;auto=format 2048w, https://cdn.sanity.io/images/76e3r62u/production/1502043edcf701cb4267cb2f6d7c6edf8e0e0cad-2500x1313.png?w=3840&amp;q=75&amp;fit=clip&amp;auto=format 3840w" src="https://cdn.sanity.io/images/76e3r62u/production/1502043edcf701cb4267cb2f6d7c6edf8e0e0cad-2500x1313.png?w=3840&amp;q=75&amp;fit=clip&amp;auto=format" decoding="async" data-nimg="fill" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover" loading="lazy"/></noscript></div></a><div class="blog-post_postCard_data__2vfn3"><a class="blog-post_postCard_tag__9RTi-" href="/[object%20Object]">Ethical AI</a><a href="/[object%20Object]"><h3>New Podcast Episode: Making AI Development Global with Google&#x27;s Laurence Moroney</h3></a><a class="blog-post_postCard_author__Y7RjO" href="/[object%20Object]">Amanda Durepos</a><p class="blog-post_postCard_date__hrDMA ">August 10, 2021<!-- --> | 45 Min Read</p></div></div><div class="blog-post_postCard__i0O-O"><a class="blog-post_postCard_image__3k8eA" href="/[object%20Object]"><div style="display:block;overflow:hidden;position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;margin:0"><img alt="" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="fill" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover"/><noscript><img alt="" sizes="100vw" srcSet="https://cdn.sanity.io/images/76e3r62u/production/65b7fac1b60586a0a7b9ff75006684e2c2467f1e-1800x900.png?w=640&amp;q=75&amp;fit=clip&amp;auto=format 640w, https://cdn.sanity.io/images/76e3r62u/production/65b7fac1b60586a0a7b9ff75006684e2c2467f1e-1800x900.png?w=750&amp;q=75&amp;fit=clip&amp;auto=format 750w, https://cdn.sanity.io/images/76e3r62u/production/65b7fac1b60586a0a7b9ff75006684e2c2467f1e-1800x900.png?w=828&amp;q=75&amp;fit=clip&amp;auto=format 828w, https://cdn.sanity.io/images/76e3r62u/production/65b7fac1b60586a0a7b9ff75006684e2c2467f1e-1800x900.png?w=1080&amp;q=75&amp;fit=clip&amp;auto=format 1080w, https://cdn.sanity.io/images/76e3r62u/production/65b7fac1b60586a0a7b9ff75006684e2c2467f1e-1800x900.png?w=1200&amp;q=75&amp;fit=clip&amp;auto=format 1200w, https://cdn.sanity.io/images/76e3r62u/production/65b7fac1b60586a0a7b9ff75006684e2c2467f1e-1800x900.png?w=1920&amp;q=75&amp;fit=clip&amp;auto=format 1920w, https://cdn.sanity.io/images/76e3r62u/production/65b7fac1b60586a0a7b9ff75006684e2c2467f1e-1800x900.png?w=2048&amp;q=75&amp;fit=clip&amp;auto=format 2048w, https://cdn.sanity.io/images/76e3r62u/production/65b7fac1b60586a0a7b9ff75006684e2c2467f1e-1800x900.png?w=3840&amp;q=75&amp;fit=clip&amp;auto=format 3840w" src="https://cdn.sanity.io/images/76e3r62u/production/65b7fac1b60586a0a7b9ff75006684e2c2467f1e-1800x900.png?w=3840&amp;q=75&amp;fit=clip&amp;auto=format" decoding="async" data-nimg="fill" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover" loading="lazy"/></noscript></div></a><div class="blog-post_postCard_data__2vfn3"><a class="blog-post_postCard_tag__9RTi-" href="/[object%20Object]">Machine Learning</a><a href="/[object%20Object]"><h3>ML Assisted Annotation Powered by MicroModels</h3></a><a class="blog-post_postCard_author__Y7RjO" href="/[object%20Object]">Saul Miller</a><p class="blog-post_postCard_date__hrDMA ">July 29, 2021<!-- --> | 7 Min Read</p></div></div><div class="blog-post_postCard__i0O-O"><a class="blog-post_postCard_image__3k8eA" href="/[object%20Object]"><div style="display:block;overflow:hidden;position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;margin:0"><img alt="" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="fill" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover"/><noscript><img alt="" sizes="100vw" srcSet="https://cdn.sanity.io/images/76e3r62u/production/fdeacb17e65c946761066de369acc4bdf1353a73-3334x1668.png?w=640&amp;q=75&amp;fit=clip&amp;auto=format 640w, https://cdn.sanity.io/images/76e3r62u/production/fdeacb17e65c946761066de369acc4bdf1353a73-3334x1668.png?w=750&amp;q=75&amp;fit=clip&amp;auto=format 750w, https://cdn.sanity.io/images/76e3r62u/production/fdeacb17e65c946761066de369acc4bdf1353a73-3334x1668.png?w=828&amp;q=75&amp;fit=clip&amp;auto=format 828w, https://cdn.sanity.io/images/76e3r62u/production/fdeacb17e65c946761066de369acc4bdf1353a73-3334x1668.png?w=1080&amp;q=75&amp;fit=clip&amp;auto=format 1080w, https://cdn.sanity.io/images/76e3r62u/production/fdeacb17e65c946761066de369acc4bdf1353a73-3334x1668.png?w=1200&amp;q=75&amp;fit=clip&amp;auto=format 1200w, https://cdn.sanity.io/images/76e3r62u/production/fdeacb17e65c946761066de369acc4bdf1353a73-3334x1668.png?w=1920&amp;q=75&amp;fit=clip&amp;auto=format 1920w, https://cdn.sanity.io/images/76e3r62u/production/fdeacb17e65c946761066de369acc4bdf1353a73-3334x1668.png?w=2048&amp;q=75&amp;fit=clip&amp;auto=format 2048w, https://cdn.sanity.io/images/76e3r62u/production/fdeacb17e65c946761066de369acc4bdf1353a73-3334x1668.png?w=3840&amp;q=75&amp;fit=clip&amp;auto=format 3840w" src="https://cdn.sanity.io/images/76e3r62u/production/fdeacb17e65c946761066de369acc4bdf1353a73-3334x1668.png?w=3840&amp;q=75&amp;fit=clip&amp;auto=format" decoding="async" data-nimg="fill" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover" loading="lazy"/></noscript></div></a><div class="blog-post_postCard_data__2vfn3"><a class="blog-post_postCard_tag__9RTi-" href="/[object%20Object]">Ethical AI</a><a href="/[object%20Object]"><h3>New White Paper by Partnership on AI: Responsible Sourcing of Data Enrichment Services</h3></a><a class="blog-post_postCard_author__Y7RjO" href="/[object%20Object]">Kristen Itani Koue</a><p class="blog-post_postCard_date__hrDMA ">July 15, 2021<!-- --> | 4 Min Read</p></div></div></div></section><section class="umoja-l-grid-section umoja-l-grid-section--flat-top umoja-u-bg--white"><div class="button_wrapper__3lRbv button__light__3zLll "><button class="button_btn__1qxP1"><h3 class="button_text__3_sCS">Load More</h3></button></div></section></main><footer class="footer_wrapper__2VAfJ"><div class="umoja-l-grid--12"><div class="footer_upper__2a6XG"><div><h4>Newsletter</h4><p>Subscribe today and be the first to receive the latest from Sama.</p></div><div class="footer_upper_right__cpliC"><div><p class="footer_nav_head__1keQK">Guides</p><a class="footer_nav_link__X1RNI" href="/training-data-for-autonomous-driving">Autonomous Transportation</a><a class="footer_nav_link__X1RNI" href="/training-data-for-ecommerce">E-Commerce</a><a class="footer_nav_link__X1RNI" href="/training-data-for-ar-vr">AR/VR</a><a class="footer_nav_link__X1RNI" href="/data-quality">Data Quality</a></div><div><p class="footer_nav_head__1keQK">Company</p><a class="footer_nav_link__X1RNI" href="/our-story">Our Story</a><a class="footer_nav_link__X1RNI" href="/our-team">Our Team</a><a class="footer_nav_link__X1RNI" href="/mission-vision-values">Our Mission</a><a class="footer_nav_link__X1RNI" href="/careers">Careers</a><a class="footer_nav_link__X1RNI" href="/company-contact">Contact</a></div></div></div><div class="footer_middle__iiTSJ"><div class="footer_middle_left__3ff78"><a href="/"><span class="visually-hidden">Sama</span></a></div><div class="footer_middle_right__2b-lC"><div class="footer_social__1NFfV"><a href="https://www.facebook.com/samaartificialintelligence" class="footer_social_icon__wI2OK" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 35.1 64.89"><title>facebook</title><g id="fb3209c8-23d4-4288-984d-a2b8f32b7f0c" data-name="Layer 2"><g id="ba1a815d-13f1-45f9-9321-18e9a3cfa5db" data-name="Layer 1"><path d="M35.1,11.26V1.36A1.35,1.35,0,0,0,33.76,0H25.35A15.34,15.34,0,0,0,14,4.35C11.24,7.2,9.78,11.22,9.78,16v7.34H1.34A1.34,1.34,0,0,0,0,24.66V35.32a1.35,1.35,0,0,0,1.34,1.35H9.78V63.55a1.34,1.34,0,0,0,1.34,1.34h11a1.34,1.34,0,0,0,1.34-1.34V36.67h9.87a1.35,1.35,0,0,0,1.34-1.35V24.66a1.37,1.37,0,0,0-.7-1.18,1.47,1.47,0,0,0-.67-.16H23.49V17.1c0-1.72.25-2.69.84-3.37s1.88-1.13,3.77-1.13h5.66A1.34,1.34,0,0,0,35.1,11.26Z"></path></g></g></svg></a><a href="https://www.instagram.com/sama_ai_" class="footer_social_icon__wI2OK" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 57 57"><title>insta</title><g id="ef02a3ef-c0d3-4be7-9d4e-2c42263777a3" data-name="Layer 2"><g id="a0a92778-6c5c-4e06-a08f-641546580dd0" data-name="Layer 1"><circle cx="28.5" cy="28.5" r="9.24"></circle><path d="M41.57,0H15.43A15.45,15.45,0,0,0,0,15.43V41.57A15.45,15.45,0,0,0,15.43,57H41.57A15.45,15.45,0,0,0,57,41.57V15.43A15.45,15.45,0,0,0,41.57,0ZM28.5,42.74A14.24,14.24,0,1,1,42.74,28.5,14.26,14.26,0,0,1,28.5,42.74ZM44.46,17a5,5,0,1,1,5-5A5,5,0,0,1,44.46,17Z"></path></g></g></svg></a><a href="https://twitter.com/SamaAI" class="footer_social_icon__wI2OK" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 62 51.19"><title>twitter</title><g id="e7743e55-1863-47ad-a995-4b7f1f924f07" data-name="Layer 2"><g id="ec8c6fd9-2f52-4507-9a24-804bc60dbc33" data-name="Layer 1"><path d="M23.59,51.19c-10.35,0-18.53-1.81-22.44-5l-.07-.06L1,46.1a3.19,3.19,0,0,1-.84-3.35l0-.1a3.24,3.24,0,0,1,3-2,26.57,26.57,0,0,0,7.06-1,13.45,13.45,0,0,1-7.07-8.16,2.92,2.92,0,0,1,1-3.38,3.06,3.06,0,0,1,.88-.45,19.52,19.52,0,0,1-4-7.18l0-.08,0-.09a3,3,0,0,1,1.4-3.23,3,3,0,0,1,1.43-.4,15.15,15.15,0,0,1-1.14-3.49A14.59,14.59,0,0,1,4.24,3.47l.38-.77a2.15,2.15,0,0,1,3.44-.56l.7.7c5.53,5.81,10.49,8.56,19.06,10.44a15.17,15.17,0,0,1,4.1-8.75A14.39,14.39,0,0,1,42.19,0h0c2.84,0,6.36,1.62,8.49,2.77,1.83-.6,4-1.53,6.32-2.51a2.88,2.88,0,0,1,3.22.57,2.85,2.85,0,0,1,.62,3.11c-.17.47-.36.92-.57,1.36a3.07,3.07,0,0,1,.84.58,3.13,3.13,0,0,1,.78,2.92l0,.1a11.92,11.92,0,0,1-4.78,6.56C56.73,35.23,41.84,51.19,23.59,51.19Z"></path></g></g></svg></a><a href="https://www.linkedin.com/company/sama-ai/" class="footer_social_icon__wI2OK" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 59.71 60.79"><title>linkedin</title><g id="bd073dff-b6ea-4cf0-bfeb-c3ce392ee6a5" data-name="Layer 2"><g id="f6b6eb77-7c10-4e27-a3d0-6f8b14e97f2e" data-name="Layer 1"><path d="M59.65,60.79l-12.35,0,0-19.36c0-4.62-.07-10.56-6.41-10.58s-7.44,5-7.45,10.21l0,19.7-12.36,0,.09-40.95,11.87,0v6.57h.16c1.66-3.13,5.7-6.42,11.73-6.41,12.51,0,14.81,8.28,14.79,19l-.05,21.85Z"></path><path d="M7.17,14.35a7.18,7.18,0,1,1,7.18-7.18A7.17,7.17,0,0,1,7.17,14.35Z"></path><rect x="0.98" y="19.8" width="12.39" height="40.95"></rect></g></g></svg></a><a href="https://www.youtube.com/c/SamaAI" class="footer_social_icon__wI2OK" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 65 44.63"><title>youtube</title><g id="b03ac260-8029-40d4-832d-1f2d757db2d5" data-name="Layer 2"><g id="eca1bfe4-f0ec-4c24-9169-be6d2f8b24b5" data-name="Layer 1"><path d="M55,0H10A10,10,0,0,0,0,10V34.63a10,10,0,0,0,10,10H55a10,10,0,0,0,10-10V10A10,10,0,0,0,55,0ZM40.89,24.41,28.3,31.18a2.31,2.31,0,0,1-3.41-2V15.48a2.3,2.3,0,0,1,3.42-2l12.6,6.89a2.31,2.31,0,0,1,0,4.06Z"></path></g></g></svg></a></div></div></div><div class="footer_lower__1z3Av"><div class="footer_lower_left__141hE"><a class="footer_nav_link__X1RNI" href="/terms-of-service">Terms</a><a class="footer_nav_link__X1RNI" href="/privacy-policy">Privacy</a><a class="footer_nav_link__X1RNI" href="/quality-and-information-policy">Quality &amp; Information</a></div><div class="footer_lower_right__22vMw"><h6>Copyright © <!-- -->0<!-- --> Sama Inc.</h6><h6>All rights reserved.</h6></div></div></div></footer></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"config":{"footerNav":{"items":[{"_key":"f255606f8f25","_type":"navDropdownMenu","items":[{"_key":"76389ad94cbb","_type":"navItem","title":"Autonomous Transportation","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"training-data-for-autonomous-driving"}}}},{"_key":"5f64a8d6a69d","_type":"navItem","title":"E-Commerce","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"training-data-for-ecommerce"}}}},{"_key":"f10e54ae04d0","_type":"navItem","title":"AR/VR","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"training-data-for-ar-vr"}}}},{"_key":"fd729b522a77","_type":"navItem","title":"Data Quality","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"data-quality"}}}}],"title":"Guides","url":null},{"_key":"681ef7d8763a","_type":"navDropdownMenu","items":[{"_key":"6238a422b667","_type":"navItem","title":"Our Story","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"our-story"}}}},{"_key":"258985d6d46b","_type":"navItem","title":"Our Team","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"our-team"}}}},{"_key":"e0a76077324a","_type":"navItem","title":"Our Mission","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"mission-vision-values"}}}},{"_key":"239e49661b0d","_type":"navItem","title":"Careers","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"careers"}}}},{"_key":"e005a740cd80","_type":"navItem","title":"Contact","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"company-contact"}}}}],"title":"Company","url":null}]},"logo":{"asset":{"_createdAt":"2021-12-09T21:42:35Z","_id":"image-4f4e2f86a8fad952c02dffffd7008aa35f83c850-2380x636-svg","_rev":"7Z7VDk3xHzg51hvomGzc99","_type":"sanity.imageAsset","_updatedAt":"2021-12-09T21:42:35Z","assetId":"4f4e2f86a8fad952c02dffffd7008aa35f83c850","extension":"svg","metadata":{"_type":"sanity.imageMetadata","blurHash":"D009jvfQfQfQfQfQfQfQfQfQ","dimensions":{"_type":"sanity.imageDimensions","aspectRatio":3.742138364779874,"height":636,"width":2380},"hasAlpha":true,"isOpaque":false,"lqip":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAFCAYAAABFA8wzAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAr0lEQVQYlU3QyUoDQBAE0HeIEjeixAVyESRI3BNFJSZxAU/+/wdJQSEehu6Zqa6uKnjBGz7xgTuc4gAj7LWe4LD9cc8Y+9jBUed84QGrEt/gEleYY9Ylr3jGU/tV77fFLirId0nWBYYgoACi+r3D6YPN0vwFm4VxmNlgfkK4qcLUZdVMcdHB+75FzWPfzzCpxXNcFxuOP2uxnhNLASaX5LjbnJJf6jYG2PpXh812/AvSEQ+GGZqgYgAAAABJRU5ErkJggg==","palette":{"_type":"sanity.imagePalette","darkMuted":{"_type":"sanity.imagePaletteSwatch","background":"#040404","foreground":"#fff","population":100.29,"title":"#fff"},"darkVibrant":{"_type":"sanity.imagePaletteSwatch","background":"#424242","foreground":"#fff","population":0,"title":"#fff"},"dominant":{"_type":"sanity.imagePaletteSwatch","background":"#040404","foreground":"#fff","population":100.29,"title":"#fff"},"lightMuted":{"_type":"sanity.imagePaletteSwatch","background":"#4c4c4c","foreground":"#fff","population":0,"title":"#fff"},"lightVibrant":{"_type":"sanity.imagePaletteSwatch","background":"#bcbcbc","foreground":"#000","population":0,"title":"#fff"},"muted":{"_type":"sanity.imagePaletteSwatch","background":"#4c4c4c","foreground":"#fff","population":0,"title":"#fff"},"vibrant":{"_type":"sanity.imagePaletteSwatch","background":"#7f7f7f","foreground":"#fff","population":0,"title":"#fff"}}},"mimeType":"image/svg+xml","originalFilename":"e20f8cc53e5f74df10ae9a822edb7ec2c4d00f02-2380x636.svg","path":"images/76e3r62u/production/4f4e2f86a8fad952c02dffffd7008aa35f83c850-2380x636.svg","sha1hash":"4f4e2f86a8fad952c02dffffd7008aa35f83c850","size":2009,"uploadId":"jTUF9DIFqAwpLJ0GcI9bRqb17D69QQlN","url":"https://cdn.sanity.io/images/76e3r62u/production/4f4e2f86a8fad952c02dffffd7008aa35f83c850-2380x636.svg"}},"mainNav":{"items":[{"_key":"58c18e9aa9ea","_type":"navDropdownMenu","items":[{"_key":"b5b5b8bee78b","_type":"navCat","items":[{"_key":"0e80156a2f1a","_type":"navItem","title":"How it Works","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"how-it-works"}}}},{"_key":"40bacee029b4","_type":"navItem","title":"Video Annotation","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"video-annotation"}}}},{"_key":"32650ef07503","_type":"navItem","title":"Image Annotation","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"image-annotation"}}}},{"_key":"fe9137cd0167","_type":"navItem","title":"3D \u0026 LiDAR Annotation","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"3d-lidar"}}}},{"_key":"d9a1316d400a","_type":"navItem","title":"Natural Language Processing","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"natural-language-processing"}}}},{"_key":"ac12c7c5d70a","_type":"navItem","title":"Data Curation (Beta)","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"data-curation"}}}}],"title":"Platform","url":null},{"_key":"37ff4fa913bd","_type":"navCat","items":[{"_key":"6026b1a9314e","_type":"navItem","title":"Semantic Segmentation","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"semantic-segmentation"}}}},{"_key":"f4611b19b406","_type":"navItem","title":"Polygons","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"polygons"}}}},{"_key":"5155d874d6c8","_type":"navItem","title":"Bounding Boxes","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"bounding-boxes"}}}},{"_key":"9ef3c1e21e74","_type":"navItem","title":"Key Points","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"key-points"}}}},{"_key":"314d4c00d351","_type":"navItem","title":"Cuboids","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"cuboids"}}}},{"_key":"8e17a6388d74","_type":"navItem","title":"Lines \u0026 Arrows","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"lines-and-arrows"}}}}],"title":"Shapes","url":null}],"title":"Platform","url":null},{"_key":"112867ca4d03","_type":"navDropdownMenu","items":[{"_key":"22699c7e06cb","_type":"navItem","items":null,"title":"Transportation \u0026 Navigation","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"transportation-navigation"}}}},{"_key":"122ae5928d6d","_type":"navItem","items":null,"title":"Retail \u0026 E-Commerce","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"retail-ecommerce"}}}},{"_key":"7bb234b69fb0","_type":"navItem","items":null,"title":"Consumer \u0026 Media","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"consumer-media"}}}},{"_key":"33e6a886b39d","_type":"navItem","items":null,"title":"Biotech \u0026 Medtech","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"biotech-medtech"}}}},{"_key":"d095b2619c4e","_type":"navItem","items":null,"title":"Robotics \u0026 Manufacturing","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"robotics-and-manufacturing"}}}},{"_key":"2c4b82a94d79","_type":"navItem","items":null,"title":"Food \u0026 Agriculture","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"training-data-food-agriculture"}}}}],"title":"Industries","url":null},{"_key":"c47e8763a906","_type":"navDropdownMenu","items":[{"_key":"1d563df30b3f","_type":"navItem","items":null,"title":"Quality","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"quality-training-data"}}}},{"_key":"041725f35d96","_type":"navItem","items":null,"title":"Security","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"security-and-trust"}}}},{"_key":"fd64ede25798","_type":"navItem","items":null,"title":"Ethical AI","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"our-impact"}}}},{"_key":"398dcbb1c95d","_type":"navItem","items":null,"title":"Compare","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"compare"}}}},{"_key":"93bdfdd87879","_type":"navItem","items":null,"title":"Partners","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"partners"}}}}],"title":"Why Sama","url":null},{"_key":"1d38bf63df54","_type":"navDropdownMenu","items":[{"_key":"be81659b38a5","_type":"navItem","items":null,"title":"API Documentation","url":{"_type":"link","externalUrl":"https://docs.sama.com/reference/overview","internalLink":null}},{"_key":"2cec80e94962","_type":"navItem","items":null,"title":"Blog","url":{"_type":"link","internalLink":null,"internalLink_custom":"/blog"}},{"_key":"09e284fcb1d3","_type":"navItem","items":null,"title":"Events","url":{"_type":"link","internalLink":null,"internalLink_custom":"/events"}}],"title":"Resources","url":null},{"_key":"dbee93713c19","_type":"navDropdownMenu","items":[{"_key":"12d594a568bf","_type":"navItem","items":null,"title":"Our Story","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"our-story"}}}},{"_key":"ce36540a102d","_type":"navItem","items":null,"title":"Our Team","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"our-team"}}}},{"_key":"34fc328e8022","_type":"navItem","items":null,"title":"Careers","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"careers"}}}},{"_key":"c1fe2961020a","_type":"navItem","items":null,"title":"Contact","url":{"_type":"link","internalLink":{"slug":{"_type":"slug","current":"company-contact"}}}},{"_key":"ebd81873e538","_type":"navItem","items":null,"title":"Press","url":{"_type":"link","internalLink":null,"internalLink_custom":"/press"}}],"title":"Company","url":null}],"nav_cta":{"_type":"button","link":{"_type":"link","internalLink":{"_ref":"136788cb-06a6-4f27-b75b-07faf403bfa6","_type":"reference"}},"title":"Request a Demo","type":"secondary"}}},"topPost":{"_createdAt":"2021-12-08T20:36:31Z","author":{"_id":"e8a1f3c6-0b07-4638-93d5-61a0924f32e3","avatar":{"_type":"image","asset":{"_ref":"image-2f8efc599a377bd580d0ae12f022a15e7e1acacd-632x676-webp","_type":"reference"}},"bio":"Amanda is the Content Marketing Manager at Sama, where each day she gets to learn about AI and how it’s transforming industry. She’s passionate about technology but periodically tries to get away from her computer to explore more analog things like collage and quilting.","name":"Amanda Durepos","slug":{"_type":"slug","current":"amanda-durepos"}},"estimatedReadingTime":7,"featured_image":{"_type":"image","asset":{"_ref":"image-30d0eaccd1e57322b31a4e16b84576ef1f8db57e-1920x960-png","_type":"reference"}},"meta_description":"zeroG partnered with Sama to empower turnaround coordinators to make data-driven decisions that improve time efficiency, cut costs, and ensure safety.","openGraphImage":{"url":"https://cdn.sanity.io/images/76e3r62u/production/30d0eaccd1e57322b31a4e16b84576ef1f8db57e-1920x960.png"},"plaintextBody":"zeroG, a Lufthansa Systems company, uses AI, data science, and analytics to help airports and airlines optimize operational efficiency. zeroG partnered with Sama to power their Deep Turnaround solution, which gives turnaround coordinators unprecedented visibility into turnaround processes, so they can shorten timelines, cut costs, and ensure optimal safety.\n\n\n\nIn aviation, turnaround management refers to the process of preparing an aircraft for its next flight. It involves the seamless orchestration of a series of cross-functional tasks — cleaning, catering, refueling, boarding, and loading baggage to name a few — all in service of shortening the time between an aircraft’s arrival at the terminal gate and its departure for the next scheduled flight.\n\nIf you’ve ever sat in an airplane on the tarmac for 20 minutes longer than anticipated, you’ve felt the effects of inefficient turnaround times. But passengers are not the only ones paying the price; every unnecessary minute that an airplane spends on the tarmac costs airlines money. Of the $60 billion that disruptions are estimated to cost airlines, a staggering $25 billion is due to lost productivity and downtime.\n\nAirlines and airports can unlock significant cost savings by shaving even a couple of minutes off of their turnaround times, but many struggle to do so for one simple reason: they lack visibility into the end-to-end process.\n\n\n\nThe black box of turnaround times\n\n\nToday, turnaround remains an opaque and uncoordinated process for many airlines. The many tasks that must be executed in parallel are tracked by disparate systems or in some cases, manually, with multiple human stakeholders capturing time stamps. This lends itself to human error, a lack of precision, and ultimately, unreliable data for the decision-making required to optimize turnaround times.\n\nWithout detailed and comprehensive data, it is difficult for airports to perform root cause analysis of inefficiencies in their turnaround processes. Additionally, airlines cannot effectively track SLAs with their various contractors – sometimes up to 12 at once, from catering to fueling and cleaning services.\n\nWhat’s worse, also obscured by this black box is data on safety liabilities; information that could help mitigate and prevent dangerous situations on the tarmac. Foreign objects and debris — garbage, ice, birds, safety pylons, or even tools left behind during maintenance — are a common cause of delays and accidents. Still, prevention remains a largely manual process.\n\nAirports must operate as efficiently as possible within their current infrastructure to maximize ROI. They need turnaround to be as smooth and quick as possible, without jeopardizing safety. Crucially, they want to ensure that they are providing a safe place for employees to work. To succeed at all of this, they need accurate data on each part of their turnaround process.\n\n\nMore reliable turnaround data with machine learning\n\n\nUsing live video feeds from cameras across an airport and computer vision, zeroG provides accurate and reliable turnaround data for airports and airlines. They are on a mission to provide unprecedented visibility into the intricacies of the turnaround process — data far more reliable than that currently at the disposal of airlines. In the words of Manuel Van Esch, Lead Consultant at zeroG:\n\n“In the aviation industry, if a tool doesn’t perform better than humans, it will not be adopted. We knew our models needed to be more reliable than a human. For that, we needed exquisitely labeled data.”\n\nzeroG needed a labeling partner to help annotate their vast dataset of videos from various airports. The videos captured a wide variety of objects – some moving, some stationary: aircraft types, water trucks, catering trucks, employees, safety pylons, foreign object debris that could potentially get sucked into the engine.\n\nSource.\n\nzeroG needed high-precision polygon and key point labels for their multi-layered processes, with little to no margin of error. To deliver the highest possible quality data, they partnered with Sama to annotate their datasets.\n\n\nHow Sama helped zeroG deliver quality datasets at scale\n\n\n\nWhen zeroG set out to push the limits of what is possible with highly accurate turnaround data, they could not compromise on any aspect of their project. They worked with extremely high-resolution video material and wanted to complement their dataset with labels that were equally high-quality.\n\nIn the early phases of the project, when experimenting with new sensors and training their models, zeroG worked directly in the Sama platform. This allowed them to work in an agile fashion, optimizing models early on in the process and enabling their experts to set a gold standard for high-quality labels. Once zeroG was ready to scale annotations, Sama annotators were able to hit the ground running. As Van Esch put it:\n\n“Sama allowed us to stay close to our data in the early stages of our project, to catch edge cases early and iterate on our instructions. By the time we were ready to scale, Sama’s workforce of annotators had a perfect understanding of the quality we needed for our models.”\n\nThis “nail it before you scale it” approach enabled zeroG to work iteratively and efficiently, in a fast-paced but controlled environment. Sama and zeroG teams remained in close communication with near-daily interaction, a collaborative approach that resulted in the delivery of a dataset at 97% quality SLA.\n\nWhat AI-powered insights can do for aviation\n\n\nHigh-quality labeled data is the fuel for zeroG’s mission to create more reliable turnaround data for airlines and airports. With more visibility into the end-to-end turnaround process, bottlenecks can be identified, and delays mitigated or even avoided entirely. This kind of unprecedented transparency has the potential to transform an opaque and reactive turnaround process into a measured and proactive one.\n\nUsing AI for real-time operations can help airlines and airports build a competitive advantage and drive real value: in the form of fewer delays, less waste, more secure work environments, and happier passengers.\n\n","seo_title":"High-Quality Labeled Data Fuels zeroG’s Mission to Optimize Aircraft Turnaround Management","slug":{"_type":"slug","current":"zerog-aircraft-turnaround"},"tags":[{"label":"Case Studies","value":"Case Studies"}],"title":"High-Quality Labeled Data Fuels zeroG’s Mission to Optimize Aircraft Turnaround Management"},"firstLoad":[{"_createdAt":"2021-12-02T20:36:31Z","author":{"_id":"53157027-f755-45c1-9fe3-65d4d9db4cc0","avatar":{"_type":"image","asset":{"_ref":"image-98673ad34672168b493ce7aba4eb1e874fdb41c3-300x300-png","_type":"reference"}},"bio":"As the VP of Corporate Development and Strategy at Sama, Alex Shee is building an AI business ecosystem around Sama's cutting-edge AI platform. In his role, Alex leads key partnerships, GTM strategy, and management of the Montreal Sama office. He was also recently selected as one of the top 250 upcoming leaders in Canada by the Governor General of Canada (the equivalent of Presidential Award), one of the top 4 business development and sales leaders in tech by Floodgates in their Annual Anchor List and 2021 \"Power Player\" by the Peak.","name":"Alex Shee","slug":{"_type":"slug","current":"alex-shee"}},"estimatedReadingTime":3,"featured_image":{"_type":"image","asset":{"_ref":"image-f80b5e83a9927bafc61285d9e7a16b07070f53c1-1200x630-png","_type":"reference"}},"meta_description":"Sama is thrilled to partner with research center Mila, to help further the mission of creating innovative technologies to drive the AI industry forward. ","openGraphImage":{"url":"https://cdn.sanity.io/images/76e3r62u/production/f80b5e83a9927bafc61285d9e7a16b07070f53c1-1200x630.png"},"plaintextBody":"As AI continues to deliver value in virtually every industry, companies seeing success from AI are standing on the shoulders of giants. For decades, research institutes have been driving innovation with significant contributions in the field of machine learning. These are the contributions that paved the way for the business applications we see all around us today.\n\n\n\nMila was founded by Turing Award Winner Yoshua Bengio back in 1993, with the mission to serve as a global pole for scientific advances that inspire innovation and the development of AI for the benefit of all. Since 2017, it has partnered with Université de Montréal and McGill University, closely linked with Polytechnique Montréal and HEC Montréal, to become the world’s largest academic research center in machine learning. \n\nToday, Mila brings together 750 researchers specializing in machine learning to collaborate with a wide variety of local, national, and international organizations to develop AI-driven projects, platforms, and partnerships to accelerate economic and social innovation. Sama is proud to be a part of this ecosystem:\n\n“As one of the top AI labs in the world, Mila is an incredible force for AI innovation. We are proud to work alongside Mila’s team of 750 researchers, professors, scientists, and students to develop the cutting-edge solutions leaders need to drive the AI industry forward, while leveraging bias-free and effective tech to positively impact the world around us.”\n- Wendy Gonzalez, CEO of Sama\n\nWith support from Mila, Sama co-hosted the first-ever workshop on Interactive Labeling and Data Augmentation for ICCV – one of the most prestigious AI conferences in the world – alongside Sasha Luccioni from Mila, Professor Jean-François Lalonde, and Professor Christian Gagné. Moreover, several Mila students who had the opportunity to intern at Sama through the partnership, have gone on to join our team full time. \n\n“The renewal of our partnership with Sama is a testament to their unwavering commitment to AI innovation and development as well as shared values of AI for good. We are happy to see Sama expanding its Montreal-based team and we look forward to continuing to work together to develop impactful solutions that will power tomorrow’s world.”\n- Yoshua Bengio, Founder and Scientific Director of Mila\n\nMoving forward, Sama and Mila will continue to work together to address AI and machine learning development challenges. In addition to co-authoring research with Mila professors and doctoral students, Sama will collaborate on Mila’s AI for Humanity projects, giving the team access to our high-quality training data platform to drive the development of AI for good.\n\nIn addition to partnering with Mila, Sama is continuing to invest in Montreal as a hub for AI innovation. Our AI R\u0026D center is located a block away from Mila’s offices, and we plan to significantly expand our Montreal-based team to over 400.\n\n(P.S.: Sama is hiring!)\n\n\n\nThank you to Mila for working with us and continuing to believe in the potential of AI for good. We can’t wait to see what the future holds.\n\nRead more about the partnership here.","seo_title":"Sama Partners with Mila to Solve Key Problems in AI Development","slug":{"_type":"slug","current":"sama-mila-partnership"},"tags":[{"_key":"ZLub2KFj","label":"Company News","value":"Company News"}],"title":"Sama Partners with Mila to Solve Key Problems in AI Development"},{"_createdAt":"2021-11-23T20:36:31Z","author":{"_id":"70f24746-bdb3-4801-adfd-17508d02ae50","avatar":{"_type":"image","asset":{"_ref":"image-9a5184335ade812b332047f70963b6e72a885c67-1194x1284-webp","_type":"reference"}},"bio":"Rob hosts \u0026 produces Sama's podcast, How AI Happens. How AI Happens is a podcast featuring experts and practitioners explaining their work at the cutting edge of Artificial Intelligence. Tune in to hear AI Researchers, Data Scientists, ML Engineers, and the leaders of today’s most exciting AI companies explain the newest and most challenging facets of their field.","name":"Rob Stevenson","slug":{"_type":"slug","current":"rob-stevenson"}},"estimatedReadingTime":2,"featured_image":{"_type":"image","asset":{"_ref":"image-1502043edcf701cb4267cb2f6d7c6edf8e0e0cad-2500x1313-png","_type":"reference"}},"meta_description":"Facebook Director in AI Manohar Paluri joins the Sama Podcast, How AI Happens, to discuss the state of computer vision and egocentric perception.","openGraphImage":{"url":"https://cdn.sanity.io/images/76e3r62u/production/1502043edcf701cb4267cb2f6d7c6edf8e0e0cad-2500x1313.png"},"plaintextBody":"Manohar Paluri has spent the bulk of his career developing methods to make machines see. Now, in his role as Director, Artificial Intelligence at Facebook (now Meta), computer vision is one building block in the massive undertaking of developing egocentric perception: making sense of data collected from a first-person perspective via wearable devices.\n\nMano joined our podcast, How AI Happens, to discuss the current state of computer vision, the challenges inherent in developing egocentric perception, and how Facebook is weighing the issues of transparency and privacy as personal data becomes, well, more personal.\n\nChief among considerations for Mano’s team is the shift from third-person sensor perception — that is, holding out a smartphone at arm’s length — to the first-person perspective granted to sensors in wearable tech. While it may not seem obvious, the difference in the data collected is tremendous. First-person perspective allows for better intention prediction with gaze recognition and hand-object interaction. However, it also has its own set of challenges, such as the user not bothering to hold their head as steady as they might hold a smartphone.\n\nMano explains how his team is tackling these issues, the ethical considerations at play, and the importance of not making sacrifices to transparency in the interest of accuracy. To hear Mano explain this and much more, you can stream the full episode below, or anywhere you get your podcasts.","seo_title":"New Podcast Episode: Facebook's Manohar Paluri Makes Machines See","slug":{"_type":"slug","current":"podcast-episode-facebook-manohar-paluri"},"tags":[{"_key":"6BgUw5oN","label":"Podcast","value":"Podcast"}],"title":"New Podcast Episode: Facebook's Manohar Paluri Makes Machines See"},{"_createdAt":"2021-11-18T20:36:31Z","author":{"_id":"0777f190-31e5-4a96-8066-90408db3cbc2","avatar":{"_type":"image","asset":{"_ref":"image-b1d5a2763e0b0ebd8c70e731c1e0f37a76c995f7-450x450-jpg","_type":"reference"}},"bio":"Suzin Wold is the Chief Marketing Officer at Sama, where she is helping companies deploy innovative and impactful AI technology. Suzin has 20 years’ experience delivering strategic direction and quantifiable results for early-stage startups and Fortune 100 organizations such as Bazaarvoice, Qlik, Rackspace, PayPal, and P\u0026G. Outside of the office, Suzin volunteers for causes close to her heart with her two children.","name":"Suzin Wold","slug":{"_type":"slug","current":"suzin-wold"}},"estimatedReadingTime":3,"featured_image":{"_type":"image","asset":{"_ref":"image-0434b713b7dcba1cdf98eff0eb50d7cb3d5fb008-1500x908-jpg","_type":"reference"}},"meta_description":"Sama has been recognized in Fast Company’s list of the Next Big Things in Tech for our work with our partners at Orbisk.","openGraphImage":null,"plaintextBody":"Our team at Sama is committed to building the technology solutions required to address our society’s pressing needs. From our social-impact business model and training programs that help individuals lift themselves out of poverty to partnerships with the organizations creating change across industries, we’ve always believed in accurate AI’s ability to change our world for the better.\n\n\n\nThat’s why we’re honored to announce today that we’ve been recognized in Fast Company’s first-ever list of the Next Big Things in Tech for our work with our partners at Orbisk.\n\n\n\nHighlighting the new technological breakthroughs that promise to help define the future of the industries they serve, Next Big Things In Tech finds the products, services, features, research, and other technology efforts having a positive impact on consumers, businesses, or society at large in the next five years.\n\n“The climate crisis is growing increasingly dire each day. Right now, between 8-10% of all greenhouse gas emissions are the direct result of food waste,” said Wendy Gonzalez, CEO at Sama. “As we work to find ways to be more environmentally conscious, we were inspired by the team at Orbisk’s vision for the future of food service. Together, we’re incredibly proud of the solution we’ve built and its impact in minimizing waste production.”\n\nEach year, 931 million tonnes of food end up in landfills, and of that, nearly 362 million tonnes come from the foodservice and retail industries. With Sama’s high-quality training data powering its algorithm, Orbisk’s fully automated food waste monitor allows hotels, restaurants, and cafés to reduce food waste by up to 70%. \n\nOrbisk’s solution works by scanning plates of food to detect and record what items are being thrown away automatically and their quantities. With this data, businesses can make smarter choices that help save money while reducing food waste. To ensure accuracy, Sama successfully labeled hundreds of thousands of images of food, rapidly iterating to reach consistent attribute tagging. This allows Orbisk to improve accuracy and build a robust product capable of accurately recognizing food of all kinds in all steps of the cooking process.\n\nTo date, our partnership has enabled businesses to reduce an average of 8,000lbs of food waste per year. In total, prototyping projects alone have saved over 220,400lbs to date, and we’re aiming to reach over 220,000,000lbs saved annually by 2025.\n\n“Fast Company is thrilled to highlight cutting-edge technologies that are solving real-world problems in unexpected ways. From climate change and public health crises to machine learning and security, these technologies will certainly have a profound impact on the future, and we’re honored to bring attention to them today,” says Stephanie Mehta, editor-in-chief of Fast Company.\n\nTogether, we look forward to helping reduce the rate of climate warming by mitigating food waste.\n\nLearn more about our partnership and its success here, or check out the full list of Fast Company's Next Big Things in Tech here.","seo_title":"Orbisk’s Sama-Powered Food Waste Solution Named to Fast Company’s First-Ever List of the Next Big Things in Tech","slug":{"_type":"slug","current":"fast-company-next-best-things-tech-2021"},"tags":[{"_key":"7hzpVymO","label":"Company News","value":"Company News"},{"_key":"7nlxNL7J","label":"Awards","value":"Awards"}],"title":"Orbisk’s Sama-Powered Food Waste Solution Named to Fast Company’s First-Ever List of the Next Big Things in Tech"},{"_createdAt":"2021-11-16T20:36:31Z","author":{"_id":"e8a1f3c6-0b07-4638-93d5-61a0924f32e3","avatar":{"_type":"image","asset":{"_ref":"image-2f8efc599a377bd580d0ae12f022a15e7e1acacd-632x676-webp","_type":"reference"}},"bio":"Amanda is the Content Marketing Manager at Sama, where each day she gets to learn about AI and how it’s transforming industry. She’s passionate about technology but periodically tries to get away from her computer to explore more analog things like collage and quilting.","name":"Amanda Durepos","slug":{"_type":"slug","current":"amanda-durepos"}},"estimatedReadingTime":7,"featured_image":{"_type":"image","asset":{"_ref":"image-ae9f62c210539de6ed2b60b71efa4be6b90021c6-1920x960-png","_type":"reference"}},"meta_description":"Learn how Volumental partnered with Sama to accurately label the datasets that fuel the computer vision technology for their mobile foot scanning app.","openGraphImage":null,"plaintextBody":"Volumental produces shoe recommendations for millions of shoppers by leveraging a combination of 3D foot scans, retail purchase data, and AI. Their team partnered with Sama to label the datasets that fuel the computer vision technology for their mobile foot scanning app — one piece of Volumental’s technology suite which empowers retailers and brands to create frictionless and more personalized experiences for their customers, both in-store and online.\n\nOrdering shoes online can feel like a gamble. How many of us have anticipated the arrival of a fresh pair of sneakers, only to have to promptly return them due to poor fit? According to one study, about 52% of us have experienced that disappointment.\n\nShoe size labels can be a poor indicator of actual fit, both across categories of footwear and even within the same brand. And with an estimated $428 billion in merchandise returned to retailers in 2020, there exists a huge opportunity for retailers and brands to do some damage control.\n\nFor Sweden-based Volumental, the answer isn’t to solve for consistency or standardization within the shoe manufacturing process, but rather to leverage AI to deliver more accurate fit recommendations: an industry segment they are dubbing FitTech™.\n\n\n\nAI can help deliver more delightful, personalized retail experiences\n\n\nFor many years, Volumental has been outfitting shoe retailers with in-store scanners which capture 3D scans of customers’ feet within seconds. These scans are cross-referenced with their extensive database of retail purchase data to deliver product recommendations to shoppers, based on accurate foot measurements and the purchase behavior of consumers with similar feet.\n\nThis technology empowers shoppers to make good buying decisions, but it also supplies retailers with data-driven insights to help them provide more personalized recommendations to consumers both in-store and online.\n\nArmed with millions of foot scans and a database of purchase behavior from customers of nearly 100 of the world’s top retailers and brands, Volumental then set out to tackle the challenge plaguing e-commerce stores and shoe-wearers everywhere: online returns due to poor fit.\n\n\n\n(Image source: Volumental website.)\n\n\n\nThe Volumental mobile app delivers the same benefits to consumers, but this time, from the comfort of their homes. The user experience is seamless: take a few photos of your feet from key angles and receive accurate foot measurements along with data-backed recommendations for shoes that are sure to fit like a glove.\n\nData labeling challenges for high-precision foot scanning\n\n\nThe Volumental mobile app user experience may be straightforward, but the challenge of developing this technology was not. To deliver a seamless experience in the app, Volumental had to solve a range of technical problems.\n\nWhile the LiDAR capabilities that come equipped in modern-day smartphones work well to make many AR experiences more accurate and realistic, they are not useful for foot scanning. Existing AR frameworks on the market did not provide the level of accuracy required for Volumental’s mobile foot scanner, so they set out to build their own proprietary models.\n\n\n\nThese images of Volumental employees’ feet show the difference between foot scans reconstructed with sensors native to modern smartphones (left) vs the level of precision required to train their own proprietary models: pixel-perfect masks (right). (Source.)\n\n\nTo obtain the high-accuracy segmented images they needed to power their mobile scanning, Volumental began the search for a data labeling partner. They knew that the dataset had to adhere to an extremely high standard of quality if their mobile solution was to deliver the seamless experience their users were accustomed to in-store.\n\nMikael Andersson, Sr Product Owner at Volumental explained why this requirement was imperative:\n\n“For our mobile app, we needed extremely precise segmented data because we knew that every missed pixel would easily add up to millimeters of lost accuracy.” \n\nIn addition to a high standard of label quality, the data labeling project would also require annotators to know how to handle edge cases such as shadows, low-contrast light, and occlusions. These exceptions needed to be handled with consistency if the training data was to make Volumental’s algorithms behave predictably. For these edge cases, and to ensure that their solution would be able to scale without compromising quality, Volumental needed tight feedback loops to keep their models high-performing. \n\nTo meet all these requirements, Volumental partnered with Sama to deliver high-precision segmented labels for their datasets. Mikael explains:\n\n“Having worked with different cloud providers where the staff doing the actual work was always very hidden from us, we appreciated the transparency Sama gave us. They were communicative and very easy to work with from data collection to project management.”\n\nDelivering a delightful and uncompromisingly accurate experience to their users was important to Volumental, but so was social sustainability. As CMO Brent Hollowell explained, building a mass-market experience that is representative of the world must include diverse and representative datasets:\n\n“At the core of our interest in social sustainability are inclusivity and cultural diversity. Diversity of data is the strength of our AI-powered Fit Engine and it's also what helps us succeed as a company.”\n\nPartnering with Sama assured Volumental that the diversity of their datasets would be accurately represented in their models: to deliver hyper-personalized experiences to delight their users across the globe.\n\n\n\nBetter labels lead to better business outcomes\n\n\nIn part thanks to accurately labeled data, Volumental is creating more delightful omnichannel shopping experiences for consumers. Accurate 3D foot scans are just one piece of the puzzle: combined with their extensive database of purchase behavior and proprietary ML algorithms, Volumental can deliver hyper-personalized recommendations to consumers.\n\nThese recommendations don’t only provide better shopping experiences for users, they remove friction from the buying process and result in fewer online returns. The end result? Happy, loyal customers and ultimately, more revenue for retailers and brands — all thanks to AI-powered fit recommendations.\n\n","seo_title":"Accurate Data Labeling Powers the Volumental Shoe Recommendation App — Helping Retailers Convert Mobile Customers","slug":{"_type":"slug","current":"volumental-shoe-sizing-app"},"tags":[{"_key":"91OCKFjv","label":"Case Studies","value":"Case Studies"}],"title":"Accurate Data Labeling Powers the Volumental Shoe Recommendation App — Helping Retailers Convert Mobile Customers"},{"_createdAt":"2021-11-10T15:20:55Z","author":{"_id":"e8a1f3c6-0b07-4638-93d5-61a0924f32e3","avatar":{"_type":"image","asset":{"_ref":"image-2f8efc599a377bd580d0ae12f022a15e7e1acacd-632x676-webp","_type":"reference"}},"bio":"Amanda is the Content Marketing Manager at Sama, where each day she gets to learn about AI and how it’s transforming industry. She’s passionate about technology but periodically tries to get away from her computer to explore more analog things like collage and quilting.","name":"Amanda Durepos","slug":{"_type":"slug","current":"amanda-durepos"}},"estimatedReadingTime":9,"featured_image":{"_type":"image","asset":{"_ref":"image-e7a31b38bf862680df4a21015cebc9ab8232c733-5472x3648-jpg","_type":"reference"}},"meta_description":"Last week, Sama secured Series B, cementing the viability of purpose-driven companies—and hopefully inspiring others to pursue purpose and profitability. ","openGraphImage":{"url":"https://cdn.sanity.io/images/76e3r62u/production/e7a31b38bf862680df4a21015cebc9ab8232c733-5472x3648.jpg"},"plaintextBody":"Last week, Sama secured $70M in Series B funding, the second round of equity financing for the company. This marks the transition from solidifying product-market fit to truly taking the business and impact to scale.\n\nOn the road to Series B, Sama has received many recognitions for its innovative technology and impact sourcing model, including inclusion on the Forbes AI 50 and Fast Company’s World Changing Ideas lists.\n\nThat said, getting to this milestone has not been straightforward. The team had to overcome the “valley of death” and bust many myths along the way. One by one. A few of them are highlighted below, cementing the viability of purpose-driven companies and hopefully inspiring others to bust myths on their path to purpose and profitability.\n\nMyth #1: Impact requires a trade-off between purpose and profit\n\nAlthough impact investing has become more mainstream over the past decade, the notion that there is a trade-off between impact and financial return is still very much alive — despite academic evidence pointing to the opposite. While purpose-driven companies may integrate impact in their business models in different ways, Sama’s Series B raise solidifies the fact that there is no trade-off between purpose and profit as you grow, when impact is inherent to your value proposition and product. \n\nThe company partners with big corporations—Google, NVIDIA, Walmart—to provide them with quality data to train their machine learning models. Sama provides high-quality annotated text, images, and videos for a range of use-cases, powering robot-assisted surgery, autonomous vehicles, personalized online shopping experiences, and more. Sama hires over 90% of its workforce from low-income backgrounds and marginalized populations in Kenya and Uganda, training them and granting them employment in the digital economy. What’s more, Sama is an anti-ghost work company. Each of its employees is hired ethically and receives fair compensation and benefits. \n\nFor over 10 years, Sama has partnered with leading global brands. Their stellar commercial performance has been achieved as they continue to scale opportunities for underserved individuals through the digital economy. Sama was one of the first AI companies to become B-Corp certified and has helped over 56,000 people lift themselves out of poverty. Its training and employment program was recently validated by an MIT-led Randomized Controlled Trial, confirming the effectiveness of its impact sourcing model by demonstrating that individuals that received both training and inclusion in Sama’s hiring pool exhibited lower unemployment rates and higher average monthly earnings.\n\n\"Sama’s openness to evaluate the impact of their programs with the rigor of a randomized control trial was refreshing and shows that they’re a leader in the movement toward ethical practices within the AI industry.\"\n-David Atkin, Professor of Economics, MIT\n\nMyth #2: Diversity is a pipeline problem\n\nWe have all seen the data in one shape or form: white men get the lion’s share of investor funding. Despite the movements calling for diversity, women-led startups account for just 2.2% (https://pitchbook.com/news/articles/the-vc-female-founders-dashboard) of the $150 billion invested in companies by VCs annually. Many have argued that the lack of diversity is caused by a pipeline problem, that there simply are not enough qualified investment opportunities – especially as it regards tech businesses. With the Series B raise, Sama paves the way for busting the myth once and for all. \n\nSama is a female-founded and led tech company that is deploying an impact sourcing model creating opportunities for marginalized communities, with over half of Sama agents intentionally being women. Founded by late Laila Janah and now led by CEO Wendy Gonzalez, the company has achieved responsible hyper-growth, and consequently, for the second year, a spot on the Inc. 5000 list as one of America’s fastest-growing private companies. The data speaks for itself.\n\nSama’s Series B raise represents the largest round for a female-led AI infrastructure in history. The round is led by CDPQ’s Equity253 fund that targets companies that leverage diversity and inclusion as a vector of development. The fund is put in motion because studies clearly validate that diversity in companies fosters better decision-making and has a positive impact on innovation, risk management, productivity, and financial performance. \n\n\"At the BESTSELLER FOUNDATION the rationale for investing in diversity is also clear – not only for profit and for purpose, but also to ensure structural change through the positive ripple effects that transcends generations. Sama embodies such rationale by improving employment and income for those with the greatest barriers to work and changing the narrative for their dependents\"\n- Anne Cathrine Garde, Investment Manager and Head of Scaleups.\n\nMyth #3: Impact does not fit the venture capital model\n\nSama was started as a non-profit organization because no one believed it could ever be a business. When late-founder Laila Janah pitched venture capitalists on Sandy Hill Road, the impact sourcing model was disregarded as it did not fit the typical parameters of hyper-growth and capital efficiency. On the opposite side of the spectrum, grant givers did not believe that impact sourcing could meet the basic needs of the marginalized communities. Sama proved both groups wrong. The company was ahead of its time, but luckily the landscape has changed a lot since then, and with the Series B raise, Sama is showcasing that impact is fit for staged financing.\n\n\n\nSince 2015, with Wendy Gonzalez on board, the company has focused on building a repeatable and scalable business model. From 2016 and onwards, Sama sustained itself on earned revenues and became a profitable nonprofit. With the BESTSELLER FOUNDATION investment in 2017, the Kenyan subsidiary was transformed from a non-profit to a for-profit company.\n\nJannek Hagen, Managing Director was a part of the team driving the investment decision:\n\n\"We invested in Sama Kenya back in 2017 as the first investor. Since then, we have witnessed how the team has transformed from a not-for-profit to a full-fledged, for-profit AI technology company\".\n\nThe investment set the precedent for Sama at group level to be transformed into a for-profit to attract further financing, talent, and expertise the following year. In 2019, Sama completed their USD 14.8M Series A funding with venture capital Ridge Ventures leading the round supported by Social Impact Ventures, Bluecrest Limited Capital, Salesforce Ventures, and BESTSELLER FOUNDATION.\n\nToday, Sama has taken the next step in its journey with the Series B raise that will take its purpose and profitability to new scales. In 2021, the landscape of impact investing looks different with the proliferation of many asset classes that are embracing an impact lens, covering the whole investment lifecycle. The time for launching an impact business is ripe and a big recognition goes to founders like Leila Janah, who ahead of her time proved the viability of purpose-driven businesses.\n\nLet us bust more myths together, one by one!\n\nHow are you busting the myths of impact? In a Sub-Saharan African context, where Sama and the BESTSELLER FOUNDATION have a strong presence, the journey towards impact equilibrium is twofold. Let us jointly normalize the concept of purpose-driven businesses while highlighting and measuring the impact that is happening on the ground. Our goal is to continue to ensure that impact is not diluted or misrepresented but used as a competitive advantage.\n\n**This post was jointly authored by Anne Cathrine Garde, BESTSELLER FOUNDATION and Amanda Durepos, Sama.**\n\n---\n\nAbout BESTSELLER FOUNDATION: We support entrepreneurs and invest in businesses that work for supporting the wellbeing of our natural world, for creating better jobs and opportunity, for sustainable growth. Our work is made possible through the support of BESTSELLER, an international, family-owned fashion group. Right now, our main priority is to help tackle one of the biggest challenges of our time - how to reduce waste, how to reuse and recycle valuable resources. And how to find economically viable opportunity in doing so. These efforts are currently focused on providing early-stage capital for (aspiring) circular economy ventures in Sub-Saharan Africa. For more information, visit www.bestseller.org.","seo_title":"Getting to Series B: How Sama is Proving Impact is a Strategy for Business Success","slug":{"_type":"slug","current":"impact-business"},"tags":[{"_key":"K6VnIonn","label":"Company News","value":"Company News"},{"_key":"Y81NfQzJ","label":"Impact","value":"Impact"}],"title":"Getting to Series B: How Sama is Proving Impact is a Strategy for Business Success"},{"_createdAt":"2021-11-04T14:08:29Z","author":{"_id":"a009d418-aa96-47ac-a73a-fd2cd52c79d9","avatar":{"_type":"image","asset":{"_ref":"image-e0d717f753ba4876a6b0dbf9f125cf6c3d27e545-500x500-webp","_type":"reference"}},"bio":"Wendy Gonzalez is an executive passionate about building high-performing, high-functioning teams that develop and scale innovative, impactful technology. With two decades of managerial and technology leadership experience for companies including EY, Capgemini, Cycle30 (acquired by Arrow Electronics) and General Communications Inc, Gonzalez is currently the CEO of Sama, the provider of accurate data for ambitious AI, used by leading technology companies such as Walmart, Google, Nvidia and Getty. Before taking on her role as CEO, Gonzalez spent 5 years at Sama as COO, and is an active Board Member of the Leila Janah Foundation.","name":"Wendy Gonzalez","slug":{"_type":"slug","current":"wendy-gonzalez"}},"estimatedReadingTime":4,"featured_image":{"_type":"image","asset":{"_ref":"image-5129846575d1f7d8aa08a3784a1ff9eced8ba307-1000x500-png","_type":"reference"}},"meta_description":"Sama has raised $70M Series B funding to build the first end-to-end AI platform that enables teams to manage the complete AI lifecycle.","openGraphImage":{"url":"https://cdn.sanity.io/images/76e3r62u/production/5129846575d1f7d8aa08a3784a1ff9eced8ba307-1000x500.png"},"plaintextBody":"Sama has raised $70M of Series B funding to build the first end-to-end AI platform that enables teams to manage the complete AI lifecycle from start to finish. We’re partnering with some of the world’s top venture capital firms: our round is led by Caisse de dépôt et placement du Québec (CDPQ), a global investment group, with participation from First Ascent Ventures and Vistara Growth, in addition to all existing investors. Read more here.\n\n\n\n\nAt Sama, we’re shaping a future of AI that’s more accurate, fair, and ethical. Since Day 1, we’ve been committed to building technology that has a positive impact — across industries, but also across the globe.\n\nThis is no small task, and we’ve been busy since announcing our Series A funding in 2019. \n\nWe have opened offices in Kampala, Uganda, and Montreal, and Samasource became Sama. We were honored to win the 2020 AI Breakthrough Award for Best Image Processing Solution and this year were included in lists such as Forbes AI 50, Inc. 5000 Fastest-Growing Private Companies, 2021 AITechAward in Machine Learning Platforms category, and Fast Company’s World Changing Ideas.\n\n\n\n\nSama became one of the first AI companies to receive B Corp Certification and was recognized on the Best for the World™ 2021 list for BLabs Workers category. Our training and employment programs continue to help individuals lift themselves out of poverty — 56,000 individuals to date — with our 3-year Randomized Control Trial by MIT validating our long-term impact on employment rates and earnings.\n\n\n\n\nUp to now, we’ve annotated over 1B image points on our platform (an average of 1M tasks completed per month) for industry-leading companies by the likes of Google, Walmart, and NVIDIA. And we did all this while achieving up to 99.9% data accuracy for our clients, versus an industry average of 94%.\n\nWhat’s next?\n\n\nWorking with these exceptional clients for over a decade has highlighted an enormous area of opportunity, both for the future of Sama and for the AI industry as a whole. Enterprise AI teams have adopted a growing number of niche tools to support their end-to-end AI development cycle, and a lot of time (and accuracy) is lost building the infrastructure and pipeline required to integrate it all.\n\nAt Sama, we believe there’s an opportunity to bridge this gap, and to do so responsibly.\n\nToday, I’m thrilled to share that we’ve raised $70M of Series B funding to build the first end-to-end AI platform that enables teams to manage the complete AI lifecycle from start to finish. To achieve this, we’re partnering with some of the world’s top venture capital firms: our round is led by CDPQ with participation from First Ascent Ventures and Vistara Capital Partners, in addition to all existing investors.\n\nWith this funding and the support of our investors and community, we’re more committed than ever to building AI that delivers a positive impact: both for our clients who rely on us for high-quality data for their models, and for the lives of the professionals at the core of ethical AI supply chains.\n\nI want to thank all our investors for their trust in our mission, and for their commitment to changing the world for good. Thank you to our clients for entrusting us with delivering the high-quality data you need to fuel innovation. And an immense thanks to the Sama team and community for years of hard work, heart, and dedication.\n\nThank you — each and every one of you — for being part of this journey with us. We wouldn’t be where we are today without you. Let’s continue to shape a future of AI that’s more accurate and fair, together.\n\n\n\nWendy","seo_title":"Sama Raises $70M Series B to Build a More Accurate, More Ethical, End-to-End AI Development Pipeline","slug":{"_type":"slug","current":"series-b"},"tags":[{"_key":"oEH9XOvc","label":"Company News","value":"Company News"}],"title":"Sama Raises $70M Series B to Build a More Accurate, More Ethical, End-to-End AI Development Pipeline"},{"_createdAt":"2021-10-25T18:45:56Z","author":{"_id":"70f24746-bdb3-4801-adfd-17508d02ae50","avatar":{"_type":"image","asset":{"_ref":"image-9a5184335ade812b332047f70963b6e72a885c67-1194x1284-webp","_type":"reference"}},"bio":"Rob hosts \u0026 produces Sama's podcast, How AI Happens. How AI Happens is a podcast featuring experts and practitioners explaining their work at the cutting edge of Artificial Intelligence. Tune in to hear AI Researchers, Data Scientists, ML Engineers, and the leaders of today’s most exciting AI companies explain the newest and most challenging facets of their field.","name":"Rob Stevenson","slug":{"_type":"slug","current":"rob-stevenson"}},"estimatedReadingTime":2,"featured_image":{"_type":"image","asset":{"_ref":"image-1502043edcf701cb4267cb2f6d7c6edf8e0e0cad-2500x1313-png","_type":"reference"}},"meta_description":"Embodied Chief Technology Officer Stefan Scherer explains how conversational AI, few-shot learning, and lean robotic design brings Moxie the Robot to life.","openGraphImage":null,"plaintextBody":"When you were a child, do you remember learning what it means to be kind?\n\nWhat about reading sadness in someone’s face, understanding the anger you felt, or respecting personal space?\n\nIf you don’t remember learning about any of these human moments, then Moxie, a conversational AI robot built by Embodied, is the android friend you never knew your childhood self needed.\n\nMoxie is designed to teach children social and emotional maturity, both through human-to-robot conversation as well as various missions that send the child out into the world to learn about kindness and friendship.\n\nTo learn more about what makes Moxie tick, we hosted Stefan Scherer, Chief Technology Officer at Embodied, on the latest episode of our How AI Happens podcast. Stefan explains the language processing happening within Moxie, and how the team was able to generate reliable conversational ability through lean few-shot learning.\n\nMoxie’s ability to shift between dynamic responsive conversation and scripted programmatic content raises fascinating questions about conversational AI: exactly how much responsiveness is required to create the feeling of a 1:1 conversation? And how can technologists draw a circle around a concept such as “kindness” in such a way that enables it to be shared between a child and an AI?\n\nStefan explains all this and more in the full episode. And, of course, we hear from Moxie, too.\n\n\n\n","seo_title":"New Podcast Episode: Moxie the Conversational AI Robot Teaches Children Kindness","slug":{"_type":"slug","current":"moxie-the-robot-teaches-children-kindness-conversational-ai-child-development"},"tags":[{"_key":"0CNsp60Q","label":"Podcast","value":"Podcast"}],"title":"New Podcast Episode: Moxie the Conversational AI Robot Teaches Children Kindness"},{"_createdAt":"2021-10-13T13:21:47Z","author":{"_id":"e8a1f3c6-0b07-4638-93d5-61a0924f32e3","avatar":{"_type":"image","asset":{"_ref":"image-2f8efc599a377bd580d0ae12f022a15e7e1acacd-632x676-webp","_type":"reference"}},"bio":"Amanda is the Content Marketing Manager at Sama, where each day she gets to learn about AI and how it’s transforming industry. She’s passionate about technology but periodically tries to get away from her computer to explore more analog things like collage and quilting.","name":"Amanda Durepos","slug":{"_type":"slug","current":"amanda-durepos"}},"estimatedReadingTime":8,"featured_image":{"_type":"image","asset":{"_ref":"image-5b774da87e817474525a389529a9ea674aa54a11-4167x2084-png","_type":"reference"}},"meta_description":"Find out how accurately labeled data is helping PolyPerception provide material recovery facilities with better visibility into their waste streams.","openGraphImage":null,"plaintextBody":"PolyPerception provides an AI-powered waste management platform to plastics and material recovery facilities. Their technology tracks each individual piece of waste at an object level. This gives the facilities visibility into their waste streams so they can operate more efficiently and responsibly. The team has partnered with Sama to help fuel this technology, to further their mission of empowering stakeholders across waste management—from recyclers to municipalities to legislators—to make more sustainable decisions about waste and its impact on the environment.\n\nThe challenge of visibility in the waste management industry\n\n\nAs the volume of discarded waste continues to grow globally each year, so has the pressure on our landfills, resources, and environment. With waste disposal technology, infrastructure, and regulators struggling to catch up, one significant barrier blocks the way: visibility.\n\nOn average, 8 tonnes of waste passes through a waste sorting facility every hour. If this sounds like an almost inconceivable amount of waste, that’s because it is. For the operators of sorting centers, understanding and documenting the waste passing through each day remains a major challenge, forcing the waste management industry to operate with a lot of blind spots. \n\nTo circumvent this and better understand the makeup of the waste passing through, many facilities implement a sampling process, but sample sizes are typically small. Though 8 tonnes of waste passes through a facility every hour, sampling is often only done once a week. Even representative sample sets can be skewed by the subjectivity of human bias, making consistency a challenge.\n\nThese factors lead to datasets that don’t often show the full picture, forcing waste management facilities to largely operate on human intuition; a less-than-ideal approach to the decision-making required to meet federal regulations and to cut operational costs.\n\nChanging packaging trends and regulations\n\n\nThe sheer volume of waste passing through a facility is not the only obstacle for sorting center operators. In the world of waste management, tons of other parameters are constantly changing, transforming the efficiency and economics of the process.\n\nFor starters, brands often change their packaging in ways that impact how it’s sorted and where that packaging finally ends up. For example, Heinz is currently using a multi-layer PET bottle with a non-recyclable barrier, but this is soon slated to change in Europe. These changes often happen suddenly and without warning, giving facilities little time to react to make sure these bottles now end up in the appropriate output stream — and therefore at the right recycling facility.\n\nPackaging also varies from region to region. If a facility onboards a new source of waste, they may not understand regional differences, and their machines may not be optimized to sort materials accordingly. Slight margins of error are amplified when you consider the number of sorting steps and the sheer volumes processed.\n\nKeeping up with shifting packaging trends is only compounded by regulations that are also constantly in flux, and differ from one country to the next. All these variables from the outside require sorting facilities to constantly optimize their processes and plan strategically, but this is hard without quantifiable data.\n\n\nAccurate data labeling plays a crucial role\n\n\nThis is where PolyPerception comes in: to give operators in facilities continuous visibility into their waste flow systems by adding cameras to monitor the end-to-end process. This means tracking each piece of waste at an object level. Traceability and transparency allow these facilities to operate more efficiently, bringing them better commercial terms and preparing them to tackle new legislation and packaging trends.\n\nIn order to deliver actionable insights and quantitative data to their clients, PolyPerception set out to build a robust multi-object tracking model, but quickly found that the accurate labeling of data would play a key role.\n\nCrucially, they needed a labeling solution that could accurately label waste objects that travel quickly on conveyor belts in facilities with less-than-ideal lighting conditions. But they also needed a partner who could effectively annotate millions of waste objects while accounting for a wide range of packaging types, materials, processing speeds, and processing conditions.\n\nRafael Hautekiet, CEO of PolyPerception, was delighted by how feedback loops with Sama’s managed workforce of annotators resulted in better quality data for their model – annotations with an average Quality Score of 99%:\n\n“The team quickly learned to distinguish between waste objects, which differ greatly from region to region. Communication channels remained open for feedback, and we had a continuous open discussion about how the efforts were progressing.”\n\nThis line of open communication was important to PolyPerception when they were evaluating different data labeling partners. They wanted to feel like annotators were an extension of their own team, both to maintain visibility into the labeling process and to ensure that the many nuances of their labeling needs were being met.\n\nWhat more reliable, quantifiable data can do for waste management\n\n\nFor PolyPerception, accurate data in waste management has cascading effects for society – a rising tide to lift all boats. With more reliable data:\n\nRecyclers can better understand the composition of their input waste streams, allowing them to reach higher recycling rates, meet federal and state regulations while also cutting costs. \n\nMunicipalities and government agencies can confidently establish data-backed regulations that will have positive lasting impacts.\n\nConsumers can be better informed of how their actions have a direct and measurable impact on the world around us.\n\nThe economic, social, and environmental benefits of the above are too numerous to list, but they are at the core of PolyPerception’s longer-term vision: of empowering stakeholders across waste management to make more impactful decisions, to move the world toward a circular future.\n\nTo PolyPerception COO Parshva Mehta, accurately labeled data plays a small but important part in this ambitious undertaking. This is why it was important for PolyPerception to do data labeling a different way: to work with a provider whose values aligned with theirs, and with their mission to make the world a better place. In Parshva’s words:\n\n“There’s a possibility to make an impact on legislation and on the environment, but not without accurately labeled data. We appreciate that Sama started out by disrupting the status quo and by having a strong social mission. We really resonate with this.”\n\n\n\n\n","seo_title":"How More Accurate Data Labeling is Helping PolyPerception Advocate for Responsible Waste Management","slug":{"_type":"slug","current":"/polyperception-case-study"},"tags":[{"_key":"qRJxxe50","label":"Case Studies","value":"Case Studies"}],"title":"How More Accurate Data Labeling is Helping PolyPerception Advocate for Responsible Waste Management"},{"_createdAt":"2021-08-17T19:01:34Z","author":{"_id":"26fb3cc4-608d-40e1-bb4d-955bceda232a","avatar":{"_type":"image","asset":{"_ref":"image-f1fd7fbcc4633299cdbedddba22cb44e24f17317-518x518-svg","_type":"reference"}},"bio":"From self-driving cars to smart hardware, Sama fuels AI. Founded over a decade ago, we’re experts in image, video and sensor data annotation and validation for machine learning algorithms in industries including automotive, navigation, AR/VR, biotech, agriculture, manufacturing, and e-commerce. Our staff are driven by a mission to expand opportunity for low-income people through the digital economy, and our social business model has helped over 50,000 people lift themselves out of poverty.","name":"Sama Team","slug":{"_type":"slug","current":"sama-team"}},"estimatedReadingTime":2,"featured_image":{"_type":"image","asset":{"_ref":"image-0434b713b7dcba1cdf98eff0eb50d7cb3d5fb008-1500x908-jpg","_type":"reference"}},"meta_description":"We’re honored to have made the Inc. 5000 List of America’s Fastest-Growing Private Companies… for the second year in a row.","openGraphImage":null,"plaintextBody":"We have some exciting news to share:\n\nWe’re honored to have made the Inc. 5000 List of America’s Fastest-Growing Private Companies… for the second year in a row!\n\nCompanies on the 2021 Inc. 5000 were ranked according to percentage revenue growth from 2017 to 2020 – a badge we’re especially proud to wear considering the challenges that 2020 brought businesses everywhere.\n\nNot only have the companies on the 2021 Inc. 5000 been very competitive within their markets, but this year’s list also proved especially resilient and flexible given 2020’s unprecedented challenges. Among the 5,000, the average median three-year growth rate soared to 543 percent, and median revenue reached $11.1 million. Together, these companies have added more than 610,000 jobs over the past three years.\n\nAs Scott Omelianuk, editor-in-chief of Inc. put it:\n\n“Building one of the fastest-growing companies in America in any year is a remarkable achievement. Building one in the crisis we’ve lived through is just plain amazing. This kind of accomplishment comes with hard work, smart pivots, great leadership, and the help of a whole lot of people.”\n\nOur CEO Wendy Gonzalez also shared some words about the recognition:\n\n“Last year was incredibly challenging for everyone and this recognition is a testament to our ability to not only endure through tremendous adversity as a team but come out stronger. I am incredibly proud of the work we’ve continued to accomplish, including the impact we’ve had as an industry leader in AI and our commitment to the ethical supply chain.”\n\nWe're thrilled to share the news, and even more excited about continuing our journey in 2021 and beyond with renewed ambitions.\n\nComplete results of the Inc. 5000, including company profiles and an interactive database that can be sorted by industry, region, and other criteria, can be found here.","seo_title":"Sama Made the Inc. 5000 List (For the Second Year in a Row!)","slug":{"_type":"slug","current":"inc-5000-2021"},"tags":[{"_key":"i2eTixTT","label":"Company News","value":"Company News"}],"title":"Sama Made the Inc. 5000 List (For the Second Year in a Row!)"},{"_createdAt":"2021-08-10T18:08:48Z","author":{"_id":"e8a1f3c6-0b07-4638-93d5-61a0924f32e3","avatar":{"_type":"image","asset":{"_ref":"image-2f8efc599a377bd580d0ae12f022a15e7e1acacd-632x676-webp","_type":"reference"}},"bio":"Amanda is the Content Marketing Manager at Sama, where each day she gets to learn about AI and how it’s transforming industry. She’s passionate about technology but periodically tries to get away from her computer to explore more analog things like collage and quilting.","name":"Amanda Durepos","slug":{"_type":"slug","current":"amanda-durepos"}},"estimatedReadingTime":45,"featured_image":{"_type":"image","asset":{"_ref":"image-1502043edcf701cb4267cb2f6d7c6edf8e0e0cad-2500x1313-png","_type":"reference"}},"meta_description":"Laurence Moroney, Lead AI Advocate at Google, joined our podcast to talk about how his team is democratizing access to AI's development.","openGraphImage":null,"plaintextBody":"Many of the exciting advances in AI have resulted from well-funded companies and research departments, stocked with high-performance computers and every shiny toy the AI practitioner could want. But according to Laurence Moroney, Lead AI Advocate at Google, that’s not the only way to develop artificial intelligence.\n\n\n\nSo these students got together and they realized... if they take a picture of the sky, they have data. If they measure the air quality on the sensor, they have a label.\n\nLaurence joined our podcast, How AI Happens, to share examples of exciting advances in AI that are happening all over the world, many with no more than basic mobile devices. Laurence and his team have made it their mission to evangelize the opportunity of AI and work towards democratizing access to the technology’s development, a mission they accomplish via MooCs, YouTube, and a series of books on AI development. \n\nIn this episode, Laurence shares the nature of AI hype cycles, how AI practitioners can navigate them within their own organizations, and some of the amazing opportunities coming into play when access to AI \u0026 ML is made global. You can stream the full episode below, tune in via your favorite podcasting app, or read the whole transcript below.\n\n(audio embed)\n\nTranscript:\n\n0:00:00.0 Laurence Moroney: And then when they fed this back through an intention mechanism, they realized they didn't build a camouflage detector, they built a cloudy sky detector.\n\n0:00:12.2 Rob Stevenson: Welcome to 'How AI Happens', a podcast where experts explain their work at the cutting edge of artificial intelligence. You'll hear from AI researchers, data scientists and machine learning engineers, as they get technical about the most exciting developments in their field and the challenges they're facing along the way. I'm your host, Rob Stevenson, and we are about to learn how AI happens.\n\n0:00:42.0 RS: You don't have to be an AI expert to be skeptical about all the hype surrounding artificial intelligence and machine learning. Every company claims they have it, every sales deck mentions it, and worse, the media act as if the rise of the machines is happening sometime in late 2022. But amidst this hype cycle, those in the know understand the opportunity has never been greater. Enter Laurence Moroney. He's an industry veteran who has authored several books on AI development and even advises British Parliament on their AI approach. His mission in his role at Google is to evangelize the opportunity of AI and work towards democratizing access to the development of this technology.\n\n0:01:25.3 LM: I'm an AI lead at Google, so I lead the developer advocacy team, and our job is really to help inform and inspire the world around machine learning, artificial intelligence, deep learning and all that good stuff. So working with developers, working with communities, universities, all of those kind of folks to really help scale out the message and the opportunity that's there with AI.\n\n0:01:47.4 RS: Laurence joined the podcast to discuss the nature of AI hype cycles, how AI practitioners can navigate those cycles within their own organizations, and some of the amazing opportunities coming into play when access to AI and ML is made global.\n\n0:02:01.8 LM: As for my background, I was doing developer advocacy for a few years prior to Google, at places like Microsoft, a wonderful start-up in Israel, called Main Soft, and at Reuters, the news agency, kind of doing an internal advocacy role there, and then prior to that, the typical software engineer, all of those kind of things. Although my background at school was actually physics, my degree was in physics, but I came to the realization that nobody hires physicists, or very few people hire them, and I guess I wasn't good enough a physicist to be hired, so I ended up in this wonderful field instead.\n\n0:02:37.9 RS: It's interesting, you're the second individual I've spoken to, who got their start in physics and now have a career in AI. Is that a natural progression? What do you think is the link there?\n\n0:02:48.2 LM: I honestly... I don't think there is a natural progression, it's probably just a happy coincidence and maybe you're over-fitting in your audience. Sorry, AI joke there. For me, my path to AI actually came... It was really interesting that... 'cause when I first graduated college as a physicist, and it was in the UK, and it was in the middle of the worst recession that they had had since World War II. The current one, obviously, 'cause of COVID, is even worse. But back then, this was a pretty bad one, there was no kind of jobs or anything. And the government launched an initiative in 1992, the UK government, that they were gonna put together a cohort of 20 people to become AI specialists who could maybe be form the backbone of trying to help industry through AI and all of that kind of thing. And they needed people who were smart, but unemployed, and I at least fit one of those criteria, and I was unemployed, but we kinda did this battery of tests, it was like these kind of strange movies kind of thing, and I was accepted into the cohorts, which was really, really cool. And then I guess I got bitten by the AI bug then.\n\n0:03:53.7 LM: But in 1992, trying to do any kind of AI program was intensely difficult, it really didn't have any practical use. We were learning the languages like Prolog and LISP, and there was no industrial use for them, but there was some really fun academic stuff that you could do. But in the end, the program failed completely, but the potential was there, and I gotta give credit to the UK government, of figuring this out back in 1992. They were a little bit early, but it was really cool that they did it. It's funny that recently, in the last couple of years, I've been doing briefs to the UK governments, around AI, and I was like, \"Hey, do you know about that program?\" And of course, the MPs there, they're all long gone, the ones that did it. And the current ones were like, \"Did we really do that? That's awesome.\" I guess that's what got me bit by the bug and that led me down a career of programming and software engineering, to get me where I am today.\n\n0:04:45.9 RS: I'm interested to hear that you are spreading this message of the opportunity of AI, but then you also see all of these companies who are sort of saying they have AI or using AI in their messaging. Is there a gap of actual technology there? What is the difference between the reality of the technology and maybe the hype surrounding it?\n\n0:05:04.7 LM: Yeah, it's a great question. And by the fact that there's so many people doing this and waving around the AI magic pixie dust, hoping for customers or VC funding, that... If nothing else, that is a signal that this technology does have legs. The question is, does it get lost in the hype cycle or do we bust out of the hype cycle and start doing something really interesting? I always like to talk about, there's the... Gartner has this hype cycle curve, where you start with the peak of inflated expectations, and then you drop into something called the trough of disillusionment, and then once you're in the through of disillusionment, that's when you can really understand what the technology is and then you start climbing up through the plateau of productivity. And the kind of behavior that you're talking about just means that we're kind of on the wrong side at the moment, of this peak of inflated expectations. Part of what I'd like to describe my job as, is to do some quantum tunneling through that peak and end up in the trough of disillusionment. So I'm a professional disillusioner.\n\n0:06:02.9 LM: And then once you get into that and you understand what the technology actually is and what it does, then you can start being really useful with it. Obviously, you can look to the past to be able to predict the future. And in my career, there have been two massive tectonic shifts in computing. The first was really the widespread advent of the web and internet technology. The second was the smartphone. And if you think about, exactly the same thing happened in both those cases. I'll talk about the smartphone, which is the more recent one. So the hype cycle at the time was like, \"Throw away your desktop PC, throw away your laptop. You'll be able to do everything on your phone.\" And it's like, \"Forget about Office Suites, forget about programming environments, all of these kind of things. You're gonna get your phone, you're gonna plug it into a station on your desk and a big monitor will magically appear and it'll change how work is done.\" Well, that didn't happen. That to me, was a great example of the hype around the smartphone. But the smart phone still was a massive revolutionary technology that created a tectonic shift in the industry. I saw a stat, the largest creator of jobs in Western Europe during COVID, was the smartphone ecosystem. So not just people building smartphone applications, but people using them, and all of the stuff around that, like delivery services and all that type of stuff.\n\n0:07:23.8 LM: So we could see, that revolution started in 2007, and even now, 14 years later, the economy is benefiting greatly from it. The web revolution, the same thing, there was a whole ton of hype around the web, every shop in existence will go out of business, libraries will close. There was disruption and there were changes as a result of the web, but of course there was, I believe, an overall net gain. So when you start seeing these kind of things like when the hype first came in, but the people who were able to see through the hype and to be able to do something reasonable and productive when they fell into the trough of disillusionment, created whole new industries. Google came out as a result of the web, Amazon, Facebook, the Apple are the highest market cap company in the world right now, and that came as a result of the smartphone revolution. So there's so much that can happen when you can understand the actual limitations, start building to them and then rise up through that plateau of productivity as it's called in the Gartner Hype Cycle.\n\n0:08:28.1 LM: And that's really what I'm here to do, that's my role at Google, is to help people who are technically savvy to understand, \"Here's the possibility of things that you can do. Here's what you need to communicate within your business,\" and so when your product managers or when your CEO is wanting to wave that AI magic pixie dust, that kind of stuff, then it's the case, well, you can be the person who's got the expertise, who's able to say, \"I know this domain, and here's where AI can be used in this domain for real.\" And it might be nice to attract attention through marketing or through VC, but when you build a real product and you start building a real market around that, that's when the business can take off.\n\n0:09:09.7 RS: So if I'm an AI practitioner and I am contending with the hype around AI, or the example you gave of the CEO who's white boarding, \"Can we do this with AI?\" How can I level set expectations? There seems to be this little bit of education necessary, to make sure that people are steeped in reality when it comes to, \"What can this technology do? And what can you reasonably expect within your organization?\"\n\n0:09:34.5 LM: Yeah, I think effective communication is the number one tool, managing upwards like that is the number one tool. I've had a number of those conversations with folks who just thought that they can wave their arms and say, \"AI,\" and then find a programmer who could build the AI for them as they envisage it. But then to kinda just talk them through, \"Well, this is how it actually works, this is what it actually is. And if you wanna reach these goals, here's the kind of work that you would need to do, to be able to reach them.\" And often, it's setting lower goals and having a plan to be able to reach those lower goals and then use that as a plateau to go further and further and further. And I find in general, like CEO speak or CXO speak, they like that, instead of a yes person going, \"Yes, we can do whatever your vision is,\" that kind of thing, to actually be able to say, \"Well, here's a plan for how we can get to a very profitable future. It may not be the vision that you have, but it's concrete,\" and often, the folks in that level see themselves as the inspirational folks who get the plan moving in that direction by setting the goal and setting the long-term vision.\n\n0:10:45.8 LM: And when somebody can communicate up like that to say it's like, \"Well, we can't reach the exact nirvana that you're specifying, but we can build great products to do A, B, C and D, not all the way A through Z, and we can do it in this time frame,\" that, having that level of expertise to be able to speak to that comfortably and realistically, ends up being, I think, a great gift for everybody. If we go back to the conversation of what AI is and what AI isn't, is that I always like to draw this diagram that I say, \"Okay, here's traditional programming,\" and traditional programming, I draw it as a box, and that box is saying, \"You're putting rules in, you're putting data in and you're getting answers out.\" This is what programmers and the software department in your company have been doing since the dawn of software time, and the case is, what a programmer does is, they have to figure out how to express those rules in the programming language, so computers can do the work.\n\n0:11:36.2 LM: So for example, a very simple thing, like in financial services, there's a ratio called the price over earnings ratio, that's often a good one to determine the value of a company or one of the signals to determine the value of a company. And that's a very simple rule. Get the data of the price, get the data of the earnings, divide one by the other, and then you get an answer. There's obviously far more complex ones than that, but I like to use that one as a simple example. And you hire programmers because they know how to express those rules in a programming language and run them in an infrastructure. In the machine learning and AI world, I flip the axis around on that box. So instead of you trying to figure out the rules, you give the machine the answers and the data, and you have it figure out the rules. So for something like price over earnings, it's overkill, you don't need to do it. But what if there are patterns in your data that you don't see?\n\n0:12:26.5 LM: There are things about this company, and you can get a wealth of data around a company that you're doing an analysis on, and you can see that this company has done extremely well in the stock market, but you have no idea why, and this company has done extremely well, and you have no idea why, and then these bunch have done badly and you've no idea why. So then you have the answers, they've done well, they've done badly. You have the data, and the idea behind machine learning and AI is then, you can build a system that can do that pattern matching of the answers to the data and figure out what the rules are, to be able to do that.\n\n0:13:01.1 LM: So for you to do that effectively, you need good data scientists. It's not just, you get a shovel and you throw the data into the machine and something magic happens. You have your data scientists to try and make this as efficient as possible by picking the columns in the database or maybe doing feature crosses on those columns, where multiply this one by this one, that kind of thing. And the same way as your coders today, they're not just typing on a keyboard and stuff magically appears, they are figuring out the rules, they're figuring how to scale them. And that's really where the magic of good data science department applies, and so you've got skilled people who know the domain data, who know how to build models, so that the data is being used efficiently, so you can train a model in a couple of hours instead of a couple of decades, and that kind of thing. So it's like, that's where those folks, beyond trendy, really, really can show massive value for the company. And I'd say the same analogy, if you can get a programmer to build an effective program that runs your business in a day or a week, as opposed to an ineffective programmer who takes years to do the same task.\n\n0:14:07.6 LM: The same kind of thing can be applied with data scientists, that they can figure out which parts of the data to shovel, which parts not to shovel, they can figure out how to label those answers and all of those kind of things, so that the machine learning engineer can do their job effectively.\n\n0:14:21.7 LM: The way I generally like to define AI itself is, when you make a machine that responds the same way that an intelligent being would respond. So computer vision is a good example of that, is that if I show you a picture of a cat... If I show you a picture of my pet, you would say, \"That is a dog.\" Showing a computer a picture of my pets, prior to AI machine learning, deep learning, it would see a whole bunch of pixels and it has no parsing of the content, other than white pixels, blue pixels, those kind of things. When you start using machine learning and deep learning to then kind of train a computer to understand the difference between a cat and a dog, and then I show it this picture of my pets, and the computer will say, \"That's a dog.\" Now, the computer is responding in the same way as an intelligent being would respond, and that to me, is what artificial intelligence is all about. So you play it a sound, and instead of it saying, \"Here's a number of audio levels,\" it's actually able to determine your speech and to determine what you're saying, the same way as an intelligent being would. That to me, is artificial intelligence.\n\n0:15:31.6 LM: There are lots of ways that you can get there. Machine learning, deep learning are probably the most efficient way for things like computer vision, for audio processing, for tax processing and those types of things. So if we think about it and what it is... In terms of what it is and what it's not, it's not this magic thing that you can just say, \"We're gonna... Like in a Dilbert cartoon, we're gonna say, \"Let's put machine learning and artificial intelligence into our product and we get an upgrade.\" It doesn't really work like that.\n\n0:15:58.7 RS: The training is in the interest of an inference. When your technology can make an inference, an accurate inference, it has mimicked human cognition, right?\n\n0:16:06.5 LM: Yeah, exactly. And the nice thing is then, it can even go beyond human cognition, and let me give an example of this, that blows my mind. And so we worked on a project for diabetic retinopathy, at Google, where diabetic retinopathy is the world's leading cause of blindness. And the thing about it is that, it's easy to diagnose and it's easy to cure with early diagnosis. India has the world's second largest population, but it has a shortage of over 100 thousand qualified ophthalmologists. So we worked with doctors and hospitals in India to gather lots and lots of retina scans, to see... We'd label these retina scans... Data plus answers. We'd label these, based on those five different buckets, no diabetic retinopathy, all the way up to severe, and trained a machine learning model on this, to be able to be an artificial intelligence to respond the way an ophthalmologist would, and it ended up, like the publications that we did in various journals, showed that it was at least equivalent to a qualified ophthalmologist and often better. And so, that's the first mind-blowing part.\n\n0:17:13.5 LM: But then the second mind-blowing part, and the one that really hooked my interest in this was then a scientist within Google was looking at the data and realized, we don't just have labels of the diagnosis, we also have labels of the person's birth gender, or the person's age, or the person's blood pressure.\n\n0:17:33.6 LM: Now, an ophthalmologist can look at the scan of an eye and see blood clots and determine do they have diabetic retinopathy or not, but an ophthalmologist can't look at that scan and pick their age, or pick their gender. And so what if you have all of this data, you have your answers, you have your data, what if we could feed this into a model and do it? And it ended up, they trained a model that was 98% accurate in picking the assigned gender at birth, which is as good as, if not better than the average human, but obviously much better than a human looking at a retina, that kind of thing. It would be 50-50, it would be a coin flip. But it was 90% accurate, and it was also able to predict their age, with a mean average error of about three years.\n\n0:18:19.1 LM: And a few times in the past, I've told this story to an audience and I'd asked the audience to guess my age, and on average, the audience was... The mean average error from the audience was way more than that, and they're looking at me, they're looking at my gray hair, they're looking at my mannerisms, they're not looking at my retina, and they're still getting it even more wrong than this was... Again, looking at the retina. So we talk about human cognition and that kind of stuff, but in some ways, doing this kind of pattern recognition, we can go beyond human cognition, with examples like that one. If you have the data and if you have the labels that it's possible now for a machine to be able to do the matching of that data to that label and spot patterns that you as a human, wouldn't previously spot, and then there's massive, untold opportunities in that. So again, if we get down into that trust of disillusionment, and part of that is, I'm saying Machine Learning is fancy pattern matching.\n\n[chuckle]\n\n0:19:13.8 LM: And that kind of thing. There's nothing magical about it. And then when you understand that and you say, \"Well, I have this wealth of data in my business, can I find new business opportunities with this?\" And the answer to that, then is potentially, yes. In the same way as that scientist at Google was able to build a system to be able to predict somebody's age from a retina scan, which nobody knows how you can look at a retina and determine an age. From the model that they built, so you can now do an audit of that, and there's something called attention mechanisms, so you can see where the computer is paying attention to, to be able to derive what it is in a retina that let's you pick somebody's age. But it's like those are the kind of things that now, that the brute-force aspect of sheer compute power, doing that kind of pattern matching allows you to come up with these new scenarios that will rise you up through that plateau of productivity.\n\n\n\n0:20:07.3 RS: Yes, so you said it was an attention mechanism? And this allows you to clue in on, this is the variables that it was taking into account to result in this insight?\n\n\n\n0:20:18.1 LM: Yeah. Exactly, exactly. I teach it in one of my Coursera courses. I do advanced computer vision. And there's one really fun example that we go through in that one, it's not the retina one, that one is a little bit too complex, but there's a very famous machine learning exercise, which is pictures of cats and dogs that I was talking about earlier on, and how you train a computer to be able to recognize the difference between a cat and a dog. And you build a machine learning model in the course that can quite accurately tell the difference between a cat and a dog. But then you also do the attention mechanism stuff on that. And it turns out the primary difference that in this case, the computer was looking at, to pick the difference between a cat and a dog was the eyes. Sometimes you think, \"Oh the cat has pointy ears, the dog has floppy ears, for the most part\" or \"Their noses look differently\", but for the most part, when this model was actually working to pick the difference, it was like those were the features that it had zeroed in on and so then I was able to learn from that and go, \"Aha, so now when I build a model, maybe I should focus on the eyes to be more efficient\", that type of thing.\n\n0:21:16.7 RS: Yes, it strikes me as a crucial mechanism in removing harmful biases, for example, from a black box AI, from being able to look under the hood and say, \"Okay, this is what it was looking at to get this insight\". That can help remove a lot of this fear and a lot of these potentially, harmful biases or incorrect assumptions that technology would make.\n\n0:21:41.9 LM: Yeah, yeah, exactly. And there's a technique, it's also called... There's a thing that you can build, called a class activation map, and the idea with the class activation map is, you're seeing what the computer was paying attention to. A funny story about them, the US Army, realized that maybe computer vision could be used to see things and images that humans couldn't see. And say take for example, on the battlefield, what about being able to see a camouflaged tank? That like a human could look at it, camouflage is designed to fool the human eye, but what if you could have a machine be able to detect like a camouflage camouflaged tank? So they did an experiment where they got a bunch of data scientists and a bunch of machine learning engineers and they gave them a tank, and they said, \"Hey, you go out into the woods and one day take a whole bunch of pictures of this tank un-camouflaged\", and then the following day, they got the camouflage nets, and they put the camouflage nets on the tank and take a whole bunch of pictures of this tank camouflaged. And so build a model off of this one to see if you can pick a camouflage tank or a non-camouflage one, and they did what all good data scientists do, they had a training set of data, they had a test set of data, they had a validation set.\n\n0:22:54.3 LM: They built their model, they ran it and it was like 99% plus accurate. And they were like, \"Oh my gosh, we have built something that can really, really change the course of the battlefield\". They presented that to the Army, the Army loved it, and then they took it out and tested it and it failed completely.\n\n0:23:10.8 RS: Oh no. [chuckle]\n\n0:23:12.5 LM: And the reason why it failed completely was that, they took the pictures of the un-camouflaged tank one day, and they took the pictures of the camouflaged tank on another day. And on the day that they took the camouflaged tank, the sky was cloudy, and on the other day they had a blue sky, and then when they fed this back through an intention mechanism, they realized they didn't build a camouflage detector. They built a cloudy sky detector. [chuckle] So with the black box element of this kind of thing, it's easy to think that these are hard to debug and that kind of stuff, but they're not necessarily that difficult to debug if you understand how they're architected, and if I gave the elevator pitch for how you to do this, when you train an AI system or a machine learning system, you're flowing data one way and doing back prop the other way, but when you wanna do these attention mechanisms, those kind of things, it's just the way of flowing data in the other direction and effectively de-compiling it. If you're going through convolutions, you're de-convoluting it and that kind of stuff, and you can get a pretty good estimate for how the computer is looking at your data.\n\n\n\n0:24:12.1 RS: What is the difference between a classification map and an attention mechanism?\n\n\n\n0:24:15.6 LM: A class activation map is a...\n\n\n\n0:24:17.1 RS: Yeah. Thank you.\n\n\n\n0:24:18.3 LM: So it's a case of when you build a Convolutional Neural Network in particular, you're learning filters that can isolate features in a map, and a class activation map is where you just figure out where those features are on, you light it up on a diagram with a heat map or something like that. And that is a type of attention mechanism. There are also other ways of you being able to pick out attention within a machine learning model or something like that. Class activation maps are a very common one that are used in Computer Vision. Anything that you do, Convolutional Neural Networks, to be able to identify features, there are... Sometimes also use used Convolutions for sequence maps, so if you wanna predict weather in the future or something like that, you may use a one-dimensional convolution on that, and you can potentially have a class activation map there where it spots like, \"Hey, when you got a spike followed by a dip\", in this kind of thing, then that's usually followed by something else. But typically, it would be in an image-based one, it's where it's most commonly used.\n\n0:25:17.6 RS: I like how you mentioned the examples of the smartphone and understanding Hype Cycles, I'm curious if there are any lessons you think we can learn from the way that, that sort of technology was deployed and iterated upon that we can correct or do better with... As AI is sort of spread to the world.\n\n0:25:35.7 LM: The first part is letting people realize that they are in a hype cycle, we've been in hype cycles before. The people who were successful, were the ones... Or initially successful, at least, were the ones who saw through that, and this is what they did to see through it. Exposure to the platform, exposure to the technology, trying out new and exciting and different things, there's a whole graveyard of failed apps on Android and iOS, which laid the framework and the pathway for those apps that were successful. So really being those kind of early adopters, having that, try what you can, learn, iterate, continue. That's what's led to success, and I think that's the same kind of thing that can lead to success in the AI space.\n\n0:26:19.2 LM: One of the advantages of the AI space, is that the amount of investment that you have to make to be successful is a lot less than the amount of investment you might have previously had to make to become the big mobile app developer or to become the big website, and as a result, you don't necessarily have to be housed in the traditional areas on centers of excellence and success. So if we can try to democratize AI as much as possible by making it as available to as many people as possible so that they can seize opportunities that the rest of us may not actually think about, that could pave the way to success for them and for everybody else also. For every success, there's probably going to be a 100 failures, and it's really understanding that, realizing that, but I would rather have a 101,000 people do something so that there's a thousand successes and 100,000 failures, than have 1001 people do it where there's only one success, if my math add up. I told you I'm not very good at math.\n\n[chuckle]\n\n0:27:23.4 RS: Yeah, the one... Yeah, I think that adds up. Of course, the YouTube channel and the MOOCS, and a lot of the content that you produce is any in the interest... It's accessible anywhere. Someone who has internet access can learn from an expert, such as yourself. I do worry though, at what point is there a breakdown in terms of the hardware and the ability to actually design this technology? Does one need access to cloud computing and a work horse of a laptop to be able to play in this field?\n\n0:27:53.3 LM: To be able to get started and play in this field, absolutely no. To be able to go huge in this field, you do need access to high-end hardware like GPUs and TPUs and that kind of thing. So to split those two audiences for the Getting Started one, that's where we've been very carefully focused on easy high-level APIs that will run in Python, which is easy to install and use, that you can do on any laptop with the a CPU so that you can get up and running and kick the tires with these kind of things and to make that as quick and easy as possible for anybody to do. When you go beyond that though, and you start trying to train bigger models, not everybody has access to GPUs, not everybody has access to TPUs. So part of our strategy there was, we have this thing called Google Colab, and Google Colab is an in the browser notebook that runs with a Google Cloud back-end that can provide you free access to a GPU or a TPU. Obviously it's limited, but it's pretty generous. It's many hours of training that you can get for that, and all you need is a browser and a web connection to be able to do that, if you don't already have the hardware.\n\n0:29:00.3 LM: So that's the one first part of the offering. The another part of the offering though, is that when we start thinking about where do your models execute? Okay, so that many models are gonna be built to execute in data centers, the likes of Google or Amazon or Microsoft or on... But that's not the only area of opportunity, we can see that the area of opportunity on mobile handheld devices, embedded systems and all those kind of things is possibly even larger. And with the price of them dropping sharply, the hardware to build a phone is getting cheaper, the hardware to build embedded systems is getting cheaper. Then as long as we have an ecosystem of tools that will allow you to build for them with as low a dollar cost of entry as possible, as low an intellectual cost of entry as possible, those kind of things, that's when those markets can be seeded and those markets can grow. And like I said, I think we can all benefit. Let me share one example, 'cause there's a great project that... It was a couple of years ago, that was built by a bunch of high school and college students in India. And it's called Air Cognizer, I think that's the right phrase.\n\n0:30:09.6 LM: And it's on the YouTube web... The TensorFlow YouTube website. And what they did was that they realized that in their city in India, there was extensive air pollution. And you know what it's like, we all probably are encountering and nowadays with fires nearby, I live in Washington, so every year we have to start looking at air quality because of forest fires. But what they realized was that, when you look at air quality and you see it on the news, or you see it on a website, that's the air quality at a sensor, which is being operated by somebody. Now that might be 20-30 miles from where you live, and the air quality where you live might be severely worse. Elderly parents that they had and grandparents were afraid to go out because they don't know the air quality and they could get sick. So these students got together and they realized if they get a sensor to measure the air quality, and they get a phone, a cheap Android phone with a camera on it, if they take a picture of the sky, they have data. If they measure the air quality on the sensor, they have a label, and if they go all over their city and they take lots of these pictures and lots of these sensors, you do that basic pattern matching to kind of build a model where you're saying, \"Well, when the sky looks like this, the pollution is like this\".\n\n0:31:27.8 LM: And they turned that into an app, and now lots of folks in India can use that app where they can just take a photo of the sky and see a good prediction of the air quality near them, instead of looking at the news and seeing an air quality indicator that could be 20, 30, 40 miles away. And it's like little things like that, little innovations like that, because these were high school and college students, they don't have a lot of money, they're not forming a startup where they're hiring developers to do this kind of thing, the equipment for them to do that, was basic laptops that they had, the data? They generated the data themselves because they had the sensor and they had a cell phone where they could take a picture of the sky, they were able to build a model for this using the open source ecosystem, and they were able to deploy it for free to Android phones.\n\n0:32:12.5 LM: These kind of things, when I talk about really lowering that bar so we could raise the floor, but now it's like, \"Well, the rest of the world can benefit from what they learned\", because we now have the same problem in the West because of forest fires. And I could potentially go out and do the same thing to build an Air Cognizer for Washington State without needing to invest millions of dollars in a start-up to do so. So when you bust through that hype cycle and you understand how this works, then you can think like that, and that's what they did, they thought like that, and boom, they came up with this really cool solution.\n\n0:32:42.8 RS: This is the focus of your, about to be published, new book. Is that correct?\n\n0:32:47.1 LM: Yep, so it's an AI and Machine Learning for On-Device Development is my upcoming book. I originally was gonna create this mega-book for O'Reilly called AI and Machine Learning for Coders. We realized this weighed too much for one book.\n\n[chuckle]\n\n0:33:02.4 LM: So last year, I released the AI and Machine Learning for Coders, and now this year, it's kind of like the complimentary book/sequel, which is a AI and Machine Learning for On-Device Development. So it's really showing you how, as a mobile developer, you can start using models on Android, on iOS and a little sprinkling of doing it in a server with remote access or doing it on things like Raspberry Pi. It's packed with lots of examples of things like, you take a picture, here's how you can detect a face in the picture. Or here's how you can count the number of objects in the picture, like maybe you're building an app that's counting traffic, driving past your house. How do you count the number of cars? Those kind of examples... So I try to get very hands-on with them, of like, here's basically how this stuff works on your device. As of today, you don't train models on the device, you use models on the device. So the concept of my first book, AI and Machine Learning for Coders, or my first book in the series was really, \"Here's how you build the models\", and then the second book is, \"Okay, when you have models or there are off the shelf models available, here's how you use them, or here's how you can customize them to actually use them on your device\".\n\n0:34:16.9 RS: Okay, so the model is not constructed locally? The model is accessed?\n\n0:34:20.9 LM: Yeah. As of today, trying to train a model on a mobile device, it's just going to be very hostile towards your battery because model training is very intensive. We are doing a lot of work on making that better, but as of today... Yeah, as a developer, you're better off training a model in the cloud with something like Colab, or on your developer workstation and then deploying it to your device.\n\n0:34:46.4 RS: Yes.\n\n0:34:47.3 LM: But that's changing. That is changing.\n\n0:34:48.1 RS: Yeah. Well, fans of this podcast will remember our episode with Sama CEO, Wendy Gonzalez, who was speaking about this similar kind of problem of, \"How do we democratize access to AI?\" and I can envision an approach to that, which is just drop a 100 copies of your book and a 100 Android devices, just anywhere in the world and let a rip, right?\n\n0:35:10.2 LM: Yeah, yeah, please do. I'd love to see the results.\n\n0:35:16.8 RS: Laurence has published all manner of content about the realities an opportunities of AI, both philosophical and technical. In the episode description, you'll find links to his MOOCs, books, and the TensorFlow YouTube channel where he frequently contributes. You can also find Laurence's resources on the new, How AI Happens, LinkedIn group. Here, we'll post all the research and resources mentioned by our guests and give you the opportunity to rub shoulders and ask follow-up questions with the experts you hear featured on the show. Just search How AI Happens on LinkedIn and say, hello. How AI Happens is brought to you by Sama. Sama provides accurate data for ambitious AI. Specializing in image, video and sensor data annotation and validation for Machine Learning algorithms in industries such as transportation, retail, e-commerce, media, MedTech, robotics and agriculture. For more information, head to sama.com.","seo_title":"New Podcast Episode: Making AI Development Global with Google's Laurence Moroney","slug":{"_type":"slug","current":"/podcast-google-global-ai-development"},"tags":[{"_key":"saTGs8ZP","label":"Ethical AI","value":"Ethical AI"},{"_key":"jr0XK73o","label":"Podcast","value":"Podcast"}],"title":"New Podcast Episode: Making AI Development Global with Google's Laurence Moroney"},{"_createdAt":"2021-07-29T17:14:55Z","author":{"_id":"10ead718-57e1-41a8-b846-da3c81cc323a","avatar":{"_type":"image","asset":{"_ref":"image-a4c79da81bb1e23ce10fba84ea2cba5efe67a2a5-200x200-webp","_type":"reference"}},"bio":"Currently a Director of Product Management at Sama, Saul is passionate about the intersection of technology and social impact. He manages Sama’s data labelling products to ensure high quality training data efficiently and reliably reaches our customers. Experienced in both product and professional services, Saul is a proven leader who takes a data driven approach to expanding Sama’s capabilities and features. When not at work, you can usually find Saul enjoying the outdoors and spending time with his family.","name":"Saul Miller","slug":{"_type":"slug","current":"saul-miller"}},"estimatedReadingTime":7,"featured_image":{"_type":"image","asset":{"_ref":"image-65b7fac1b60586a0a7b9ff75006684e2c2467f1e-1800x900-png","_type":"reference"}},"meta_description":"ML Assisted Annotation can help you generate high-quality pre-labeled and human-assisted annotations, for predictably higher quality data in half the time.","openGraphImage":null,"plaintextBody":"Your machine learning model is only as good as the data it’s trained on. And with 80% of AI project time being spent training the large volume of data necessary to train a model, efficiency improvements early on in the process are sure to have compounding effects.\n\nAt Sama, we have a dedicated Machine Learning team working at the forefront of AI research to identify optimization opportunities just like this, so we can develop advanced annotation tools to smooth the path to production for our clients. One of the papers the team presented at last year’s CVPR—Human-Centric Efficiency Improvements in Image Annotation for Autonomous Driving—shared an approach to speeding up polygonal instance segmentation using ML.\n\nToday, this technology has been incorporated into our platform to make our clients’ labeling process more efficient.\n\nWe call it ML Assisted Annotation powered by our MICROMODEL technology, and it’s already helping clients predictably get higher quality training data in half the time.\n\nRead on for an overview of ML Assisted Annotation powered by MICROMODEL technology how it can help you develop models that are more scalable, robust and accurate – and can be brought into production more quickly.\n\nWhat is ML Assisted Annotation powered by MICROMODEL technology?\n\n\nML Assisted Annotation (MAA) powered by MICROMODEL technology is an architecture that allows Sama to expedite the labeling process by drawing from a library of models trained on specific use cases. MAA can be used to generate high-quality pre-labeled annotations, which annotators validate to help them continuously improve over time.\n\nThis powerful combination of skilled annotators and an AI-powered platform allows us to deliver a high standard of label quality to our customers every time, along with efficiency improvements and quicker time to market.\n\nHow it works\n\nIn order to understand how MAA works, we first need to discuss the DEXTR model. DEXTR, or “Deep Extreme Cut,” is a publicly available object segmentation model for images and videos. \n\n\n\nWe’ve outlined the DEXTR model and our approach in detail in this post.\n\n\n\nMany ML methods like DEXTR have been suggested to speed up the process of instance segmentation, but these are not typically tested in a high-scale production environment, nor are ML outputs easily edited by human annotators. This makes it difficult to confidently reach the label quality standards required to run a model in production.\n\nMAA combines the well-known DEXTR approach with a raster-to-polygon algorithm to make results easily editable by a human in the loop. We’ve found that this approach—which pairs skilled annotators with ML-powered automation—significantly increases labeling efficiency and quality.\n\nLet’s see what that looks like in practice, using an example from the Autonomous Vehicle industry.\n\nMachine-Assisted Polygon Annotation\n\n\n\nWhen an annotator logs into the Sama annotation platform, they are presented with this workspace. In this example, the workspace is customized to allow the annotator to draw instance segmentation polygons around each of these vehicles:\n\n\n\nYou’ll notice that there are several vehicles in this image. In a manual context, it could take a human several hours to deliver high-quality annotations of every single vehicle:\n\nWhat the manual annotation process would look like (sped up significantly): several clicks are required to draw a polygon around each of the vehicles.\n\n\n\nThis process is significantly accelerated with Machine-Assisted Polygon Annotation.\n\nThe model allows the annotator to use a crosshair tool to identify only four extreme points: left, right, top and bottom boundaries. These four clicks are the only inputs needed to create a heat map that is then sent to the inference server, returning an accurate prediction of a raster mask.\n\n\n\nWith Machine-Assisted Polygon Annotation, annotators only need to perform four clicks to produce an accurate raster mask prediction.\n\n\n\nMachine-Assisted Polygon Editing\n\n\n\nA polygon prediction can then be further refined by an annotator by switching into editing mode. This enables annotators to label precisely and ensure that high-quality requirements are met without compromise.\n\n\n\nIn this example, the raster mask prediction is edited by the annotator to ensure precise and high-quality labels.\n\n\n\nThis mode also enables annotators to use more than four extreme points in order to produce even more accurate predictions. A fifth user input point can easily be added, with the model immediately incorporating the new input to update its prediction.\n\nIf an ML model struggles to identify specific shapes, annotators can add a few more inference points to help result in a more accurate prediction, and then refine that prediction manually to ensure high-quality labels.\n\nResults from ML Assisted Annotation powered by MICROMODEL technology\n\n\nOur clients are already seeing impressive results from MAA powered by MICROMODEL technology:\n\nPredictably producing 94-98% IOU (Intersection over Union) accuracy\nBecause our models are pre-trained on specific use cases for better performance out of the gate, our clients are seeing a quicker time to accuracy.\n\n2-4x more efficient annotation process\nYou can clearly see above that using MAA over a more manual polygon labeling approach results in significant time savings. But it’s also an iterative process with a human annotator in the loop; modifications to the predictions get fed back into the training data pipeline to retrain the model, enabling it to perform better predictions over time.\n\n\nQuicker time to market\nThe end result for our clients is faster iterations and a quicker time to market. A more efficient annotation process results in more data returned quickly, and ultimately a significantly shorter path to production.\n\nWhat’s more: increasing the efficiency of this labor-intensive manual data annotation process reduces the barrier to entry for more ML teams... and not just those with large R\u0026D budgets. Technology like this can also help democratize data labeling by driving down cost, so we can see even more deserving companies leverage AI to drive value for their business.\n\nSmall teams who are getting started with labeling may not have yet defined what type of annotations they need, or how much data they need to be successful. MAA can help them iterate more quickly, developing models in short increments rather than in large, cumbersome workstreams. The end result is a quicker time to value, and ultimately, to market — for organizations of all shapes and sizes.\n\nLearn more about ML Assisted Annotation powered by MICROMODEL technology by watching our recent webinar here, or reading more about it here.","seo_title":"ML Assisted Annotation Powered by MicroModels","slug":{"_type":"slug","current":"ml-assisted-annotation-micromodels"},"tags":[{"_key":"giPx682h","label":"Machine Learning","value":"Machine Learning"},{"_key":"JOgmW0AV","label":"Data Annotation","value":"Data Annotation"},{"_key":"45fExC0n","label":"Autonomous Transportation","value":"Autonomous Transportation"}],"title":"ML Assisted Annotation Powered by MicroModels"},{"_createdAt":"2021-07-15T18:31:35Z","author":{"_id":"37c7e793-6028-467d-9b68-f43d8c0e1812","avatar":{"_type":"image","asset":{"_ref":"image-0bedc789dac36f338b424f6e94cdc5966aeff856-360x360-webp","_type":"reference"}},"bio":"Kristen Itani Koue is passionate about using tech as a force for good. As Sama's Senior Impact Manager, Kristen leads efforts to measure and expand the impact of Sama's ethical supply chain and sustainability initiatives. Kristen is most happy when pushing companies to do what is best for all their stakeholders, running around after her two young children, or climbing up or snowboarding down mountains.","name":"Kristen Itani Koue","slug":{"_type":"slug","current":"kristen-itani-koue"}},"estimatedReadingTime":4,"featured_image":{"_type":"image","asset":{"_ref":"image-fdeacb17e65c946761066de369acc4bdf1353a73-3334x1668-png","_type":"reference"}},"meta_description":"This whitepaper by Partnership on AI examines the working conditions of data enrichment professionals, and what can be done to improve them.","openGraphImage":null,"plaintextBody":"When an organization sets out to build and deploy an AI model, a number of decisions have to be made. Decisions around building a robust data pipeline are among the most fundamental.\n\n\n\nChoosing the right data partner will obviously impact how well a model performs. If we look a bit deeper, we also find that this choice has a real impact on the well-being of the whole data supply chain workforce. Sometimes collectively referred to as data enrichment professionals, all workers involved in the data supply chain—from labelers to quality analysts—are affected by an organization’s data procurement decisions. \n\nThe complexity of the data procurement process combined with a lack of standards around equitable data supply chains has several downstream implications for these essential but largely unseen workers. Despite their foundational role, working conditions for data enrichment professions can be precarious: they can face wage uncertainty, low pay and limited career growth opportunities.\n\nYour data enrichment choices have a direct impact on workers’ well-being\n\nThere is, however, an opportunity to make a difference. The decisions organizations make while procuring data labeling services can have a meaningful impact on the working conditions of data enrichment professionals.\n\nPartnership on AI (PAI) is a multistakeholder organization that brings together academics, researchers, civil society organizations, companies building and using AI technology, and other groups working to better understand AI's lasting impacts. Their latest white paper Responsible Sourcing of Data Enrichment Services examines the working conditions of data enrichment professionals, and seeks to:\n\nCritically evaluate the impact of the industry’s current practices on workers;\n\nExplore practices the industry can adopt to improve worker well-being; and\n\nAdvance the discourse around the future of data enrichment work and the indispensable role it plays in AI development.\n\nThe paper gives AI practitioners and decision makers visibility into the impact of their decisions in procuring data enrichment services – from selecting providers, to running pilots, to conducting quality assurance and more.\n\nRead the full white paper: Responsible Sourcing of Data Enrichment Services\n\nSince data labeling is a key part of this equation, it was important for Sama to partner with PAI: to both share our lessons learned from several years of striving for more ethical supply chains, and to continue to learn from industry peers about responsible AI.\n\nIn fall of 2020, Sama participated in the five-week series of Responsible Sourcing workshops held by PAI, the output of which was a set of strategic recommendations for this white paper. The workshops brought together more than 30 professionals from different areas of the data enrichment ecosystem including representatives from data enrichment providers, researchers and product managers at AI companies, and leaders of civil society and labor organizations. \n\nTo fully understand the impact of AI on society, one must examine the bias and shortcomings of models, but also the means by which they are created.\n\nWhile more work and research is needed, Responsible Sourcing of Data Enrichment Services shares actionable insight that practitioners can use as a starting point to raise important conversations with internal stakeholders. Our hope is that these conversations will be a step in the right direction: toward giving data enrichment professionals better recognition for the critical role they play in building AI. \n\nRead the full white paper: Responsible Sourcing of Data Enrichment Services\n\n\n","seo_title":"New White Paper by Partnership on AI: Responsible Sourcing of Data Enrichment Services","slug":{"_type":"slug","current":"partnership-on-ai-whitepaper"},"tags":[{"_key":"pwpeoY5O","label":"Ethical AI","value":"Ethical AI"},{"_key":"Sf3M02mD","label":"Impact","value":"Impact"}],"title":"New White Paper by Partnership on AI: Responsible Sourcing of Data Enrichment Services"}],"featuredPosts":[{"_id":"27487c06-2421-4d39-8930-6c49a04ffa49","featured_image":{"_type":"image","asset":{"_ref":"image-19ef3f125b8ee47727e9f94dc3aff7ed08a4d78c-720x360-png","_type":"reference"}},"slug":{"_type":"slug","current":"devops-tools-for-cloud-native-engineering"},"tags":[{"_key":"8AQmCtzZ","label":"Sama Engineering","value":"Sama Engineering"},{"_key":"Gl5fCrPJ","label":"Featured","value":"Featured"}],"title":"Factotum: Containerizing DevOps Tools for Cloud Native Engineering and CI/CD"},{"_id":"085b9054-a1d9-447e-a430-d2df9be23647","featured_image":{"_type":"image","asset":{"_ref":"image-04641081798c41caebebefa44685828d80b7434f-1200x675-png","_type":"reference"}},"slug":{"_type":"slug","current":"part-1-automating-model-training-on-the-cloud"},"tags":[{"_key":"OdrqBpR4","label":"Sama Engineering","value":"Sama Engineering"},{"_key":"Yw0P8Jjj","label":"Training Data","value":"Training Data"},{"_key":"08tVZdgm","label":"MLOps","value":"MLOps"},{"_key":"eMmu9s8R","label":"Featured","value":"Featured"}],"title":"The Sama MLOps Pipeline: Automating Model Training on the Cloud"},{"_id":"d57f4906-bd8a-4f84-8d9d-8bea5e8d7797","featured_image":{"_type":"image","asset":{"_ref":"image-6abd2f846119ba50a8325787f99ec3aeec68ffff-1076x605-png","_type":"reference"}},"slug":{"_type":"slug","current":"fast-vector-annotation"},"tags":[{"_key":"fJGSmFCx","label":"Vector Annotation","value":"Vector Annotation"},{"_key":"21OOfbwx","label":"Polygons","value":"Polygons"},{"_key":"SPABaoXN","label":"Sama Engineering","value":"Sama Engineering"},{"_key":"O5nmmFbm","label":"Featured","value":"Featured"}],"title":"Fast Vector Annotation with Machine Learning Assisted Annotation"}],"morePosts":[{"_createdAt":"2021-06-24T16:00:00Z","author":{"_id":"e8a1f3c6-0b07-4638-93d5-61a0924f32e3","avatar":{"_type":"image","asset":{"_ref":"image-2f8efc599a377bd580d0ae12f022a15e7e1acacd-632x676-webp","_type":"reference"}},"bio":"Amanda is the Content Marketing Manager at Sama, where each day she gets to learn about AI and how it’s transforming industry. She’s passionate about technology but periodically tries to get away from her computer to explore more analog things like collage and quilting.","name":"Amanda Durepos","slug":{"_type":"slug","current":"amanda-durepos"}},"estimatedReadingTime":5,"featured_image":{"_type":"image","asset":{"_ref":"image-080a7046afcb7bb25b5af2d55d55883944f1c5eb-3334x1668-png","_type":"reference"}},"meta_description":"Project Guideline by Google partnered with Sama to help people who are blind run without a guide, using only a smartphone, headphones, and a yellow guideline.","openGraphImage":null,"plaintextBody":"Project Guideline is an early-stage research project by Google that explores how on-device machine learning can help people with reduced vision to walk and run for exercise independently. The team has partnered with Sama to help fuel their experimental technology; allowing people who are blind and low vision to use a mobile phone, headphones, and a yellow guideline painted on the ground run without a guide.\n\n\n\nOverview\n\n\n\nIt started with a pointed question during a Google hackathon in the fall of 2019. Thomas Panek, an avid runner and CEO of Guiding Eyes for the Blind, posed the question to a group of designers and engineers:\n\n“Would it be possible to help a blind runner navigate, independently?”\n\nPanek, who is blind himself, has completed more than twenty marathons, including five Boston Marathons. While he’s had guide dogs and volunteer human guides to run with him throughout the years, he’s always had to follow—even though his legs and lungs had the capacity to go faster.\n\nIt quickly took off from there. By end of day, the team had a working prototype. Less than a year later, Panek was able to run independently for the first time in more than two decades.\n\nRead the full case study here. \n\n\n\nThe Challenge\n\n\n\nThe machine learning algorithm developed by Google requires only a line painted on a pedestrian path. Runners wear a regular Android phone in a harness around the waist and count on the camera to feed imagery to the algorithm, which identifies the painted line. The algorithm is tasked with detecting whether the line is to the runner’s left, right, or center, so audio signals can be sent to the runner to guide them to stay on track.\n\nFor humans, it’s fairly straightforward to recognize a line and follow it. For a machine learning model, it isn’t that easy. Imagine the running motion: as you start moving your feet you step from left to right, introducing a shake that can make the guideline blurry. Moving outdoors to Panek’s preferred running location, you introduce even more variables. The model must be able to handle a wide range of weather and lighting conditions, or objects like fallen leaves blocking the guideline.\n\n\n\nThe Solution\n\n\n\nSama’s expert annotators draw precise polygons around the single solid yellow lines and a single center line in the images. To do this, the Sama team underwent an intensive training process and continues to meet with Google’s engineers each week to check in on quality, discuss edge cases, and receive new instructions. Thus, with enough examples to learn from, the algorithm is trained to distinguish the pixels in the yellow line from everything else.\n\nBut quality AI starts with quality data. And quality data has to be diverse. The Project Guideline data needed to encompass every imaginable scenario that a runner might encounter. In Panek’s case, we quickly noticed that we had to include the runner’s hand blocking the guideline. This was solved by having our annotators infer the position of the line behind the hand—a great example of something that is easy for a human to do but rather difficult for a computer. By continuously adding these variations to the dataset, the model is getting smarter over time. \n\nFueling the cutting-edge technology that helps a blind man run without assistance truly is the stuff that dreams are made of. Sama’s annotators, the experts giving artificial intelligence its intelligence, love being a part of Project Guideline. Bridget Nattabi, who has worked on this project since its kickoff in July 2020, shares her thoughts:\n\n“Working on this project has allowed me to grow and master polygon annotation with high efficiency and accuracy. I also feel honored to be part of a team that is creating a life-changing navigation experience for the blind. It’s heartwarming to consider that what I do gives people a chance to navigate the world without a guide just like any sighted individual would.”\n\nA word from Project Guideline\n\n“Sama was a force multiplier for us and a key success factor for our project. They delivered high-quality annotated data on time, listened to our feedback, and were very flexible in accommodating our requests.”\n-Xuan Yang, Computer Vision Researcher at Google\n\nRead the full case study here. \n","seo_title":"How Sama's Accurate AI is Helping Blind Runners Run Independently","slug":{"_type":"slug","current":"google-project-guideline"},"tags":[{"_key":"3slAcWy7","label":"Case Studies","value":"Case Studies"}],"title":"How Sama's Accurate AI is Helping Blind Runners Run Independently"},{"_createdAt":"2021-06-22T15:12:25Z","author":{"_id":"f972de8a-10c1-45e3-97c9-ac490eaceabe","avatar":{"_type":"image","asset":{"_ref":"image-4aa17073cfd70d2e8f7d8ed85325c14cb1519577-692x691-jpg","_type":"reference"}},"bio":"Loic has over 20 years of industry experience in the Cloud services and AI industry. At Sama he works as the VP of Research \u0026 Development. His experience includes Fortune 500 Companies such as Salesforce.com, Unity Technologies, and AT\u0026T where he led the development of large scale AI, data analytics, and cloud solutions. Loic received his MS in computer science from UTBM, France.","name":"Loic Juillard","slug":{"_type":"slug","current":"loic-juillard"}},"estimatedReadingTime":3,"featured_image":{"_type":"image","asset":{"_ref":"image-c1343597ce10d9d908608f0be0204d0a0d9a09b2-1200x600-png","_type":"reference"}},"meta_description":"Sama's third annual Innovation Week is coming to a close, and once more, our teams have given us plenty to be excited about.","openGraphImage":{"url":"https://cdn.sanity.io/images/76e3r62u/production/c1343597ce10d9d908608f0be0204d0a0d9a09b2-1200x600.png"},"plaintextBody":"At Sama, we are committed to building a platform that helps organizations bring their models to production more quickly. But succeeding in a field as nascent as AI requires more than just bright minds and an ambitious roadmap.\n\n\n\nA key ingredient to success – and one that is often overlooked – is a mindset of experimentation; a culture that encourages teams to solve issues in new and more efficient ways. Early on, we found that our teams thrive in this kind of environment, which is why this week, we’re wrapping up our third Innovation Week. \n\nThe concept is simple: for one week, anyone can work on a project they feel passionate about. During the week, you have free rein to work on whatever you please in whichever way you choose. There are no meetings and no interruptions. You are free to try new things, invent and learn. You can be part of a team or single-handedly run with your ideas.\n\nThis year, some exciting highlights include:\n\nA more efficient approach to video tracking and segmentation\n\nNew Smart Polygon tool that is able to greatly increase annotation efficiency\n\nEnhancing our platform through the introduction of a Sama CLI and data pre-processing capabilities\n\nThe atmosphere is incredible. The week starts with a kickoff, where everyone can present the problem they are trying to solve and how they are planning to solve it. The closing ceremony is called the “Grand Reveal,” where each individual or group demos what they worked on. It sometimes yields incredible presentations... other times, it doesn't.\n\nFreedom breeds creativity. These events have brought some of the most core and innovative components of our platform. Last year, over 50% of all Innovation Week projects were incorporated into our roadmap and implemented within the following two quarters. A perfect example of this is our MicroModel technology, which was initially borne from one of these projects. We now use this technology for Machine Assisted Annotation for deployments in the automotive and e-commerce industries.\n\nNot only does the event enhance our product, it also attracts talent. This year, we doubled our R\u0026D team and are continuing to grow.\n\nInnovation Week gives our R\u0026D team the space to do what they do best: observe, experiment and discover.\n\nInnovation Week is coming to a close, and once more, our teams have given us plenty to be excited about. As always, many of the projects are slated to appear on our platform. In particular, we saw lots of work around our new 3D LiDAR annotation automation and data processing… stay tuned for that.\n\nAt Sama, Innovation Week is designed to encourage experimentation by allowing teams to shift the focus from our immediate customer’s needs to a broader, more bold approach to Machine Learning. If that sounds fun to you, consider checking out the open roles on our R\u0026D team here.","seo_title":"Innovation Week: How Sama Builds a Culture of Experimentation","slug":{"_type":"slug","current":"innovation-week-2021"},"tags":[{"_key":"DWf2I8xz","label":"Machine Learning","value":"Machine Learning"},{"_key":"TUfn4bAi","label":"Product","value":"Product"},{"_key":"imL766wL","label":"AI Practitioners","value":"AI Practitioners"}],"title":"Innovation Week: How Sama Builds a Culture of Experimentation"},{"_createdAt":"2021-06-17T19:32:29Z","author":{"_id":"e8a1f3c6-0b07-4638-93d5-61a0924f32e3","avatar":{"_type":"image","asset":{"_ref":"image-2f8efc599a377bd580d0ae12f022a15e7e1acacd-632x676-webp","_type":"reference"}},"bio":"Amanda is the Content Marketing Manager at Sama, where each day she gets to learn about AI and how it’s transforming industry. She’s passionate about technology but periodically tries to get away from her computer to explore more analog things like collage and quilting.","name":"Amanda Durepos","slug":{"_type":"slug","current":"amanda-durepos"}},"estimatedReadingTime":21,"featured_image":{"_type":"image","asset":{"_ref":"image-1502043edcf701cb4267cb2f6d7c6edf8e0e0cad-2500x1313-png","_type":"reference"}},"meta_description":"Sama CEO Wendy Gonzalez joins our podcast, How AI Happens, to discuss why providing work is a more impactful approach to ending poverty than providing aid.","openGraphImage":{"url":"https://cdn.sanity.io/images/76e3r62u/production/1502043edcf701cb4267cb2f6d7c6edf8e0e0cad-2500x1313.png"},"plaintextBody":"AI starts with data. Clean, curated, labeled, ready-to-plug-into-your-models data. But it’s not always so easy to get your hands on a dataset that you can use to quickly and efficiently push your models into production. That’s why the individuals who focus on data labeling are so important, and why Sama is making huge efforts to cultivate the next generation of AI specialists.\n\n\nRather than sourcing from the world’s top universities or boot camps, Sama has spent several years delivering a course entitled Artificial Intelligence 101 to individuals in Kibera, the largest slum in Africa. This course teaches students the basics of AI work, simultaneously training AI talent and taking aim at systemic poverty.\n\nTo discuss the results of these efforts, Sama’s CEO Wendy Gonzalez joined our podcast, How AI Happens. Wendy explains:\n\n🖥️ Why providing work is a more impactful approach to ending poverty than providing aid;\n\n🎓 How Sama’s program has measurably improved women’s income and employment rate by 60%;\n\n✏️ How data labeling skills provide talent with access to employment in AI and beyond.\n\n\n\nStream the full episode below, and if you’d like to see the nuts and bolts of how Sama’s mission has affected the community, be sure to check out MIT’s recent RCT study measuring the impact of AI education on student employability and income.\n\n\n(audio embed)\n\n\n\nAnd don’t forget to subscribe to How AI Happens on your favorite podcast streaming platform!\n\n\n\nTranscript:\n\n\n0:00:00.0 Wendy Gonzalez: Talent is distributed equally, but opportunity is not. And the best way to solve poverty, which is basically the root cause for every major social ill in the world, is by giving work, not giving aid.\n\n0:00:16.8 Rob Stevenson: Welcome to How AI Happens, a podcast where experts explain their work at the cutting edge of artificial intelligence. You'll hear from AI researchers, data scientists, and machine learning engineers as they get technical about the most exciting developments in their field and the challenges they're facing along the way. I'm your host, Rob Stevenson, and we are about to learn how AI happens.\n\n0:00:48.1 Rob: Today on How AI Happens, we're going to talk about the next generation of AI specialists. While this might bring to mind the image of a starry-eyed Stanford PhD or a youthful self-taught prodigy, I'm thinking of a different source of talent, because over the last several years, Sama has been delivering a training program entitled Artificial Intelligence 101 to eager individuals in Kibera, the largest slum in Africa, just outside Nairobi, Kenya. To learn more about how an AI company can train technical talent while simultaneously taking aim at systemic poverty, I sat down with Wendy Gonzalez, CEO of Sama.\n\n0:01:29.5 Wendy: I have, I'm a little embarrassed to say, over 25 years of experience [chuckle] in enterprise technology and SaaS and AI. I started my career in management consulting, really helping large companies figure out how to leverage technology. I switched over to the enterprise side, working to really implement disruptive technologies. And then I co-founded an Internet of Things startup about a decade back to build a SaaS platform, because again, IoT was a way to really disrupt the industry, and then I switched over to AI. So I joined Sama in 2015, and Sama is the leading trainee data as a service platform. We work with the Global 2000 to really help enable their mission-critical AI applications.\n\n0:02:13.1 Wendy: I was really compelled to join Sama because I believe deeply in Sama's mission, and really, our philosophy that Sama was founded on is that talent is distributed equally, but opportunity is not. And the best way to solve poverty, which is basically the root cause for every major social ill in the world is by giving work, not giving aid. Over a trillion dollars' worth of aid has been donated to sub-Saharan Africa since the 1960s, yet, GDP hasn't changed. While it seems like the right thing to do, the best thing that we can do to really solve the problem of poverty is by providing financial independence, and that's through giving work. And so, Sama's mission of purposely hiring people in underserved communities to give work was something that really spoke to me, because as a child of immigrants and marrying my husband who was the first person in his entire family through all generations to go to school, the power of work is transformative in terms of lifting up your community and lifting up the people in your family.\n\n0:03:11.4 Wendy: So that mission spoke very, very deeply to me. Sama had a really unique and audacious view on this, which is, it's not just about kind of paying living wages and providing employment and benefits; it's about purposely hiring people who've got the greatest barriers to employment. And so, Sama's model is to hire, in underserved communities, 50% women, 50% youth who have household incomes of less than $2 a day, which is the World Bank standard for poverty. And so I was fascinated with this idea of taking purposeful action and change to hire people and not just sort of provide wages, but really provide a transformative career path to hopefully break the poverty cycle permanently.\n\n0:03:52.1 Rob: This notion of giving work rather than aid has taken the form of Sama's Digital Basics Program, that's the AI 101 course I mentioned earlier. As Wendy explains, this training is the first step in removing the barriers to access between eager, underserved workers and well, work.\n\n0:04:12.8 Wendy: One of the challenges is not just about, \"Hey, I'm in this situation, I didn't graduate from a fancy school or college,\" it's also by just having access to the network to get jobs. So I say barriers to employment, it's not just kind of education and where I live, but it's like, \"Do I know the right people, how do we even get connected to a job?\" So the Sama Digital Basics Program really started by working with community partners in underserved communities, I'll give you an example in Nairobi, to where we work with partners like the Human Needs project that has a facility that is embedded in the Kibera slum, which is the largest slum sub-Saharan Africa. So, it was an idea to bring digital skills training into communities, so that, again, to provide and reduce that barrier to access.\n\n0:04:57.7 Wendy: And so what the Digital Basics Program does is it provides everything from basic skills like mounting and keyboards, but also the basics of AI. So what is artificial intelligence, what role does training data play in empowering artificial intelligence applications, and it's really the initial training that is necessary for somebody to come and join the Sama program.\n\n0:05:20.2 Rob: It should be pointed out here that the goal of the Digital Basics Program isn't merely to create a farm team of future Sami employees, though many of them do wind up working there; rather, the skills that go along with data labeling end up providing individuals with a much wider breadth of opportunity.\n\n0:05:38.4 Wendy: It's not just about building training for the purpose of, \"Okay, now you can do data labeling,\" what we found, if they're doing this program 'cause we were actually launched in 2008, is that the skills necessary to do labeling and tagging are critical thinking skills that actually apply to many different jobs on a go-forward basis. Our intention was always not just hiring people into Sama and you'll be with Sama for the rest of your lives; it was really about building the skills that allow our workers to go on to higher-paying jobs, return to university. That's the entire idea of when I say permanently breaking the poverty cycle, is to build the foundation so that people can move on to higher-paying jobs.\n\n0:06:16.1 Wendy: So while we have a pathway, of course, to move up within Sama, the other objective of this is to build a core set of technical skills. So the training typically occurs within community or at our offices, and then of the people who are trained, some go on and move on to other jobs, many come and apply at Sama. After doing this level of training, we would hire people in as employees, so we're different, we are not freelancing, we are not crowdsourcing. A part of our mission is to provide living wages, benefits, and professional development that'll allow people to further their careers. And so they get hired into Sama, and typically, they will do data labeling as an entry point. And so data labeling, just to take us back a little bit in terms of artificial intelligence, is that...\n\n0:07:03.7 Wendy: You think about it this way, AI is as intelligent as the trainee data it's built on, because machine learning is all about recognizing patterns, then using deep learning techniques for the application to make decisions. Put another way, before a car can drive itself, like a self-driving car, it needs to be able to detect roads, pedestrians, vehicles, and traffic signs, and training data is basically structuring the data so that a computer vision application can identify, what is a car, what's a drivable space, what's a road sign, et cetera. And while that sounds like it's relatively simple, it's actually incredibly, incredibly complex. Some of these data labeling activities include data labeling in 3D. I don't know if you've ever seen radar or light or images, it's very, very complex. And then beyond that, it's not just about detecting and after being that information correctly; it's about the precision of it. And so, tagging a car is not just tagging a car, sometimes you need to include the side-view mirrors, maybe you need to include the shadow under the car, maybe you do need to include what's behind the car, maybe you don't.\n\n0:08:05.3 Wendy: So there's actually quite a bit of complexity, and so, in terms of the types of work and the skills that are being built, you have to tie it back to a taxonomy, there could be business rules. So, a part of what's being developed as critical thinking skills as well.\n\n0:08:22.9 Rob: As I mentioned before, this program has been underway for years, so, this podcast episode isn't meant to be an audio press release. The reason I wanted to bring Wendy on to discuss the program is because of a recent study measuring the effects of these efforts, a six-year randomized control trial conducted by MIT and Innovations for Poverty Action, a research and policy nonprofit promoting effective solutions to global poverty. Wendy explained some findings from the report, and whether those findings were in line with initial goals.\n\n0:08:55.4 Wendy: The thing that we are really trying to understand or MIT was trying to understand is this purposeful hiring model that this part of Sama's mission, that's core to the way that we operate, is that intervention of purposely hiring somebody from an underserved community, does that actually improve their employability and their income rates in the long term? Does it actually break the poverty cycle that I was talking about before, or would these people just would have succeeded anyways?\n\n0:09:18.5 Wendy: So when we talk about really, how do you measure that, it's called a counterfactual study, which, when friends ask, \"Well, what do you mean? What is an RCT? Why did you do this study\", it's kind of like, the FDA does it for drug approvals, right? Somebody takes a placebo, somebody takes the medicine, and then you find out at the end, well, did the medicine really work? And so that's really what we're trying to do here. Did this purposeful intervention of hiring, did it make a difference in somebody's employability and growth and income?\n\n0:09:46.0 Wendy: The study wasn't just a matter of, \"Hey, let's take some surveys.\" It's actually even something in the works for six years. So we plan for a year, a year and a half, to identify folks and create a very detailed study, three years of actually surveying people on a regular basis. So, fun fact, over 2000-plus hours of surveying time [chuckle] and interviews and calls. But at the end of the day, the idea was to say, \"Did the intervention work? Is there a meaningful and material difference, and do we have all over the data over time to prove it?\" I believe all of our internal tracking that yes, we were, so it was amazing to get that ratified, that, yes, indeed, over the long term, this purposeful hiring model made a huge difference in terms of income and employment levels. But in particular for women, that was the thing that was really exciting to learn, and that was a little bit surprising, is that Sama's model has been to create a purposeful higher model of 50% women and 50% youth.\n\n0:10:48.0 Wendy: And we focused there from a mission perspective, because what I think has been well-researched and understood is that when women are lifted up in the economy and when women are lifted up in income, they contribute back to their communities, more kids go to school, there's a real network effect of investing in women. And so that was part of our purposeful hiring model, to where we have a criteria in hiring at least 50% women, which is a little bit unique, and again, purposeful. And what we found is that after this three-year time frame, there was a 60% improvement in employability and income rates for women who went through the Sama intervention versus who did not. And that was really exciting. Really, really exciting to see that that basically means that our hypothesis that women have barriers to entry and employment is indeed true, but that the purposeful hiring model at the beginning of their careers actually makes a difference in them not only having greater income but continuing to stay employed.\n\n0:11:46.5 Rob: When you look at the impact these studies showed with women specifically, was that surprising or just delighting?\n\n0:11:54.7 Wendy: It was delighting. We did a lot of community surveys before launching this program, because let's not just assume we know what people need, that's the entire point of financial independence, let's hear from them what is needed, and what we found is that, in particular, it was just really challenging for women to get into the right networks, to get jobs. And so, this was the model between that and knowing that the impact that women can have when they are lifted up, but I think what was surprising is not just the income levels, but I think what surprised me was the employment levels. I mean, 60% is significant. So, yeah, I was delighted, [chuckle] to answer your question.\n\n0:12:34.3 Rob: Of course, the ability to do a job and the ability to get a job are vastly different skills. This was reflected in the report, as researchers found subjects who did not receive employment placement assistance had a much harder time finding work, even if they had the same technical training as others. So, does that mean mere up-skilling is not enough?\n\n0:12:57.2 Wendy: I think what that speaks to is that while training is incredibly important, that action, the purposeful hiring, is really kind of what moves the needle. I'm all for providing the training, and I think what we found is that what we're trying to do is, in addition to the core skills training, is gonna be, how do we help people match themselves to jobs, and there's some pretty incredible organizations out there who are specializing in this area, so we don't assume we've gotta figure it all out. [chuckle] We are working with partners to help us on things like the skills matching. But the key thing, you have to make those purposeful choices, so, oftentimes, I talk to our customers about the ethical AI supply chain, or really how you use impact criteria as part of how you make your buying decisions. And I think that's something that's incredibly important, because I know we're going to be a proof point and move tens of thousands and over time, hundreds of thousands, of people out of poverty, but imagine the world's biggest corporations are spending trillions of dollars in procurement. Imagine what they can do if they make those same purposeful hiring decisions. We can leave millions and tens of millions of people out of poverty.\n\n0:14:08.6 Wendy: We're doing this at Sama, but I would love for every company in the world to take the same approach, and if you're not in a position to make these purposeful hiring decisions, well, work with suppliers, use social impact criteria as part of how you make your buying decisions. So the more we can get this out there, that, \"Hey, the model works,\" makes a meaningful difference, it also has tremendous business benefits as well. What happens for us, we have incredible retention, and that has created more value in our AI platform. So there are many, many different reasons to take this approach, and from a social mission standpoint, we can build incredible technology, create incredible value from our products, and we can change lives at the same time.\n\n0:14:51.5 Rob: What do you foresee in terms of additional education beyond data labeling? When you look at further development for their up-skilling, do you foresee that being an offering over time?\n\n0:15:00.8 Wendy: Oh, yeah, absolutely. I love that you mentioned that. I hate to sound buzz wordy, if you will, but I mean, data is the new code. The skills that are being built here aren't just for, \"Okay, I can label\"; it's really about building not just those critical thinking skills, but the way that we move our workforce up the value chain is that they are building analytical skills. So gone are the days where our workforce would come in and be like, \"Oh, is there a dog in this picture?\" or something like that. They're doing way more sophisticated work, doing everything from training machine learning models and driving very sophisticated training data sets, to evaluating and quality sharing training data sets that have been produced by automation, all the way to identifying what's missing, do we have a representative and complete data set? So it's really about moving from labeling and taxonomies and workflows to analytics and beyond. So, yeah, there's a lot of exciting work ahead, and beyond the analytics and management, yes, data science, that is the next frontier.\n\n0:16:08.7 Rob: If data science is the next frontier, then any company seriously deploying it has an awesome responsibility, not just to make a great product, but to utilize this technological position to up-skill the next generation of talent, and do so in such a way that ensures representative, more diverse, more creative future for our industry.\n\n0:16:31.6 Wendy: Next time on How AI Happens:\n\n0:16:38.5 Speaker 3: There's a pretty substantial rules-based expert system that sits on top of this to help manage some of the downsides of the inherent biases we have in the data.\n\n0:16:53.0 Wendy: How AI happens is brought to you by Sama. Sama provides accurate data for ambitious AI, specializing in image, video, and sensor data annotation and validation for machine learning algorithms in industries such as transportation, retail, e-commerce, media, med tech, robotics, and agriculture. For more information, head to sama.com.","seo_title":"New Podcast Episode: Sama CEO Wendy Gonzalez on Upskilling Talent from Developing Nations","slug":{"_type":"slug","current":"podcast-episode-wendy-gonzales"},"tags":[{"_key":"lSFpOak0","label":"Impact","value":"Impact"},{"_key":"hJS7rXT4","label":"Podcast","value":"Podcast"}],"title":"New Podcast Episode: Sama CEO Wendy Gonzalez on Upskilling Talent from Developing Nations"},{"_createdAt":"2021-06-08T14:17:40Z","author":{"_id":"10ead718-57e1-41a8-b846-da3c81cc323a","avatar":{"_type":"image","asset":{"_ref":"image-a4c79da81bb1e23ce10fba84ea2cba5efe67a2a5-200x200-webp","_type":"reference"}},"bio":"Currently a Director of Product Management at Sama, Saul is passionate about the intersection of technology and social impact. He manages Sama’s data labelling products to ensure high quality training data efficiently and reliably reaches our customers. Experienced in both product and professional services, Saul is a proven leader who takes a data driven approach to expanding Sama’s capabilities and features. When not at work, you can usually find Saul enjoying the outdoors and spending time with his family.","name":"Saul Miller","slug":{"_type":"slug","current":"saul-miller"}},"estimatedReadingTime":3,"featured_image":{"_type":"image","asset":{"_ref":"image-cb41c540ba989cd0258ef2f70cf6ce1bc99f2793-3334x1668-png","_type":"reference"}},"meta_description":"After a decade of successful text annotation projects, we’re launching our ML-powered Natural Language Processing Annotation Tool.","openGraphImage":null,"plaintextBody":"TLDR; After a decade of successful text annotation projects, we’re launching our ML-powered NLP Annotation Tool.\n\n\n\nIf you work in AI, you know just how hard Natural Language Processing (NLP) problems can be to solve. Though the study of NLP dates back to the 1950s, AI practitioners and researchers are still grappling with the many ambiguities and complexities of language.\n\nHumans intuitively know that words can have different meanings depending on context, and that they can acquire new meanings over time. But capturing all these nuances and incorporating them into an ML model is about as difficult as it sounds.\n\nFor every additional variable that must be accounted for in a model, there is an equal number of opportunities for NLP models to fail. Even a small margin of error can have huge consequences. A lack of varied perspectives or proper inputs can corrupt a data set, create bias, and have serious downstream consequences.\n\nThe only way to navigate this ambiguity is with large, high-quality data sets annotated by a diverse set of labelers.\n\nQuickly bring high-performing NLP models into production\n\nTraditional approaches to labeling Natural Language Processing data – especially using crowdsourced and self-service labeling platforms – often slow the path to production. Inexperienced labelers and the lack of a continuous feedback loop can result in poor quality data sets and time-consuming iteration cycles.\n\nWith Sama, you can get started quickly with self-service and scale over time, or work with our directly managed workforce of annotators trained on your specific needs and best practices for your industry. This bespoke approach enables us to deliver high-performing data sets for your projects, so you can analyze and draw insight from unstructured data for a multitude of NLP use cases – from product review analysis, to document summarization and understanding, to misinformation detection and much more.\n\n\u003e\u003e Learn more about how Sama can help make training data for NLP your competitive advantage\n\n\nOur ML-assisted labeling platform uses active learning to quickly share high precision predictions, automating away simple labels and freeing annotators to create more and higher quality labels. Built-in QA and human in the loop capabilities allow edge cases to be raised early in the process for quicker iteration cycles.\n\nThis powerful combination of skilled annotators and an AI-powered platform allows us to deliver a high standard of label quality to our customers every time, along with efficiency improvements and quicker time to market.\n\nLaunch your Natural Language Processing projects more quickly and efficiently\n\nThe field of Natural Language Processing has made significant progress in the last few years – but there’s plenty of distance still to go.\n\nWherever you want to go next with NLP, let Sama get you there faster.\n\n\u003e\u003e Learn more about how Sama can help make training data for NLP your competitive advantage","seo_title":"Quickly Bring Your NLP Models into Production with AI-Powered Annotation","slug":{"_type":"slug","current":"natural-language-processing-annotation"},"tags":[{"_key":"WOb8dgoC","label":"NLP","value":"NLP"}],"title":"Quickly Bring Your NLP Models into Production with AI-Powered Annotation"},{"_createdAt":"2021-05-27T16:50:20Z","author":{"_id":"e8a1f3c6-0b07-4638-93d5-61a0924f32e3","avatar":{"_type":"image","asset":{"_ref":"image-2f8efc599a377bd580d0ae12f022a15e7e1acacd-632x676-webp","_type":"reference"}},"bio":"Amanda is the Content Marketing Manager at Sama, where each day she gets to learn about AI and how it’s transforming industry. She’s passionate about technology but periodically tries to get away from her computer to explore more analog things like collage and quilting.","name":"Amanda Durepos","slug":{"_type":"slug","current":"amanda-durepos"}},"estimatedReadingTime":2,"featured_image":{"_type":"image","asset":{"_ref":"image-1502043edcf701cb4267cb2f6d7c6edf8e0e0cad-2500x1313-png","_type":"reference"}},"meta_description":"How AI Happens is a podcast by Sama featuring experts and practitioners explaining their work at the cutting edge of AI/ML.","openGraphImage":null,"plaintextBody":"How AI Happens is a podcast featuring experts and practitioners explaining their work at the cutting edge of AI/ML. Tune in to hear AI Researchers, Data Scientists, ML Engineers, and the leaders of today’s most exciting AI companies explain the newest and most challenging facets of their field.\n\n\n\nEvery day, most of us will touch AI in some way — whether at work, at home, or strapped to our wrists and in the palm of our hands. Applications of ML have seamlessly found their way into our lives and our businesses, thanks to a select few who have set out to reimagine how we work in virtually every industry.\n\nMany may understand how AI impacts their daily lives, but few know how to effectively build it.\n\nIn Sama’s new podcast, How AI Happens, we sit down with AI Researchers, Data Scientists, ML Engineers and the leaders of today’s most exciting AI companies to discuss the newest and most challenging facets of their field. \n\nFor our inaugural episode, we sat down with Adnan Khaleel, an HPC and AI expert from Dell. Adnan explains how Dell’s HPC customers are scaling out their AI implementations, how they balance speed vs. accuracy, and explores the parallelization of a radiology algorithm built to detect anomalous cells.\n\nListen and subscribe to How AI Happens on Apple Podcasts, Spotify, Amazon, Google Podcasts, or Overcast.\n\nHappy listening!","seo_title":"How AI Happens: A Podcast by AI Practitioners for AI Practitioners","slug":{"_type":"slug","current":"how-ai-happens-podcast"},"tags":[{"_key":"TpMM6FVr","label":"Machine Learning","value":"Machine Learning"},{"_key":"sR9wjqUS","label":"AI Practitioners","value":"AI Practitioners"},{"_key":"5AlBk2XP","label":"Podcast","value":"Podcast"}],"title":"How AI Happens: A Podcast by AI Practitioners for AI Practitioners"},{"_createdAt":"2021-05-18T15:19:55Z","author":{"_id":"a009d418-aa96-47ac-a73a-fd2cd52c79d9","avatar":{"_type":"image","asset":{"_ref":"image-e0d717f753ba4876a6b0dbf9f125cf6c3d27e545-500x500-webp","_type":"reference"}},"bio":"Wendy Gonzalez is an executive passionate about building high-performing, high-functioning teams that develop and scale innovative, impactful technology. With two decades of managerial and technology leadership experience for companies including EY, Capgemini, Cycle30 (acquired by Arrow Electronics) and General Communications Inc, Gonzalez is currently the CEO of Sama, the provider of accurate data for ambitious AI, used by leading technology companies such as Walmart, Google, Nvidia and Getty. Before taking on her role as CEO, Gonzalez spent 5 years at Sama as COO, and is an active Board Member of the Leila Janah Foundation.","name":"Wendy Gonzalez","slug":{"_type":"slug","current":"wendy-gonzalez"}},"estimatedReadingTime":3,"featured_image":{"_type":"image","asset":{"_ref":"image-b5f0cb379e97750335d17d53062508bf6115d4e7-3334x1668-png","_type":"reference"}},"meta_description":"This week, researchers at MIT released a white paper evaluating Sama's impact through a three-year Randomized Controlled Trial study. Here are their findings.","openGraphImage":null,"plaintextBody":"I am excited to announce that this week, researchers at the Massachusetts Institute of Technology (MIT) released a white paper evaluating our impact through a 3-year Randomized Controlled Trial (RCT) study. On the heels of this achievement, I am reminded of Leila Janah, my dear friend and the late founder of Sama.\n\n\n\nLeila’s dream was to conduct an RCT, often referred to as the “gold standard” in research. An RCT is an independent study, in which a third party validates the impact of your social model. We had long known we were creating impact through our own data collection, however, in 2016, we had reached a point where we wanted to have it validated.\n\nThis led to a joint effort with researchers at MIT, and with Innovations for Poverty Action (IPA), to launch a study from 2017 to 2020 to evaluate the effectiveness of our training and employment programs in creating sustainable pathways out of poverty in Nairobi, Kenya. It studied three groups of individuals from similar socioeconomic backgrounds that were randomly assigned to receive training through Sama (Group 1), or receive training and the opportunity for employment at Sama (Group 2), or receive neither training nor the opportunity for employment (Control Group).\n\n(See IPA’s project summary for an easily digestible explanation of the impact evaluation.)\n\nOver the course of the three-year study, individuals reported their employment status, earnings, and the industry they worked in. They also shared insights about their current and future outlook on aspects ranging from community engagement to health to life satisfaction. The main finding of the study is that individuals who were trained by and had the chance to work at Sama (Group 2) were more likely to experience better outcomes than other groups. By the end of the study, average earnings for individuals in Group 2 were almost 40% higher than those in the Control Group, and unemployment rates were 10% lower. Training and the opportunity to work at Sama had an even greater impact on women. Average earnings for women in Group 2 were 60% higher than women in the Control Group.\n\nThe findings reinforce what our internal research has long shown: that our impact hiring model has a positive and meaningful effect on long-term employment rates and earnings. To have MIT validate our model proves that business can be a force for social good. And while it takes conviction to run a purposeful hiring model, we are driving an ethical supply chain that meaningfully improves employment and income outcomes for those with the greatest barriers to work.\n\nIt is my hope that these findings will contribute to larger sustainable development discussions across the public and private sector and in particular will drive corporations who spent trillions each year on their supply chain to utilize impact criteria as part of their decision-making process. Think of the impact we can drive then.\n\nRead the Paper: Evaluating Sama’s Training and Job Programs in Nairobi, Kenya\n\n\n\nAuthors:\nDavid Atkin (MIT) \nAntoinette Schoar (MIT Sloan) \nKiara Wahnschafft (MIT)\n\nRead More Key Stats from the RCT","seo_title":"RCT Results from MIT: Evaluating the Impact of SamaÃƒÂ¢Ã¢â€šÂ¬Ã¢â€žÂ¢s Training and Job Programs","slug":{"_type":"slug","current":"rct-results-mit"},"tags":[{"_key":"X2bQOmTh","label":"Impact","value":"Impact"}],"title":"RCT Results from MIT: Evaluating the Impact of SamaÃƒÂ¢Ã¢â€šÂ¬Ã¢â€žÂ¢s Training and Job Programs"},{"_createdAt":"2021-05-10T15:00:00Z","author":{"_id":"e8a1f3c6-0b07-4638-93d5-61a0924f32e3","avatar":{"_type":"image","asset":{"_ref":"image-2f8efc599a377bd580d0ae12f022a15e7e1acacd-632x676-webp","_type":"reference"}},"bio":"Amanda is the Content Marketing Manager at Sama, where each day she gets to learn about AI and how it’s transforming industry. She’s passionate about technology but periodically tries to get away from her computer to explore more analog things like collage and quilting.","name":"Amanda Durepos","slug":{"_type":"slug","current":"amanda-durepos"}},"estimatedReadingTime":6,"featured_image":{"_type":"image","asset":{"_ref":"image-df166412c7473ff8223b8e439246245a9dc8ce2d-3334x1668-png","_type":"reference"}},"meta_description":"Tribe Dynamics helps customers get better ROI from influencer programs. Find out how partnering with Sama helped Tribe better serve clients and expand into new markets.","openGraphImage":null,"plaintextBody":"Tribe Dynamics launched in 2012 to help countless leading global brands operationalize, scale, and measure their internal influencer programs—all in a single influencer marketing software platform. Since 2017, the Sama team has powered this technology, training Tribe’s machine learning algorithm and increasing reporting capabilities by 350% to date, helping to build robust influencer communities for a fast-growing, global portfolio of brands.\n\nWhile influencer marketing isn’t a new concept, it has become wildly popular in the past few years. It’s a type of marketing that uses endorsements and product mentions from influencers—tastemakers who have a dedicated social following and are viewed as experts within their niche. Influencer marketing works because of the high amount of trust that influencers have built up with their following. Recommendations from influencers also serve as a form of social proof to a brand’s potential customers.\n\nBut how do you measure who talks about you and how you compare against competitors?\n\nThis is where Tribe Dynamics comes in. The universe of what they track consists of two parts. First, a growing database of over 2,500 brands, including social handles and colloquial ways of discussing the different brands. You can imagine brands missing out on key conversations if they fail to capture slang or abbreviations like #ABH, referring to the popular cosmetics company Anastasia Beverly Hills. The second part involves the influencers who live in their ambassador relationship management system. Here, the profiles and online activities of 200,000 influencers are stored and sorted by the various industries and interests. When combining this list of brands with the group of influencers, Tribe delivers a powerful collection of social posts mentioning your brand—one that truly identifies your most passionate influencer community. \n\n\n\nThe Challenge\n\n\n\nTribe Dynamics’ direct users are marketing teams tasked with measuring the ROI of their influencer program. They want to track their share of the conversation versus their market share benchmark to assess how well they’re doing against competitors and identify their most effective influencers. In short, they need Tribe Dynamics to help them cut through the noise and make data-driven decisions.\n\n\n\nHere’s an example; if a post is talking about Benefit Cosmetics and the company is tagged as such, it’s typically an easy find. However, it becomes more complicated when the algorithm needs to detect the difference between “I love this blush from Benefit” and “The benefit of using blush,” where one is a brand mention and the other a tutorial.\n\nMarketing teams also need to be able to judge the quality of an influencer’s performance and avoid spending budget on fraudulent influencers. Tribe Dynamics allows them to see who their top drivers are for Earned Media Value (EMV), a proprietary metric that measures the value of third-party digital content created about a brand, and surface the right influencers. After all, the hardest working influencers may be people they don’t even know about yet.\n\nTribe Dynamics’ machine learning algorithm catches 95% of the mentions for some brands and 60% for others, and the team relies on Sama as part of their ecosystem to deliver full accuracy.\n\n\n\nDownload the full case study here. \n\n\nThe Solution\n\n\n\nSama’s expert annotators assume the tasks of locating, extracting, and tagging key brand mentions in the posts generated by Tribe Dynamics. This human-in-the-loop intervention consistently reaches a quality SLA of \u003e99%, enabling the algorithm to become more intelligent over time.\n\nSama handles 60-70% of the monthly vetting tasks: identifying brand mentions and common language ways of talking about a brand that the algorithm isn’t yet trained on.\n\nThis continuous vetting also helps overcome model drift. In the fast-moving world of social media, training a machine learning model is not a single, finite stage in the process. Even after it’s deployed in a production environment, this steady stream of new training data—and continuous vetting—ensures the model’s predictive accuracy over time.\n\nIts success shows in the numbers:\n\nThe volume of data Tribe Dynamics is able to provide their clients has grown almost 4x—from 700 brands in 2018 to 2,600 brands this year.\n\nSama has been key to this growth.\n\n\n\nA Word from Tribe Dynamics\n\n\n\nAs almost all of the expansion involves vetting, Sama has been a valuable resource in Tribe’s international expansion.\n\n“We’ve launched in over a dozen markets in two to three years. That, without support, would’ve crushed our internal teams. Sama was able to take on additional workload quickly and efficiently. It has been a notable piece of our successful global business,” says Clare Bruzek, VP of Operations.\n\nSama has become part of the architecture of the company. When Tribe Dynamics decides to expand into different markets, Sama is integral in the scoping of these new opportunities. “It’s the only sustainable vetting option for our international expansion. We now provide international data on 1,250 brands to 94 clients across 12 global markets, making up 30% of our revenue.”\n\nBruzek shares:\n\n“Working with Sama has made a demonstrable impact in our ability not only to service our current clients better, but also to expand our services to new types of clients and new markets. We have only been able to meet client needs in that way because of what Sama has been able to achieve.”\n\nDownload the full case study here. ","seo_title":"How Sama Powers Tribe Dynamics to Measure Your Influencer Marketing Efforts","slug":{"_type":"slug","current":"tribe-dynamics-case-study"},"tags":[{"_key":"a68HDnn3","label":"Case Studies","value":"Case Studies"}],"title":"How Sama Powers Tribe Dynamics to Measure Your Influencer Marketing Efforts"},{"_createdAt":"2021-05-05T16:24:03Z","author":{"_id":"26fb3cc4-608d-40e1-bb4d-955bceda232a","avatar":{"_type":"image","asset":{"_ref":"image-f1fd7fbcc4633299cdbedddba22cb44e24f17317-518x518-svg","_type":"reference"}},"bio":"From self-driving cars to smart hardware, Sama fuels AI. Founded over a decade ago, we’re experts in image, video and sensor data annotation and validation for machine learning algorithms in industries including automotive, navigation, AR/VR, biotech, agriculture, manufacturing, and e-commerce. Our staff are driven by a mission to expand opportunity for low-income people through the digital economy, and our social business model has helped over 50,000 people lift themselves out of poverty.","name":"Sama Team","slug":{"_type":"slug","current":"sama-team"}},"estimatedReadingTime":3,"featured_image":{"_type":"image","asset":{"_ref":"image-6cb6375fa1b8e381f1db8152440adfb093b0b60f-1650x1275-png","_type":"reference"}},"meta_description":"We’re proud to announce our honorable mention in Fast Company’s 2021 World Changing Ideas Awards for AI and Data alongside leaders such as CNN and Immunai.","openGraphImage":null,"plaintextBody":"At Sama, we’ve understood the potential that AI and data have to reshape our world since day one. Already used to power essential services like surgery, vaccination development and transportation, AI is becoming more accessible every day. That’s why we’re committed to helping the world’s most ambitious AI teams develop the diverse and accurate data sets they need to ensure their algorithms succeed.\n\n\n\nToday, on the heels of our recent inclusion in the 2021 Forbes AI 50 list, we’re proud to announce our honorable mention in Fast Company’s 2021 World Changing Ideas Awards for the AI and Data category alongside leaders such as CNN, Immunai and Zoox. This is Sama’s second consecutive World Changing Ideas win, following our finalist recognition as a result of our animal conservation and anti-poaching work with Vulcan in 2020. \n\n\n\n“There is no question our society and planet are facing deeply troubling times. So, it’s important to recognize organizations that are using their ingenuity, impact, design, scalability, and passion to solve these problems,” says Stephanie Mehta, editor-in-chief of Fast Company. “Our journalists, under the leadership of senior editor Morgan Clendaniel, have discovered some of the most groundbreaking projects that have launched since the start of 2020.”\n\n\n\nThis year’s recognition highlights our AI Bias Detection solution, which was created to detect and combat systemic bias in artificial intelligence. In addition to causing inefficiencies, bias can result in dangerous algorithm outcomes across hiring, safety and criminal justice. Understanding that AI will be critical to societal change moving forward, our AI Bias Detection solution offers advanced analytics and reporting capabilities to spot and correct bias before it’s implemented.\n\n\n\n“It’s an honor to be recognized in Fast Company’s World Changing Ideas this year alongside so many incredible organizations,” says Wendy Gonzalez, CEO of Sama. “At Sama, we’re dedicated to using AI and training data for good and will continue to leverage our expertise to create the most effective, safe and equitable future of technology possible.”\n\n\n\nNow in its fifth year, the World Changing Ideas Awards honor the businesses, policies, projects, and concepts that are actively engaged and deeply committed to pursuing innovation when it comes to solving health and climate crises, social injustice, or economic inequality. This year, a panel of eminent Fast Company editors and reporters selected winners and finalists from a pool of more than 4,000 entries across transportation, education, food, politics, technology, and more.\n\n\nTo learn more about our recognition and to see the complete list of Fast Company’s World Changing Ideas honorees, click here.","seo_title":"Honorable Mention in Fast CompanyÃƒÂ¢Ã¢â€šÂ¬Ã¢â€žÂ¢s 2021 World-Changing Ideas Awards","slug":{"_type":"slug","current":"honorable-mention-in-fast-companys-2021-world-changing-ideas-awards"},"tags":[{"_key":"WOaBRJBK","label":"Company News","value":"Company News"},{"_key":"JY3FKCos","label":"Ethical AI","value":"Ethical AI"},{"_key":"JhqG8b1a","label":"Awards","value":"Awards"}],"title":"Honorable Mention in Fast CompanyÃƒÂ¢Ã¢â€šÂ¬Ã¢â€žÂ¢s 2021 World-Changing Ideas Awards"},{"_createdAt":"2021-05-03T21:44:34Z","author":{"_id":"26fb3cc4-608d-40e1-bb4d-955bceda232a","avatar":{"_type":"image","asset":{"_ref":"image-f1fd7fbcc4633299cdbedddba22cb44e24f17317-518x518-svg","_type":"reference"}},"bio":"From self-driving cars to smart hardware, Sama fuels AI. Founded over a decade ago, we’re experts in image, video and sensor data annotation and validation for machine learning algorithms in industries including automotive, navigation, AR/VR, biotech, agriculture, manufacturing, and e-commerce. Our staff are driven by a mission to expand opportunity for low-income people through the digital economy, and our social business model has helped over 50,000 people lift themselves out of poverty.","name":"Sama Team","slug":{"_type":"slug","current":"sama-team"}},"estimatedReadingTime":2,"featured_image":{"_type":"image","asset":{"_ref":"image-23697fb2984860fddca93d9a96e1ab91aa7bfb9c-2000x1125-png","_type":"reference"}},"meta_description":"Thanks to our industry-spanning work we are honored to announce our inclusion in the 2021 Forbes AI 50 list alongside other leading AI companies.","openGraphImage":null,"plaintextBody":"It goes without saying that this past year has had its challenges. Through it all, our team at Sama has remained committed to providing leading companies with the highest quality training data to get predictably higher accuracy in half the time. That’s because we know that AI is the key to the future of technology, and without diverse and accurate data sets, the technology will ultimately fail.\n\nThanks to our industry-spanning work with partners such as Walmart, Google and NVIDIA, we are honored to announce our inclusion in the 2021 Forbes AI 50 list alongside leading companies including Duolingo, Samsara and Narrativ. In its third year, the Forbes AI 50 highlights the most promising North American businesses using AI in ways that are fundamental to their operations.\n\nBy combining our expert-human-in-the-loop team with cutting-edge technology, we’ve put in the time and effort to perfect our machine learning-assisted annotation and achieve SLAs well above the industry standard this past year. With new innovations like our AI Bias Detection solution and milestones, including our B Corp Certification, we’ve continued to prioritize our double bottom line, measuring our success not only in terms of revenue but also impact.\n\n\"I’m incredibly proud of what our team has accomplished over the past year both in terms of our success providing high-quality data to industry leaders and maintaining our commitment to social impact,” says Wendy Gonzalez, CEO of Sama. “As AI becomes increasingly essential to operations across industries, we’re excited to have a hand in the future of effective, non-biased AI, and are grateful to be recognized as one of the leading companies in the space.\"\n\nThis recognition from Forbes reflects our continued success in leveraging AI for good. Through partnerships with customers to help hotels and restaurants achieve zero food waste, power anti-poaching and animal conservation efforts, enable safe and effective robot-assisted surgery and more, we’re consistently harnessing best-in-class technology to address the most pressing challenges in our society today. \n\nTo learn more about our achievements and to see the complete list of Forbes AI 50 winners, visit Forbes' article here.","seo_title":"Sama Recognized in the 2021 Forbes AI 50 List","slug":{"_type":"slug","current":"sama-recognized-in-the-2021-forbes-ai-50-list"},"tags":[{"_key":"RgPudZ6F","label":"Company News","value":"Company News"},{"_key":"kLwgMkEb","label":"Awards","value":"Awards"}],"title":"Sama Recognized in the 2021 Forbes AI 50 List"},{"_createdAt":"2021-04-16T00:13:26Z","author":{"_id":"88e43c38-2712-41d4-9df3-8656c163c4db","avatar":{"_type":"image","asset":{"_ref":"image-af1346669e45d7264a36f051c18aea76061725bb-1895x1894-webp","_type":"reference"}},"bio":"Kyra is passionate about world-changing tech and sustainability, and happiest when these come together (looking at you, Sama). Stereotypical Dutch, she enjoys urban cycling and eating stroopwafels.","name":"Kyra Harrington","slug":{"_type":"slug","current":"kyra-harrington"}},"estimatedReadingTime":6,"featured_image":{"_type":"image","asset":{"_ref":"image-a780ec1434b6c9c7513e0346be3e51fea3b5961b-1800x1013-png","_type":"reference"}},"meta_description":"We asked experts working in the field about their thoughts on the role of humans in Machine Learning, and humans and the future of ML.","openGraphImage":null,"plaintextBody":"If, as many assume, AI is to take over many organizational roles from humans, it would have to develop considerably from its current standing. MIT has defined ‘human-centered AI’ as “the design, development, and deployment of systems that learn from and collaborate with humans in a deep, meaningful way”. Therefore, to become less ‘human-centered’, you would need an AI landscape in which smart algorithms do all the heavy lifting. We asked experts working in the field about their thoughts on the role of humans in Machine Learning, and humans and the future of ML.\n\nRemo, Senior Software Engineer at Apple, states: “ML as a part of software engineering is by definition human-centric. It is human-centric because it substitutes the work of humans. For example in the recommendation of music, the identification of spam, or in object recognition. ML is human-centric because it is often (or almost always) trained on human generated or at least labeled data. And it is human-centric because ML models are built and designed on top of people's inductive biases about the world. At the same time it is not human (or human centric) because it does not reason or think. It spots patterns but does not \"connect the dots.\" \n\nIndu Khatri, the Machine Learning Lead at financial giant HSBC echoed the point above: “I would break down ML problems into three parts. The first one being identifying the problem to which ML needs to be applied and defining a broad architecture about how ML models will solve the problem. The second stage is developing the ML models and the third stage is taking actions based on ML models, getting the feedback from your environment and improving the models further. Out of these three stages, I believe Stage one will always need some kind of human intervention. With the advent of AutoML the amount of human intervention is decreasing everyday. Finally, for decreasing the amount of human intervention in Stage three we would need to improve the sample efficiency of Reinforcement Learning so that our Models can map predictions to actions in a feasible way.”\n\nConversely, Staff ML / NLP Research Scientist at Stanford and former Senior Research Scientist at Uber AI, Piero Molino, suggested that moving away from a human-centric model would be a mistake. “I believe there are several friction points that can be automated, like the data/model interface, the model search, evaluation and monitoring, Ops in general. But what I would rather wish is for ML to become more human-centric in the sense that it should put more humans at the center of its strategy rather than increased efficiency, and that is achieved with more humans in the loop evaluation and data generation processes, more robustness and more fairness evaluations.”\n\nData Scientist at Gartner, Lavi Nigam, thinks we’re already fairly close to an AutoML model, which as the name suggests, would mean far less human-interaction needed. So much so, that we could only be a few years away from this coming to fruition. “When more and more intrinsic pieces of data science workflows are automated mathematically and programmatically. We already see AutoML models that can figure out the best model for your data with any constraint defined. As the AutoML advances, we will eventually see that human interference in model building will greatly reduce and result in more optimized models. Deployment automation and Model Tracking (both part of MLOps) are other areas of focus where human involvement will drastically reduce in the coming years.”\n\nShuo Zhang, Senior Machine Learning Engineer, Bose Corporation, agreed that we’re fairly close to an ML model which requires little human interaction. “There are many current techniques that make ML algorithms less dependent on human supervision by leveraging the intrinsic structures of large amounts of data, such as self supervision and unsupervised techniques.”\n\nTwo other experts we asked agreed that ML as a concept requires human intervention. Removing this wouldn’t be beneficial and could actually be the opposite. Jason Gauci, a Software Engineering Manager at Facebook stated that “ML should work hand-in-hand with people, not replace them or automate the things that they do without oversight.” This was somewhat echoed by Sean Xie, Director of AI at Pfizer: “Current technologies are still focused on solving narrowly defined and specific problems. There’s a long way to go to be less human-centric.”\n\nIf we were to move toward an AutoML/less human-centric model, how could it be done? Yaman Kumar, PhD Computer Science at the University of Buffalo suggested that you would need to “join forces with the philosophy, metaphysics and ethics department and see the field adopting human-centric vision right, left and centre. As long as both AI and philosophy departments are cutoff and work in their own silos, things will go on as-is. Recent times have shown green shoots where more and more people from philosophy backgrounds are entering the field and guiding key areas such as fairness in ML.”","seo_title":"Experts Explain: How to Think About Human-Centered Machine Learning","slug":{"_type":"slug","current":"how-to-think-about-human-centered-machine-learning"},"tags":[{"_key":"pOB3rwcB","label":"Machine Learning","value":"Machine Learning"},{"_key":"v412ROa8","label":"Expert Advice","value":"Expert Advice"}],"title":"Experts Explain: How to Think About Human-Centered Machine Learning"},{"_createdAt":"2021-04-08T19:39:10Z","author":{"_id":"88e43c38-2712-41d4-9df3-8656c163c4db","avatar":{"_type":"image","asset":{"_ref":"image-af1346669e45d7264a36f051c18aea76061725bb-1895x1894-webp","_type":"reference"}},"bio":"Kyra is passionate about world-changing tech and sustainability, and happiest when these come together (looking at you, Sama). Stereotypical Dutch, she enjoys urban cycling and eating stroopwafels.","name":"Kyra Harrington","slug":{"_type":"slug","current":"kyra-harrington"}},"estimatedReadingTime":8,"featured_image":{"_type":"image","asset":{"_ref":"image-d6c18c74368eef76cb0a2effb3afd42265b0f90f-1800x1013-png","_type":"reference"}},"meta_description":"How do you define training data quality and measure it? How do you improve it? We go into defining, measuring, and reviewing your training data quality.","openGraphImage":null,"plaintextBody":"Data quality plays a crucial role in Machine Learning (ML) models’ performance and directly leads to your projects succeeding, failing, or going off-budget. “Garbage In Garbage Out” is a phrase commonly used in the machine learning community, which means that the training data quality ultimately determines the model’s quality.\n\nOur experience working with companies using ML has shown that the best models are based on comprehensive datasets, complete with a range of detailed labels. Unfortunately, many decision-makers still underestimate the time and resources needed to create, manage, and annotate datasets. Indeed, creating quality datasets is often one of the most expensive and time-consuming elements of building a machine learning application.\n\nBut how do you define data quality and measure it? Furthermore, how do you improve it? The answers depend on the type of problem you’re solving.\n\n\nDefining Training Data Quality in Annotation\n\nIn data annotation, we often speak of “accuracy” and “consistency”. Accuracy in data labeling measures how close the labeling is to ground-truth or how well the labeled features in the data are matching real-world conditions. Consistency refers to the degree of accuracy across the overall dataset. Are annotations consistently accurate across your datasets? Other characteristics of data quality may include completeness, integrity, reasonability, timeliness, uniqueness, validity, and accessibility.\n\nThe path to high-quality, scalable data quality always begins with a deep understanding of your project requirements, allowing you to develop well-defined annotation criteria against which to measure quality. Experienced AI-driven organizations often establish a quality rubric that describes what quality means in the context of a project. Sama’s approach is to anticipate the errors that we might see in a task and assign numerical penalty values before the annotation process starts.\n\nThe main benefit of this approach is to ensure that both your company and Sama’s team are on the same page about how quality is defined and measured. This allows our experts to create comprehensive and actionable instructions without room for interpretation and inconsistencies, saving a lot of time down the road.\n\nMeasuring Training Data Quality\n\nSeveral methods exist to help companies measure data quality. To define the correct annotation of given data, you want to start by creating annotation guidelines. On top of proposing a multi-level quality checks system, Sama’s experts have built unique know-how in efficiently designing annotation guidelines that enhance data quality.\n\nHere are some of the more common data quality measurement processes:\n\n\n1. Benchmarks or gold sets help measure how well a set of annotations from a group or individual matches the vetted benchmark established by knowledge experts or data scientists. Benchmarks tend to be the most affordable QA option since it involves the least amount of overlapping work. Benchmarks can provide a useful reference point as you continue to measure your output's quality during the project. They can also be used as test datasets to screen annotation candidates.\n\nAt Sama, Gold Tasks are used in two more ways: During training, to assess annotators and identify those ready to move into production, and once in production to generate an automated metric on quality. You can read more about Sama’s approach to gold tasks here.\n\n2. Consensus measures the percentage of agreement between multiple human or machine annotators. To calculate a consensus score, it is necessary to divide the sum of agreeing labels by the total number of labels per asset. The goal is to arrive at a consensus decision for each item. An auditor typically arbitrates any disagreement amongst the overlapped judgments. Consensus can be either performed by assigning a certain number of reviewers per data point or be automated.\n\n3. Cronbach's alpha test is an algorithm used to measure the average correlation or consistency of items in a dataset. Depending on the characteristics of research (for instance, its homogeneity), it may help quickly assess the labels’ overall reliability.\n\n4. Review is another method to measure data quality. This method is based on the review of label accuracy by a domain expert. The review is usually conducted by visually checking a limited number of labels, but some projects review all labels. Sama enables companies to easily review quality through a sampling portal: a dedicated portal providing full transparency and accountability on data quality. Your team can get full transparency on the batch’s quality and provide direct feedback to data trainers.\n\nDue to the iterative machine learning model testing and validation stages, we must keep in mind that data quality can change during a project. As you train your model or after making your solution live, you’ll probably find patterns in your inaccuracies or identify edge cases that will force you to adapt your dataset.\n\nReviewing Training Data Quality\n\nBecause no two AI projects are alike, you need to make sure that your quality assurance (QA) process is designed to meet the unique needs of your particular project. Here, both data accuracy and consistency are reviewed, separate steps that a data scientist or project manager may perform manually or in an automated way.\n\nTwo different types of QA processes:\n\nSama’s own QA managers review the tasks using both manual and automated techniques.\n\nClient’s QA process performed by a data scientist, most likely manual only.\n\nReviewing tasks is a pain point for most data science teams. We believe that your team should spend less time on these time-consuming and tedious tasks and spend more time on strategic work. As such, we created the Auto QA process. Auto QA creates an instant feedback loop to prevent logical fallacies, which helps annotators improve and get it right the first time.\n\nAutomated logic checks are triggered before a task is submitted on Sama’s platform by our annotators. Auto logic checks can identify several potential issues in your data— for instance, invalid answer combinations, repetitions, or size requirements. Leveraging Auto QA will help you prevent errors that may be impossible to detect by the manual QA review process. It reduces the time spent by manual QAs and allows annotators to focus on the more critical errors or edge cases.\n\nSama’s training data platform handles the entire annotation lifecycle and employs a quality feedback loop, enabling us to offer the highest quality SLA (\u003e95%), even on the most complex workflows. Understanding the importance and prioritizing the quality of your training data will help in achieving success with your models. The first step in gaining good quality training data begins with finding the right processes and platforms to label your training data.","seo_title":"How to Define and Measure Your Training Data Quality","slug":{"_type":"slug","current":"define-and-measure-training-data-quality"},"tags":[{"_key":"aZTVAo0T","label":"Training Data","value":"Training Data"},{"_key":"Wr7YIM1K","label":"Data Quality","value":"Data Quality"}],"title":"How to Define and Measure Your Training Data Quality"},{"_createdAt":"2021-03-29T21:29:09Z","author":{"_id":"785c9b8f-1869-4ce3-9eaa-c53945aa9736","avatar":{"_type":"image","asset":{"_ref":"image-c187f0a84bc4d1a6870d3a7a8528f242920a33aa-309x343-webp","_type":"reference"}},"bio":"Aurelie Drouet leads Product Marketing at Sama. Hailing from France and with expertise in driving revenue growth for companies in the US and Latin America, Aurelie's expertise spans several countries. She previously led Strategic Partnerships at Dreem.","name":"Aurélie Drouet","slug":{"_type":"slug","current":"aurelie-drouet"}},"estimatedReadingTime":7,"featured_image":{"_type":"image","asset":{"_ref":"image-b5b41f578774ef4d9391188b97c960a4f9e43e2c-900x506-png","_type":"reference"}},"meta_description":"87% of AI projects will never make it into production. Why? We asked ML experts.","openGraphImage":null,"plaintextBody":"In its infancy, Machine Learning was hailed as a silver bullet, something that could solve your problems and automate tasks with high-quality output with less effort required than ever. It's been predicted that 87% of AI projects will never make it into production. Why? We asked ML experts what they believe to be the main reasons that ML projects fail.\n\n\n\nTL;DR:\n\nA disconnect between the science and real-world application of the ML solutions and business expectations.\n\nUnreasonable expectations from business hierarchies on both the outcome and cost of many ML projects .\n\nAiming to develop MLOps without due diligence and research into some of the challenges that could be faced along the way .\n\nTo make an ML project successful, we need to find the right problem, use the right data, and develop the right method. Many ML projects do not meet these three requirements.\n\nIt’s not easy for companies to find enough data to run useful and high quality ML models.\n\n\n\nLavi Nigam\nData Scientist, Gartner\n\n“The biggest reason for project failure is ML processes maturity. There are too many moving and distinct pieces in ML/DL workflows and they are not mostly tied intrinsically by a single tool/framework. Enterprises and open source projects are trying to bridge this gap and I feel in the next 3 years we will see good progress in this area. Open source end-to-end MLOps are very important since they will help with wider adoption just like Tensorflow did for deep learning. Another big issue which increases project failure is that currently data science is evolving and has not reached ‘enterprise ready consensus’ with regards to set practices for different domains.”\n\n\n\nSparkle R. \nAssociate Director Data Science, Johnson \u0026 Johnson\n\n“ML projects still fail because of the disconnect between the science and real-world application of the ML solutions and business expectations. We are now in a society that prefers to use the word ML to demonstrate prestige despite having no clear impact. While there are many other reasons that can contribute to a ML project’s failure, the major factors include: a lack of high-quality data, poorly designed research questions, overly optimistic business expectations and a disconnect between the developers, product owners, data scientist and the ML-based system end users.”\n\n\n\nRemo Storni\nSenior Software Engineer, Apple\n\n“The biggest reason for ML project failures are unreasonable expectations about what is possible and unreasonable optimism about the project. People have trouble expecting the unexpected. There are a number of ML projects that fail and often these are problems that ML can't easily solve properly right now like conversational AI. There is a much bigger number of problems where ML falls short of expectations or where the project hits unexpected data or engineering problems.”\n\n\n\nRavi Dalal \nSenior Computer Vision Engineer, Walmart\n\n“I think this happens because people don’t do enough prep work before starting the project. It doesn’t make sense to apply a deep learning model where a logistic regression can do the trick. So, HOMEWORK before starting the project is very critical for the success of any ML project”.\n\n\n\nPiero Molino\nStaff ML / NLP Research Scientist, Stanford University\n\n\"Because of the uncertainty baked in many parts of the machine learning development process. It is very difficult to assess beforehand if a machine learning project will succeed without analyzing the data, training initial models, evaluating them and then iterating. Product teams still don't know if there's enough signal in the data to begin with, how much data would they need, how expensive annotating it would be, and in many cases they don't have great ways to monitor and evaluate models. All these uncertainty sources, together with wrong expectations, may lead to failure of ML projects.\"\n\n\n\nManmeet Singh\nMachine Learning Lead, Apple\n\n“There are a variety of reasons ML projects fail. Sometimes ML projects are initiated without synergy on expectations, objectives, and success criteria of the project between the business and ML teams. There could be other reasons ranging from lack of expected data, expertise, limitations in the technology itself for certain domains. This is a long-term investment \u0026 hence clear strategy and leadership support are necessary for success.”\n\n\n\nJason Gauci\nSoftware Engineering Manager, Facebook\n\n“Many projects fail because people don't trade off the benefit of success with the consequence of failure. Imagine a smart garage door opener that could open your garage door in anticipation of your arrival. If it opens when you want, it saves a button press. But if it opens when you aren't around, someone can rob your house. Because of this extreme tradeoff, the model needs to be impossibly accurate because it's competing with a button press that is almost 100% accurate”\n\n\n\nZhiyong (Sean) Xie\nDirector, AI, Pfizer\n\n“Most people just tried to find nails with an existing hammer. To make an ML project successful, we need to find the right problem, use the right data, and develop the right method. Many ML projects do not meet these three requirements. There is also miscommunication between the ML scientists and domain experts. It is not easy to find enough data to train the model. New methods need to be developed based only on the problem and available data.”\n\n\n\nIndu Khatri \nMachine Learning Lead, HSBC\n\n“Most ML projects fail because of a lack of understanding among the Business Executives about how ML models apply to their business. This usually leads to ML teams not solving the right problem for the business and underwhelming results. I have heard about projects that are stuck in research because of unclear problem statements.”\n\n\n\nYaman Kumar\nPhD Computer Science, University of Buffalo\n\n“Projects failing in ML, in my opinion, is chiefly due to three reasons: under-specification, over-expectation and clean toy datasets. While most of our datasets are so clean that the models trained on them hardly work in the real conditions, the ones that do are marred by the hype surrounding AI and ML. Underspecification normally comes from a lack of maturity in the field.”","seo_title":"10 Experts on the Biggest Roadblocks to Bringing ML Models to Production","slug":{"_type":"slug","current":"10-experts-biggest-roadblocks-ml-production"},"tags":[{"_key":"fLUi1ttd","label":"Machine Learning","value":"Machine Learning"},{"_key":"1iCuMNwM","label":"Expert Advice","value":"Expert Advice"}],"title":"10 Experts on the Biggest Roadblocks to Bringing ML Models to Production"},{"_createdAt":"2021-03-23T19:00:21Z","author":{"_id":"4e2e7cef-d6eb-4bb7-bd39-375c6299677e","avatar":{"_type":"image","asset":{"_ref":"image-f1a274bcfdb5e70d814f1bab2b6bbd644728e9be-1480x1462-jpg","_type":"reference"}},"bio":"With a background in Computer Science, Abha leads the Customer Success Engineering team at Sama. The team is responsible for managing technical relationships with customers and prospects to understand their business needs, ideate upon them, and manage the implementation and communication of the solutions developed. ","name":"Abha Laddha","slug":{"_type":"slug","current":"abha-laddha"}},"estimatedReadingTime":4,"featured_image":{"_type":"image","asset":{"_ref":"image-2be5ee7e7ae0847f3bedec01bb88266c371e3eb9-4000x2250-jpg","_type":"reference"}},"meta_description":"Sama is an expert in efficiently designing annotation guidelines that enhance data quality. Gold Tasks refer to tasks that have been annotated perfectly.","openGraphImage":null,"plaintextBody":"Several methods exist to help companies define and measure data quality. To define the correct annotation of given data, you want to start by creating annotation guidelines. On top of proposing a multi-level quality checks system, Sama’s experts have built unique know-how in efficiently designing annotation guidelines that enhance data quality.\n\nCue Gold Tasks; referring to tasks that have been annotated “perfectly” or meet the “gold standard”. Such tasks are often used by the client to communicate their expectations around precision and quality used as examples during training. At Sama, we use Gold Tasks in two more ways: During training, to assess annotators and identify those who are ready to move into production and during production to generate an automated metric on quality.\n\nGold Assessments\n\nEvery project launch at Sama is accompanied with a period of training, where annotators focus on requirements for the specific workflow, familiarizing themselves with the taxonomy, accuracy levels needed, and edge cases. Gold tasks come in as they move from classroom training to practice tasks.\n\nA gold set is created that is representative of the overall complexity of the dataset, ensuring a healthy mix of the edge cases. annotators practice on this set of which we already have “gold” answers. As each task is submitted we are able to compare the annotator answers with the gold task, generating custom metrics and error tags. The metrics are determined by the type of workflow and tool used, for e.g. in a semantic segmentation workflow, we focus on IoU calculations per label and depending on the client rubric each label may be weighed differently.\n\nThese metrics are then used to create trends and analyze each annotator’s performance individually, and provide relevant feedback. We are able to analyze trends at an asset label (which type of images are more difficult than others?), annotator level (which annotator is struggling exactly where?), and the impact of time (was today better than yesterday?).\n\nGold assessments, therefore, help us accelerate training by providing customized feedback to each annotator early on and allowing us to track their improvement over time. This enables Sama to quickly identify doubts, find edge cases, and have high confidence that annotators are ready to move on to production. Lastly, this allows us to calibrate and train the manual QAs on the specifics of this particular workflow, ensuring that nothing is missed.\n\n\nGold Metrics\n\nSimilar to gold assessments, gold metrics compare an annotator’s tasks against a known, completed task. These tasks, however, are interspersed within the production queue with the annotators unaware that these are gold tasks. These tasks then act as tests for the annotators, generating similar metrics as mentioned above. This allows the team to report upon the annotator’s performance against the gold tasks that increase insight into quality and help to further tailor training and coaching.\n\nGold metrics are most useful for clients looking to automate the quality loop on their side. Given that their Sama project team consistently samples and approves only high quality tasks, it is a neat way for them to save time and capacity.\n\nBecause no two AI projects are alike, you need to make sure that your quality assurance (QA) process is designed to meet the unique needs of your particular project. Learn more on how to supercharge your data quality with Sama's Automated Quality Accelerators. ","seo_title":"Sama's Gold Tasks: ML Training Data with Gold-Standard Quality","slug":{"_type":"slug","current":"sama-gold-tasks"},"tags":[{"_key":"YSoeJ1Gj","label":"Product","value":"Product"},{"_key":"AHeszSpi","label":"Training Data","value":"Training Data"},{"_key":"9nTBOCjn","label":"Data Annotation","value":"Data Annotation"},{"_key":"ku9OMDnM","label":"Data Quality","value":"Data Quality"}],"title":"Sama's Gold Tasks: ML Training Data with Gold-Standard Quality"},{"_createdAt":"2021-03-17T03:58:33Z","author":{"_id":"40a70383-c941-405c-ae5b-f0aba421ee53","avatar":{"_type":"image","asset":{"_ref":"image-f1fd7fbcc4633299cdbedddba22cb44e24f17317-518x518-svg","_type":"reference"}},"bio":"Rafael and Juan are both senior members of Sama R\u0026D where they focus on Data Engineering and Data Science initiatives that help boost operational efficiency and achieve the highest quality.","name":"Rafael Alfaro \u0026 Juan Esquivel","slug":{"_type":"slug","current":"rafael-alfaro-and-juan-esquivel"}},"estimatedReadingTime":6,"featured_image":{"_type":"image","asset":{"_ref":"image-bbf0e002b88fd9c3b2fcbd032f60f114bc93eed0-1980x990-png","_type":"reference"}},"meta_description":"In this series of three we’ll go into Experiment Driven Development and A/B Testing. EDD is fact-based development: based on evidence, not intuition.","openGraphImage":null,"plaintextBody":"We've previously explored the foundations of Experiment Driven Design and A/B Testing. Today we'll dig into A/B Testing with Python because analysis can be easily automated with existing open source python libraries. In this post we will explore their usage with an example. To orient the reader, we will state a few definitions to anchor the example:\n\nControl Group: current user interface.\nExperiment Group: rearranged point annotation button.\n\nH0: The mean of the annotation time for the control group is the same as the mean of the annotation time for the experiment group; there is no effect from rearranging the point annotation button.\n\nH1: The mean of the annotation time for the control group is different from the mean of the annotation time for the experiment group; there is an observable effect from rearranging the point annotation button.\n\nLet us assume that we ran an A/B Test feature experiment for two weeks. The UI modifications consisted of rearranging a button used in the process of drawing polygons around objects Let us assume these were recorded annotation times per image in minutes, for users of each variant (it can be represented as a python list):\n\nA. Control group (original arrangement):\n\nvariant_a = [150, 195, 120, 160, 97, 20, 100, 121, 250, 300, 80, 75, 100, 196, 147, 120, 100, 190, 57, 100, 157, 186, 91, 190, 210, 222, 192, 243, 99, 151]\n\n\n\nB. Experiment group (rearranged button):\n\nvariant_b = [120, 110, 96, 99, 87, 55, 43, 83, 200, 100, 125, 140, 75, 91, 141, 121, 250, 35, 94, 65, 85, 67, 93, 161, 35, 34, 111, 124, 85, 103]\n\n\n\n1. Run the t-test from the scipy.stats module of scipy (a mathematical, scientific and engineering library).\n\n\nimport scipy.stats as stats\n\nt, p = stats.ttest_ind(variant_a, variant_b, equal_var=False)\n\n\n\n2. Calculate the degrees of freedom according to Welch’s t-test definition which is the one implemented in stats.ttest_ind\n\n# For illustrative details see Wikipedia\n\n\ns1 = np.std(variant_a)\ns2 = np.std(variant_b)\nn1 = len(variant_a)\nn2 = len(variant_b)\n\ndf = np.floor(((((s1 ** 2) / n1) + ((s2 ** 2) / n2)) ** 2) /\n(((s1 ** 4) / ((n1 ** 2) * (n1 - 1))) + ((s2 ** 4) / ((n2 ** 2) * (n2 - 1)))))\n\n\n\n3. Now, using the same scipy.stats library, get the t-critical value for 95% or an alpha of 0.05 (1 - confidence level) from the t distribution’s ppf (percent point function) function and evaluate the t statistic from the previous step. If it falls in the range [-t-critical, t-critical] then H0 cannot be rejected, if it is outside, then we can reject H0 in favor of H1:\n\nalpha = 0.05\nt_critical_value = stats.t.ppf(1 - (alpha/2), df)\nnull_hypothesis = bool(t_critical_value \u003e= t_value \u003e= -t_critical_value)\n\n\n\n4. The confidence interval of variant_b (experiment) will help us visualize the difference between the two variants. If the mean of the control group doesn’t fall inside of this interval then the means of the two groups are significantly apart from each other, which suggests that the results are statistically significant.\n\ns = np.std(variant_b)\nx = np.mean(variant_b)\nn = len(variant_b)\nrho = (t_critical_value * s) / np.sqrt(n)\nconf_int = x - rho, x + rho\n\n\n\n\n\n5. Statistical power is the probability that the test correctly rejects the null hypothesis, in other words, the probability of a true positive result. This is only useful when the null hypothesis is rejected. A low value of power could be an indication that the sample size is not big enough yet to validate the results. To calculate the statistical power we use the class TTestIndPower from the module statsmodels.stats.power (https://www.statsmodels.org/stable/stats.html?highlight=power#module-statsmodels.stats.power) of the statsmodel (https://www.statsmodels.org/) library.\n\n\nfrom statsmodels.stats.power import TTestIndPower\n\n# Effect size based on Cohen’s d formula: https://en.wikipedia.org/wiki/Effect_size#Cohen's_d (https://en.wikipedia.org/wiki/Effect_size#Cohen's_d)\n\nx1 = np.mean(variant_a)\nx2 = np.mean(variant_b)\ns1 = np.std(variant_a)\ns2 = np.std(variant_b)\nn1 = len(variant_a)\nn2 = len(variant_b)\n\ns = np.sqrt((((n1 - 1) * (s1 ** 2)) + ((n2 - 1) * (s2 ** 2))) / (n1 + n2 - 2))\neffect = np.abs((x1 - x2) / s)\n\npower = TTestIndPower().power(effect, nobs1=n1, ratio=n2 / n1, df=(n1 + n2 - 2), alpha=alpha)\n\n\n\n6. Plot the sample distributions with confidence intervals as a visual aid using matplotlib library.\n\n\nimport matplotlib.pyplot as plt\n\n# Control\nfig, ax = plt.subplots(figsize=(12,6))\nxA = np.linspace(40, x1 + 3*s1, 100)\nyA = stats.norm(loc=x1, scale=s1).pdf(xA)\nax.plot(xA, yA, c='red', label='Variant A Distribution')\nax.axvline(x=x1, c='red', alpha=0.5, linestyle='--', label='Variant A')\n\n# Experimental\nxB = np.linspace(40, x2 + 3*s2, 100)\nyB = stats.norm(loc=x2, scale=s2).pdf(xB)\nax.plot(xB, yB, c='blue', label='Variant B Distribution')\nax.axvline(x=x2, c='blue', alpha=0.5, linestyle='--', label='Variant B')\n\n# Confidence interval\nax.axvline(conf_int[0], c='green', linestyle='--', alpha=0.5)\nax.axvline(conf_int[1], c='green', linestyle='--', alpha=0.5, label='Confidence Interval')\n\nplt.xlabel('Annotation Time')\nplt.ylabel('Percent of Tasks per Annotation Time')\nplt.title('Annotation Time Distributions')\nplt.legend()\nplt.show()\n\n\n\n","seo_title":"Part 3: A/B Testing with Python","slug":{"_type":"slug","current":"experiment-driven-development-part-3"},"tags":[{"_key":"2iLUnerV","label":"Sama Engineering","value":"Sama Engineering"}],"title":"Part 3: A/B Testing with Python"},{"_createdAt":"2021-03-14T21:00:14Z","author":{"_id":"4e2e7cef-d6eb-4bb7-bd39-375c6299677e","avatar":{"_type":"image","asset":{"_ref":"image-f1a274bcfdb5e70d814f1bab2b6bbd644728e9be-1480x1462-jpg","_type":"reference"}},"bio":"With a background in Computer Science, Abha leads the Customer Success Engineering team at Sama. The team is responsible for managing technical relationships with customers and prospects to understand their business needs, ideate upon them, and manage the implementation and communication of the solutions developed. ","name":"Abha Laddha","slug":{"_type":"slug","current":"abha-laddha"}},"estimatedReadingTime":4,"featured_image":{"_type":"image","asset":{"_ref":"image-0353ebdd4cd94e4e29f102019edc39fb91d77499-4485x2522-png","_type":"reference"}},"meta_description":"Automated quality accelerators are technology innovations that are focused on reducing the amount of manual quality assurance time spent in QA processes.","openGraphImage":null,"plaintextBody":"Let’s start at the beginning. What are automated quality accelerators and why should we care? Automated quality accelerators are technology innovations that are focused on reducing the amount of manual quality assurance time spent in QA processes. They may be used to expedite annotator education, generate automated measures of annotation quality, and prevent logical fallacies. Our accelerators are integrated into our Sama training data platform and can be customized for unique use cases and needs by our dedicated Customer Success Engineering team.\n\nWhile it’s key to have a human in the loop when creating and verifying training data, automating processes within the workflow improves efficiency while guaranteeing high quality, saving everyone time and money. Quality accelerators also focus the effort of Sama’s annotators towards the most challenging aspect of the task, minimizing the volume of manual rework they need to do, and catching mistakes early in the process —equipping them to do their job well. Ultimately, automated quality accelerators enable us to deliver super high-quality data for complex use cases in the most efficient manner.\n\n\n\nAutomated Quality Accelerators at Sama\n\nAutomated Logic Checks:\n\n\n\nAutomated Logic checks are triggered before a task is submitted on Sama by our annotators. Each task is assessed using a fixed set of rules to check for invalid combinations of labels within a shape, across all shapes, and dependencies with the metadata. These rules are flexible and customized to each workflow, focusing on all errors types that don’t need human judgement. If an error is found, the annotator needs to fix the task before it can be submitted. To help the annotator to fix the task and learn from their mistake, a message is displayed which contains shape specific error tags.\n\nAuto logic checks are optimized for different kinds of errors, including but not limited to:\n\nInvalid answer combinations: Combinations that the ontology prevents, for example more than two wheels being tagged on an item labeled “bicycle.”\n\nUniqueness or preventing repetitions: More than one object in an image being assigned the same unique identifier or the same label when the ontology prohibits that. For example, two noses in a single person keypoints workflow.\n\nSize requirements: Ensuring that size specifications are met. For example, guaranteeing consistent size in a 3D workflow where constant cuboid size is required or enforcing a minimum pixel rule\n\nRelational checks: Given attributes and sub-attributes, ensure that values selected aren’t contradictory. For example, a bicyclist polygon isn’t grouped/attached to a car bounding box\n\nThese checks are incredibly efficient for the following reasons:\n\nCreate an instant feedback loop to prevent logical fallacies, which helps annotators improve and get it right the first time\n\nPrevents errors that may be impossible to detect by a manual QA review processes\n\nReduces time spent by manual QAs and allow them to focus on the more critical errors or sample more tasks\n\nHelps annotators to adapt quickly to changing project instructions, ensuring that new instructions are being followed and are understood correctly\n\nLastly, it improves overall TPT and reduces the time from task creation to delivery\n\nUnder a strictly manual process, highly skilled quality assurance annotators would need to review and provide feedback manually. While the latter process is still a vital part of our human-in-the-loop data annotation, auto logic checks free up their expertise to focus on more subjective errors and edge cases.\n\nOur enterprise-level clients are successfully using Sama Quality Accelerators to realize extremely high data quality for their most complex use cases, improving overall model performance. Now is the time to supercharge your data quality.","seo_title":"Supercharge Your Data Quality with Automated Quality Accelerators","slug":{"_type":"slug","current":"data-quality-with-auto-q-a"},"tags":[{"_key":"5AiNXS1h","label":"Product","value":"Product"},{"_key":"Izvu8Acj","label":"Training Data","value":"Training Data"},{"_key":"6O3wnuJC","label":"Data Quality","value":"Data Quality"}],"title":"Supercharge Your Data Quality with Automated Quality Accelerators"},{"_createdAt":"2021-03-08T21:41:38Z","author":{"_id":"40a70383-c941-405c-ae5b-f0aba421ee53","avatar":{"_type":"image","asset":{"_ref":"image-f1fd7fbcc4633299cdbedddba22cb44e24f17317-518x518-svg","_type":"reference"}},"bio":"Rafael and Juan are both senior members of Sama R\u0026D where they focus on Data Engineering and Data Science initiatives that help boost operational efficiency and achieve the highest quality.","name":"Rafael Alfaro \u0026 Juan Esquivel","slug":{"_type":"slug","current":"rafael-alfaro-and-juan-esquivel"}},"estimatedReadingTime":4,"featured_image":{"_type":"image","asset":{"_ref":"image-bbf0e002b88fd9c3b2fcbd032f60f114bc93eed0-1980x990-png","_type":"reference"}},"meta_description":"In this series of three we’ll go into Experiment Driven Development and A/B Testing. EDD is fact-based development: based on evidence, not intuition.","openGraphImage":null,"plaintextBody":"After last week's intro into Experiment Driven Development at Sama, we'll go further into A/B Testing today. A/B Testing is a randomized experiment method to compare how two populations behave in a controlled environment and determine whether the variation of some target metrics defined are significant or not, to determine that the experiment yields better results than the alternative.\n\nWe say that a baseline sample (variation A), which normally refers to some existing system, is compared against an experimental treatment (variation B). In software development, samples drawn from the two populations will be used to analyze the metrics associated with the usage of features. We generally want to make sure that any new feature has a positive impact: improved usability, lower duration to finish a process, etc. We should aim to have data to back up our claims that a feature has benefits, otherwise it would be fair to question why we want to deploy a new feature.\n\nOne way of evaluating an A/B experiment is through the use of a t-test which works well when we expect the distribution to be normal and it also allows us to not worry about the unknown standard deviation of our data. Note, however, that population distributions are not always expected to be normal, of course, and the t-test can be replaced by some other appropriate hypothesis test, depending on the distribution of the data, e.g. Kolmogovor-Smirnov. In our case, we want to determine if the means of the two data samples are significantly different from each other, with a given confidence level (90%, 95% and 99% are commonly used values).\n\nThe framework of the experiment revolves around the definition of a hypothesis for an A/B Test as follows:\n\nH0: The mean of the baseline metric is the same as the mean of the experiment metric; there is no effect from the treatment (variation) of the experiment, thus the two means belong to the same population.\nH1: The mean of the baseline metric is different from the mean of the experiment metric; there is an observable effect from the treatment (variation) of the experiment metric and the two means belong to different populations.\n\nWe then define the timeline for the feature/process experiment, run the experiment and collect the observations from the samples of the two variants. We should have a notion in terms of how long we want to run this to collect enough information (we will treat this as out of scope on this piece, however).\n\nIn order to perform the t-test evaluation, we need per sample sizes (N), means (X) and the standard deviations (s) to calculate the statistic t and the degrees of freedom (v). This is calculated as follows (using Welch’s t-test for independent variables with unequal variances and unequal sample sizes):\n\nOnce the statistic and degrees of freedom are calculated, along with an (1-confidence level), we can evaluate our hypothesis against the t-distribution table to determine whether or not the populations are different.\n\nIf our t statistic is less than the value from the table, given the degrees of freedom and the significance level, we can reject the null hypothesis as there is enough evidence to determine that sample means are from different populations. That would mean our new feature is really different from the control and we can release it, if the direction of the variation is in line with our objectives (e.g. lower means when we want to lower durations is good).\n\nThere are other more nuanced circumstances that we can address with a similar framework. For instance, we may want to test two variants of a new feature side by side. We may also want to use a slightly different statistical tool than a t-test, depending on what our interests are. What is important is the test-driven culture that should be fostered within organizations to have a data-drive approach to justify the release of new features.\n\nNext up: A/B Testing with Python.","seo_title":"Part 2: A/B Testing","slug":{"_type":"slug","current":"experiment-driven-development-part-2"},"tags":[{"_key":"FSfjH4D2","label":"Sama Engineering","value":"Sama Engineering"}],"title":"Part 2: A/B Testing"},{"_createdAt":"2021-03-02T05:05:17Z","author":{"_id":"40a70383-c941-405c-ae5b-f0aba421ee53","avatar":{"_type":"image","asset":{"_ref":"image-f1fd7fbcc4633299cdbedddba22cb44e24f17317-518x518-svg","_type":"reference"}},"bio":"Rafael and Juan are both senior members of Sama R\u0026D where they focus on Data Engineering and Data Science initiatives that help boost operational efficiency and achieve the highest quality.","name":"Rafael Alfaro \u0026 Juan Esquivel","slug":{"_type":"slug","current":"rafael-alfaro-and-juan-esquivel"}},"estimatedReadingTime":3,"featured_image":{"_type":"image","asset":{"_ref":"image-bbf0e002b88fd9c3b2fcbd032f60f114bc93eed0-1980x990-png","_type":"reference"}},"meta_description":"In this series of three we’ll go into Experiment Driven Development. EDD is fact-based development: based on evidence, not intuition.","openGraphImage":null,"plaintextBody":"In this series of three we’ll go into Experiment Driven Development and A/B testing. The opposite of developing features based on anecdotes heard in stories from the CEO’s next-door neighbor, EDD seeks proof and is iterative. EDD can be defined as fact-based development: development based on evidence gathered from the field, not intuition.\n\nIn EDD every new feature or process implemented is validated through a formal experiment design process, which looks to test a hypothesis that describes the status quo of the feature. Example hypotheses could range from \"Making a button bigger does not impact clicks\" all the way to \"Making a web app responsive does not increase visitation\" compared. In statistical terms, the base statement is referred to as the null hypothesis (H0), the status quo, and then an alternative hypothesis(H1) is proposed. The null hypothesis will usually state that the change introduced by the experiment will not affect the current behavior while the alternative supports that there is in fact a change.\n\nThe alternative hypothesis is a prediction of what is expected to happen before running the experiment. It can be a bold statement, not an open question and it should have three parts:\n\nThe variable (if we add/change/remove...): the change that the experiment will measure against the current state of the feature/process.\n\nThe desired result (then we expected to see...): what we expect to see after the change is introduced, a qualitative difference between the current state and new state.\n\nThe rationale behind the prediction (because we have seen that...): prior knowledge that has led you to come up with the current hypothesis (from prior observation).\n\nFor example, one can define the pair of hypotheses for a new registration form in a website as:\n\nH0: Changing the registration form from multiple to single page will not impact the current user registration rate.\n\nH1: Changing the registration from multiple to single page will increase the current user registration rate by 5% because we have previously seen that there is a 5% abandon rate on the multi page form format.\n\n\n\nEDD is based on A/B Testing, which is a randomized experiment method to compare two variants of a single variable. In this case, a baseline metric is compared thanks to the definition control (status quo) and treatment (new feature) groups in order to determine if the variation has a significant impact or not. Ideally, most decisions to release a feature would be based on the results given by A/B Tests. At Sama, we want to find viable ideas or fail fast. Instead of developing a monolithic solution and pushing a release, we iterate through experiments, evaluating how features perform and, most importantly, if and how customers use them.\n\nNext up: A/B Testing and A/B Testing with Python.","seo_title":"Part 1: Experiment Driven Development","slug":{"_type":"slug","current":"experiment-driven-development-part-1"},"tags":[{"_key":"sZmqu6EC","label":"Sama Engineering","value":"Sama Engineering"}],"title":"Part 1: Experiment Driven Development"},{"_createdAt":"2021-02-09T20:20:36Z","author":{"_id":"88e43c38-2712-41d4-9df3-8656c163c4db","avatar":{"_type":"image","asset":{"_ref":"image-af1346669e45d7264a36f051c18aea76061725bb-1895x1894-webp","_type":"reference"}},"bio":"Kyra is passionate about world-changing tech and sustainability, and happiest when these come together (looking at you, Sama). Stereotypical Dutch, she enjoys urban cycling and eating stroopwafels.","name":"Kyra Harrington","slug":{"_type":"slug","current":"kyra-harrington"}},"estimatedReadingTime":6,"featured_image":{"_type":"image","asset":{"_ref":"image-f6c86749d02ef692a6d0252be265f98ef9d35890-900x507-png","_type":"reference"}},"meta_description":"We reached out to various ML experts, asking them the questions: Why is high-quality training data so important? Why do so many projects fail in ML?","openGraphImage":null,"plaintextBody":"In our guide on Data Quality we discussed the need for high-quality data for Machine Learning models. It is widely accepted that without ample amounts of high-quality training data, the application of AI and Machine Learning is impossible. This has also been seen in studies, including an IDC survey, in which only 30% of companies reported a 90% or higher success rate in their AI rollout, with reported failure rates of 10 to 49 percent. A key reason for this? Data! \n\nWe partnered with our friends from RE•WORK to reach out to various ML experts, specifically asking them the questions: Why is high-quality training data so important? Why do so many projects fail in ML?\n\n\nManmeet Singh\nMachine Learning Lead, Apple\n\n\"The core of any Machine Learning model is what input is being fed to it as the model generalizes based on these training examples. The criteria to choose a ML model is heavily dependent on the kind of input available. For the model to learn anything relevant, training data plays a key role. Imagine in a supervised setting, we are trying to do object recognition. If the labels themselves are messed up what would the model learn? Besides the quality, the quantity of training examples also plays a major role. Training data forms the basis of business decisions based on the offline KPIs being measured on their information. They are the building block for defining a roadmap to the product cycle.\"\n\n\nIndu Khatri\nMachine Learning Lead, HSBC\n\n\"There are two main reasons why quality training data is important. First one is that many problems are solved using Supervised Learning and training data forms the backbone for such applications. The second and more deeper reason is that with the advent of AutoML, democratization of ML skills, and open sourcing of cutting edge research and tools; they are no longer competitive advantages. The only way businesses can sustain a competitive advantage in AI applications is through differentiated training data.\"\n\n\nLavi Nigam\nData Scientist, Gartner\n\n\"In supervised learning, algorithms are dependent on training data to extract relevant patterns for future predictability, hence clean, unbiased and processed training data is crucially important. It’s like the “garbage in garbage out” rule for training any ML/DL models. Although, sometimes, we have lack of training data available and in such cases we have many new semi-supervised learning algorithms coming up. I see great future in such techniques as businesses all across the world don't work at Google/Facebook data scales mostly.\"\n\n\nZhiyong (Sean) Xie\nDirector, AI, Pfizer\n\n“‘Give me a lever long enough, and I shall move the world’. With deep learning methods, we may say ‘Give me data big enough and I can predict anything.’ Data is the foundation of Machine Learning, especially the deep learning method, with the machines learning everything from data. If you feed a machine biased data, it gives you biased predictions. Garbage in, garbage out.”\n\n\nShuo Zhang\nSenior ML Engineer, Bose Corporation\n\n“One point I'd like to stress is the domain knowledge and domain specificity in ML. People sometimes think of ML as a ubiquitous technology that can be universally applied to any domain. But in reality, blindly applying ML is dangerous, and you should always know your domain and your data in a very deep way.”\n\n\nPiero Molino\nNLP Research Scientist, Staff ML Team, Stanford\n\n\"How would you train models without it? More seriously, quality and size of data are important because we are trying to tackle difficult tasks and we haven't figured out yet methods that are very robust to noise or that can be trained from smaller amounts of data.\"\n\n\nSparkle R. \nAssociate Director Data Science, Johnson \u0026 Johnson\n\n\"While innovative ML tools and techniques are being developed at a rapid pace, ML practitioners can sometimes get drawn into the novelty of applying newer approaches without realizing that the fundamentals of science and lack of high-quality data make many of these algorithms impractical for some healthcare facing solutions. Additionally, in our haste to publish our findings, researchers often forget that these systems will be used by an end-user (patient and provider), and expert level performance in development does not guarantee real-world clinical utility, adoption and transferability across heterogeneous healthcare systems. So, it is equally important that the end user’s workflow is also considered to ensure that the lab to real world transition is successful.\"\n\n\nYaman Kumar\nPhD Computer Science, University of Buffalo\n\n\"Training data is so important since most of the people, books, blogs, videos start and end with supervised learning. We currently do not know how to work in unsupervised settings. As we move towards unsupervised settings, the requirement of training data as we know it will reduce. In that world, gathering training data would not be an arduous task as it is supposed to be now.\"\n\n\nKiana Alikhademi\nArtificial Intelligence \u0026 Computer Science, University of Florida\n\n\"Training data is the backbone of any machine learning system, without sufficient training data, it is impossible for a machine to learn patterns or solve problems. The importance of training data illustrates why inadequate, or low quality training data, could lead to Machine Learning systems’ failure. Training data ought to be representative of all different groups within the sample without inheriting any societal prejudices.\"\n\n\nJack Brzezinski\nChief AI Scientist at AI Systems \u0026 Strategy Lab\n\n\"I feel that the role of knowledge will be increasing. Structures, various knowledge representation types will be essential for the next wave of AI innovation. The lawmakers might soon require AI, ML models to be compliant with multiple statutes or regulations.\"","seo_title":"10 Experts Give Reasons Why High-Quality Training Data is so Important","slug":{"_type":"slug","current":"experts-high-quality-training-data-importance"},"tags":[{"_key":"HWRhI8n2","label":"Training Data","value":"Training Data"},{"_key":"QPqXb7Bo","label":"Data Quality","value":"Data Quality"},{"_key":"deOuCKuN","label":"Expert Advice","value":"Expert Advice"}],"title":"10 Experts Give Reasons Why High-Quality Training Data is so Important"},{"_createdAt":"2021-02-01T20:53:26Z","author":{"_id":"e9e6679d-3ed9-4cd2-a5a5-cfbff31e1057","avatar":{"_type":"image","asset":{"_ref":"image-e33a218a0ab19114b971463e566b1d1cbdb2e6cd-512x512-webp","_type":"reference"}},"bio":"Mathieu is a Senior Software Engineer at Sama. He has been developing software for more than two decades, with a deep passion for DevOps, CI/CD, Infrastructure as Code, Kubernetes and everything Cloud Native. A joyful father of four, Mathieu loves working with people, learning from them, sharing his love of software engineering, coaching and bringing the best out of everyone, including a smile!","name":"Mathieu Frenette","slug":{"_type":"slug","current":"mathieu-frenette"}},"estimatedReadingTime":4,"featured_image":{"_type":"image","asset":{"_ref":"image-19ef3f125b8ee47727e9f94dc3aff7ed08a4d78c-720x360-png","_type":"reference"}},"meta_description":"Introducing Factotum: an MIT-licensed, open source, kubernetes-oriented, general purpose docker container for devs/devops and custom CI/CD pipelines.","openGraphImage":null,"plaintextBody":"Our Cloud Journey\n\nLike most companies making the transition to the cloud today, one of the biggest challenges we face in empowering our engineering teams to adopt cloud-native technologies, such as Kubernetes and CI/CD, is the high barrier to entry of simply setting up their local machines with a unified set of tools and configs to interact with those cloud environments. As DevOps, we would ideally like all our colleagues to easily have access to the same tools and environments, and to even extend that to our CI/CD pipelines, which have no reason to run different versions of those tools from our development machines.\n\nContainers are a natural go-to for the job. However, while they're great at encapsulating many tools with specific versions and repeatable, predictable outcomes, containers are designed with remote services and jobs in mind, rather than day-to-day use as a local work environment.\n\nWhen you start to use containers as a daily working tool, you quickly stumble on many roadblocks. You need to master Docker CLI commands and flags to launch your container in various conditions, environment variables, volume mounts and port mappings. The syntax is different depending if you already started your container in the past, or if it's already running and you just want to open a few extra shells into it. What if you need to access multiple kubernetes clusters in parallel, each with its own container? And don't forget that if you modify any config files within your container, those will be discarded the next time you rebuild and launch a fresh copy of your image! Finally, if you want to share the same tools and configs with your colleagues and (gasp!) even your CI/CD pipeline, you're in for an extra ride!\n\nNaively patching up some Dockerfile with your desired tools is not the end of the road. For us, it was just the beginning.\n\n\nThe Process\n\nWe have long tinkered with the idea of packaging up a neat multi-purpose Docker container that would handle most use cases and make it seamless to setup and interact with different Kubernetes clusters. Starting with a few prototypes and then some inspiration from Cloud Posse's Geodesic container—which does a great job of streamlining the installation and launching of the container— our experiments finally evolved into a tool generic and mature enough to be shared with the community!\n\nOur goal has been to share such a tool with the DevOps community and, thanks to the folks at Sama, this vision has become a reality. Let me introduce you to an MIT-licensed, open source, kubernetes-oriented, general purpose docker container for devs/devops and custom CI/CD pipelines, that we affectionately called \"Factotum\" (from Latin, basically meaning an employee who does all kinds of work).\n\nCheck out the source on GitHub\n\nWhat to Know Before You Begin\n\nEven if you can try the vanilla build of Factotum as is from GitHub and Docker Hub, please understand that Factotum is really intended to be customized and made your own in order to leverage its full potential. If you settle to try it, it is worth forking the repo, customizing the Dockerfile and following the instructions in README.md. While that process of setting up your own customized build of Factotum can be rather involved, be assured that using, maintaining, upgrading and sharing it with your teammates afterwards is intended to be as straightforward as possible—it just takes that little initial effort! And, if you encounter any issues or can't figure how to set it up correctly, don't hesitate to file an issue in the GitHub repo and give us feedback.\n\nHappy factotum'ing.","seo_title":"Factotum: Containerizing DevOps Tools for Cloud Native Engineering and CI/CD","slug":{"_type":"slug","current":"devops-tools-for-cloud-native-engineering"},"tags":[{"_key":"8AQmCtzZ","label":"Sama Engineering","value":"Sama Engineering"},{"_key":"Gl5fCrPJ","label":"Featured","value":"Featured"}],"title":"Factotum: Containerizing DevOps Tools for Cloud Native Engineering and CI/CD"},{"_createdAt":"2021-01-22T23:06:59Z","author":{"_id":"88e43c38-2712-41d4-9df3-8656c163c4db","avatar":{"_type":"image","asset":{"_ref":"image-af1346669e45d7264a36f051c18aea76061725bb-1895x1894-webp","_type":"reference"}},"bio":"Kyra is passionate about world-changing tech and sustainability, and happiest when these come together (looking at you, Sama). Stereotypical Dutch, she enjoys urban cycling and eating stroopwafels.","name":"Kyra Harrington","slug":{"_type":"slug","current":"kyra-harrington"}},"estimatedReadingTime":10,"featured_image":{"_type":"image","asset":{"_ref":"image-21b9dcfd77e3eb1e777b3d499c73c89ddf24d454-1200x675-png","_type":"reference"}},"meta_description":"2021 Predictions: We asked a range of ML experts about what they believe will be the next big thing in AI and Machine Learning.","openGraphImage":null,"plaintextBody":"Large developments in AI and Machine Learning are normally announced as part of a campaign or during important keynote presentations, but we thought we would try and get a sneak peak into 2021 developments in the field. We partnered with our friends from RE•WORK by asking a range of experts in Machine Learning what they believe will be the next big thing. \n\n\n\nTushar Chugh\n\nMachine Learning Software Engineer, Google\nTushar’s predictions for 2021 are very much based on the outcome of work already underway. The former General Motors trailblazer suggested that he was on the lookout for the following in 2021: 1. The “rapid” training, evaluation and productionization for the large sequence models (such as GPT-3, T5). 2. The research towards developing giant ML models that can cater to multi-model inputs from various tasks and that can generalize well to the new tasks.\n\n\n\nZhiyong (Sean) Xie\n\nDirector, AI, Pfizer\nWith over 17 years at Pfizer working in AI, Sean has overseen tasks ranging from quantification of MRIs to multivariate analysis of disease progress and most recently, a global effort in drug safety assessments. When asked what he thought might be on the horizon for this year, Sean suggested “We may train the machine to predict the Machine Learning in 2021.” This rather funny response, if true, could see a move toward the automation in ML we have discussed for some years.\n\n\n\nAlireza Rostamzadeh\n\nData Science \u0026 Machine Learning, Apple\nAlireza’s predictions for the year ahead were quite something, and we love the sound of this! “2021 will be the year when we start to see more tangible ML-driven products; from significant improvements in self-driving cars to a wave of new products based on augmented reality”.\n\n\n\nShuo Zhang \n\nSenior Machine Learning Engineer, Bose Corporation\nWorking across NLP, Deep Learning and now ML, Shuo has a range of experience. When asked, Shuo suggested that while the question in itself is extremely broad, “I'd say that less dependence on human supervision by leveraging the intrinsic structures of large data is a path ML research is heading toward.” A step closer to the infamous automation buzzword.\n\n\n\nJason Gauci\n\nSoftware Engineering Manager, Facebook\nJason’s experience has spanned creative positions at Apple, Google and now Facebook, where he is working on a scalable reinforcement learning platform. When we asked Jason what he thought we could see in ML this year, he suggested “I predict that decision making AI will surge in popularity. Imagine an AI system that can tune constants in your code, or one that monitors your diet and suggests food. We have done a lot in the realm of signal processing but decision making continues to be an area with a lot of engineering trial-and-error.”\n\n\n\nSadid Hasan\n\nSenior Director of AI, CVS\n“In 2021, researchers would continue to take a leap in developing complex AI applications related to natural language understanding and generation by leveraging the ongoing advancements in multi-modal (text, speech, image) data fusion-based algorithms and more efficient transformer architectures. In addition, powerful synthetic data generation and augmentation techniques would enhance effective training of machine learning models by alleviating the challenges related to accessing sufficient ground-truth data. We would also see further emphasis on AI model’s ethicality, generalizability, explainability, and reproducibility along with efficient model Ops for a beneficial and robust on-device deployment. Furthermore, with the ongoing proliferation of healthcare data, the AI community would continue to build novel machine learning-based healthcare applications leading to increased AI adoption for providing meaningful decision-making support to end-users.”\n\n\n\nLavi Nigam \n\nData Scientist, Gartner\n“As more and more companies are heading towards AI/ML Maturity and working with more production scale ML/DL deployments, their focus in 2021 will be more on MLOps where they need to work on key aspects of processes like - Model/Data Drift Analysis, Model Interpretability, Data Governance, Model Scaling \u0026 AutoML. This year we will finally see many maturity models and practices for ML/DL operations coming up from different vendors and companies which will be very crucial in the long run.”\n\n\n\nBeat Raphael Schaad\n\nSenior Data Scientist, Sky\nThe Senior Data Scientist at Sky when asked his predictions for 2021 stayed very much on the hot topic of COVID. “The goal for 2021 is that Data can support governments beyond counting Covid19 cases and death rates. Given the recent events ML will have to improve massively in inference and causality issues. Forecasting methods using AI do not deliver anything on our historic events scenarios mainly because of the lack of explainability of the generated features.”\n\n\n\n\nIndu Khatri\n\nMachine Learning Lead, HSBC\n2021 will be the Year of MLOps. Many cloud services such as GCP's AI Platform, AWS's Sagemaker have matured over the past years and are readily adaptable by ML teams. In 2021 and beyond Data Scientists would have to work collaboratively with DEs/MLEs to take their beautiful models from lab to production and realize business value.\n\n\n\nPiero Molino\n\nNLP Research Scientist, Staff ML Team, Stanford\nThe former Senior ML practitioner at UBER, currently makes up part of Professor Chris Ré's Hazy Research group on ML systems in California. Piero suggested that in industry he believes “models will start to come closer instead of being in separate silos like it happens today in most cases. In academia, I believe there will be more research focused on robustness and generalization.”\n\n\n\nKiana Alikhademi \n\nResearch Assistant, University of Florida\n“The year 2020 has significantly changed our lives in many ways. The way we work, communicate and learn has been impacted in many significant ways. Artificial Intelligence continues to be a key technology trend when it comes to the things that will change how we live, work, and play in the near future. I predict that artificial intelligence will be the main technology in the following areas: If any threat such as COVID were to occur, then that should be detected ahead of time. To help businesses prepare for situations like this pandemic, we must understand how users behave during such an emergency.”\n\n\n\nDivya Sivasankaran\n\nSenior Machine Learning Engineer, Autodesk\n\"Over the last year, we've seen a lot of local AI startups being sold/acquired. Many are struggling to bring AI products to market. I believe the pandemic merely sped up the inevitable. The silver lining to me here is that we've managed to cut down on the hype (noise) giving us time and resources to focus on impact with tangible applications (signal). That's why I think 2021 will be the year we will start to see AI/ML features in production for real - i.e., AI will successfully make the transition from being a spice to a core ingredient.\"\n\n\n\nRavi Dalal\n\nSenior Computer Vision Engineer, Walmart\n“I think 2020 has changed the world around us in many ways, and the retail sector is no exception. The pandemic has taught everyone new ways of shopping whether it's online, curbside pickup or concierge services for senior members. And AI/ML has played a major role in shaping all these solutions everywhere. Going in 2021, I feel rather than quantity of data, the market will shift towards the quality of data being captured. As more and more data points are getting captured to make better ML models; need of the hour is to make light weight models that can run on edge devices to filter and capture just the meaningful data. I call it -- \"AI/ML for Intelligent Data collection.”\n\n\n\nShaina Raza\n\nResearch Assistant, Ryerson University\nShaina, a former lecturer and now Research Assistant in ML gave one of the more exciting predictions on the list, which would mean large scale change and positive development in the field in the months to come. “Machine Learning is going to be easier, cheaper and beyond the limitations. The exponential power of the computing resources will be in the hands of laymen, and we see the revolution of the world much more earlier than expected.”\n\n\n\nYaman Kumar\n\nPhD Computer Science, University of Buffalo\nYaman’s research in ML revolves around Adversarial Networks, Interpretability, QA and Speech and is supported by Google doctoral Fellowship. Yaman simply suggested three compartments of ML which he believes will develop rapidly in 2021. “Unsupervised Learning, Explainability, and Fairness in ML.” Surprisingly, it was the only mention of unsupervised learning, which has seen to become somewhat of a buzzword at the backend of 2020.\n\n\n\nJack Brzezinski\n\nChief AI Scientist at AI Systems \u0026 Strategy Lab\nI feel that the role of knowledge will be increasing. Structures, various knowledge representation types will be essential for the next wave of AI innovation. The lawmakers might soon require AI, ML models to be compliant with multiple statutes or regulations.\n\n\n2021 is set to be uncertain in many ways. However, as with last year, hard work behind the scenes in AI and ML continues. The range of optimistic yet realistic predictions above, if accurate, could see 2021 as a year in which some of the largest steps in AI and Machine Learning advancement for some years take place.\n\nWhat do you think could be developed this year?","seo_title":"What's next? 17 Machine Learning Predictions for 2021","slug":{"_type":"slug","current":"17-machine-learning-predictions-for-2021"},"tags":[{"_key":"94W8kEob","label":"Machine Learning","value":"Machine Learning"},{"_key":"CKYpD6ea","label":"AI","value":"AI"},{"_key":"CzXbg3oJ","label":"Expert Advice","value":"Expert Advice"}],"title":"What's next? 17 Machine Learning Predictions for 2021"},{"_createdAt":"2021-01-19T17:04:40Z","author":{"_id":"7aaa2439-b7e5-45f9-8aee-1fbf3c9b5fb8","avatar":{"_type":"image","asset":{"_ref":"image-0a1ace27b14b286beed42f8a189b8d4c9f8d5e71-512x512-webp","_type":"reference"}},"bio":"Dimitri is a software developer specializing in computer vision. Currently a machine learning developer at Sama, Dimitri has developed machine learning based products for the entertainment and image processing industries. He holds a Masters degree in Electrical Engineering from McGill University, where he performed research on active object recognition. He is passionate about the use of technology in order to combat income inequality and climate change. Outside of work, he can be found playing beach volleyball or table tennis depending on the time of year.","name":"Dimitri Gallos","slug":{"_type":"slug","current":"dimitri-gallos"}},"estimatedReadingTime":3,"featured_image":{"_type":"image","asset":{"_ref":"image-04641081798c41caebebefa44685828d80b7434f-1200x675-png","_type":"reference"}},"meta_description":"The Sama MLOps Pipeline: At Sama, we decided to build our own automated training pipeline in order to limit costs, and to avoid tying ourselves to a particular cloud provider.","openGraphImage":null,"plaintextBody":"Training computer vision models are notoriously computationally intensive, often requiring multiple GPUs. It's therefore usually not performed locally. One of the challenges when it comes to launching training jobs on the cloud or private GPU clusters is dealing with all the required manual steps. For example, when using AWS, ML engineers need to spin-up an EC2 instance manually to launch a training job, and then manually decommission it once the training job is completed. \n\nAlthough commercial tools do exist to automate this process (for example, SageMaker or DataBricks), at Sama we decided to build our own automated training pipeline in order to limit costs, and to avoid tying ourselves to a particular cloud provider.\n\nOur pipeline allows our ML Engineers and Scientists to launch a training job on the cloud by simply pushing the code they developed locally to a predefined git branch. This is very simple to achieve using a modern CI/CD platform like Codefresh. A Codefresh “trigger” can be set to track commits to specific git branches of a repository. Once a commit is pushed to a target branch, a Codefresh pipeline is triggered. The pipeline is just a workflow defined in a yaml file that executes the following steps :\n\nClone the tracked git repository, as well as any other repos required (in our case we have a separate repo for all our ML related tools).\n\nBuild a Docker image with all the project specific dependencies.\n\nDeploy the training job on the GPU cluster.\n\nSend a slack message to a channel that tracks all the training jobs whenever the results are ready or if there is an error.\n\nThe specific implementation of step 3 above depends on the particular frameworks and libraries that are used to train the models and track their performance, as well as the cloud provider. In our team, we use Kubernetes to orchestrate the creation and decommission of the instances. We also use mlflow to manage the ML lifecycle, which has built-in Kubernetes deployment support. So for us, this step simply reduces to doing some cloud provider specific configuration and running an mlflow experiment with Kubernetes as the backend.\n\n\n\nSummary of the Sama automated training pipeline: from pushing the code to Github to running the training job on AWS.\n\n\n\nAside from the obvious time savings, the advantage of this setup is that it is fully configurable and can be made to work with any cloud provider, or even a private GPU cluster. As an added bonus, it enforces experiments to be separated in different git commits, which we find is good practice.","seo_title":"The Sama MLOps Pipeline: Automating Model Training on the Cloud","slug":{"_type":"slug","current":"part-1-automating-model-training-on-the-cloud"},"tags":[{"_key":"OdrqBpR4","label":"Sama Engineering","value":"Sama Engineering"},{"_key":"Yw0P8Jjj","label":"Training Data","value":"Training Data"},{"_key":"08tVZdgm","label":"MLOps","value":"MLOps"},{"_key":"eMmu9s8R","label":"Featured","value":"Featured"}],"title":"The Sama MLOps Pipeline: Automating Model Training on the Cloud"},{"_createdAt":"2021-01-15T20:16:49Z","author":{"_id":"a009d418-aa96-47ac-a73a-fd2cd52c79d9","avatar":{"_type":"image","asset":{"_ref":"image-e0d717f753ba4876a6b0dbf9f125cf6c3d27e545-500x500-webp","_type":"reference"}},"bio":"Wendy Gonzalez is an executive passionate about building high-performing, high-functioning teams that develop and scale innovative, impactful technology. With two decades of managerial and technology leadership experience for companies including EY, Capgemini, Cycle30 (acquired by Arrow Electronics) and General Communications Inc, Gonzalez is currently the CEO of Sama, the provider of accurate data for ambitious AI, used by leading technology companies such as Walmart, Google, Nvidia and Getty. Before taking on her role as CEO, Gonzalez spent 5 years at Sama as COO, and is an active Board Member of the Leila Janah Foundation.","name":"Wendy Gonzalez","slug":{"_type":"slug","current":"wendy-gonzalez"}},"estimatedReadingTime":3,"featured_image":{"_type":"image","asset":{"_ref":"image-fbafe834a890fa166be72754be9610ccc9693cd2-2844x1600-png","_type":"reference"}},"meta_description":"Samasource is now Sama. The same team that powers the world’s most ambitious AI, but with a new name that represents our vision moving forward.","openGraphImage":null,"plaintextBody":"As we welcome 2021, we’re ready to embrace new beginnings. Therefore, we’re proud to announce that we’re now Sama, the same team that powers the world’s most ambitious AI projects with high-quality and accurate data, but now with a new name that represents our vision moving forward.\n\nThis change comes at a time when our company is at an inflection point. Our technology has quickly become the preferred option by some of the most sophisticated companies in the world. I know Leila would be proud to see where our company is today and that we are finally sharing Sama, a name we’ve used internally for years, with the world.\n\nIf there’s one thing we can all agree on, it’s that 2020 was a tumultuous year. From a deadly pandemic to global protests and a national presidential election - its events are sure to impact our society for years and decades to come.\n\nAs many of you know, our founder, Leila Janah, passed away following a battle with epithelioid cancer. Since then, we’ve worked to fulfill her legacy through technological innovation, navigated the challenges of COVID-19, and stepped up to address the ongoing challenges of bias in artificial intelligence (AI).\n\nDespite its challenges, 2020 was a year of milestones and achievements for our team. From our recognition on the Inc. 5000 list as one of North America’s fastest-growing private companies to launching our Machine Learning Assisted Annotation, we look back on 2020 with an overall optimism for the future.\n\nSama reflects several business developments we’ve made over the past few years. Starting with our transition to a for-profit business model in 2018, we’ve been focused on scaling our business to continue to provide high-quality training data for ambitious AI for some of the world’s leading brands such as Google, NVIDIA, and Walmart.\n\nSince transitioning our business model, we’ve streamlined our approach to focus on accuracy, consistency, and simplicity, launching solutions like PII Data Anonymizer and Machine Learning Assisted Annotation. These solutions leverage automation in addition to human oversight, allowing efficient yet accurate data annotation and validation to take place. Moving forward, our focus is on further developing our tech as we expand our presence in industries such as retail, biotechnology, and media communications.\n\nWe are excited to share this milestone in our company’s history as we embark on our journey to becoming the world’s leading training data provider. Our new name, in conjunction with our doubled down approach to technology, will guide our company and our mission for years to come.\n\nWithout our community's help, we could not have gotten to where we are today, so thank you to all the Sama employees, investors, and partners for supporting us throughout this transitional journey.\n\nWe can’t wait for you to see what’s next.","seo_title":"We Are Now Sama: Accurate Data For Ambitious AI","slug":{"_type":"slug","current":"samasource-is-now-sama"},"tags":[{"_key":"GS7wrNs8","label":"Company News","value":"Company News"},{"_key":"BSu2iKoe","label":"Leila Janah","value":"Leila Janah"},{"_key":"YLf0JPMd","label":"Data Quality","value":"Data Quality"}],"title":"We Are Now Sama: Accurate Data For Ambitious AI"},{"_createdAt":"2020-12-21T18:21:43Z","author":{"_id":"785c9b8f-1869-4ce3-9eaa-c53945aa9736","avatar":{"_type":"image","asset":{"_ref":"image-c187f0a84bc4d1a6870d3a7a8528f242920a33aa-309x343-webp","_type":"reference"}},"bio":"Aurelie Drouet leads Product Marketing at Sama. Hailing from France and with expertise in driving revenue growth for companies in the US and Latin America, Aurelie's expertise spans several countries. She previously led Strategic Partnerships at Dreem.","name":"Aurélie Drouet","slug":{"_type":"slug","current":"aurelie-drouet"}},"estimatedReadingTime":9,"featured_image":{"_type":"image","asset":{"_ref":"image-bc8699367ba2089c548213191c632bbbe6c784a9-1280x640-png","_type":"reference"}},"meta_description":"If you have a passing interest or are just looking for a refresher in all things Machine Learning, we have put together a list of 10 books.","openGraphImage":null,"plaintextBody":"With the year coming to a close and not many other things to do than to stay put at home, this holiday season could be the perfect time to dive deeper into some books. If you have a passing interest or are just looking for a refresher in all things Machine Learning, we have put together a list of 10 books. Although published at varying points in the development of Deep Learning and Machine Learning, each book offers unique insights. It was near impossible to narrow the list to just ten, but we couldn’t look past those below.\n\n\n1. Grokking Deep Learning\nAndrew W Trask (2009)\n\nGrokking Deep Learning is suggested to be a perfect place to delve into the subset of Machine Learning, not only describing and explaining APIs and frameworks, but also talking the reader through how they can actually build algorithms from scratch. This hands-on style of writing will help you build an AI capable of beating you in a classic game of Atari and Neural Networks capable of understanding basic images. While this is not a beginners guide, experience with calculus is not required, merely a high school level of mathematical understanding.\n\n2. The Hundred-Page Machine Learning Book\nAndriy Burkov (2019)\n\nAn all you need to know guide to Machine Learning in just 100 pages, what more could you need? The Director of Data Science at Gartner, Andriy, suggests that you are a mere read of this book away from being ready to build complex AI systems, pass an interview or start your own business. Also available on Kindle, the 2019 release covers gradient descent, cluster analysis, dimensionality reduction and more. Is it for you? Andriy suggests that it is suitable for those both working in the field and those dipping their toe to find out more about the increasingly complex field of Machine Learning.\n\n\n3. Introduction to Machine Learning with Python: A Guide for Data Scientists\nSarah Guido \u0026 Andreas C. Mueller (2016)\n\nAlthough released in 2016, this 400 word bible for Machine Learning gives a great grounding in the basics of ML, providing a thorough and hands-on approach to Python use in ML. Learn not only what the most important concepts and algorithms are, but also when and how to use them. Imperative topics including machine learning workflow: data preprocessing and working with data are covered, as well as training algorithms, evaluating results, and implementing those algorithms into a production-level system\n\n\n4. Machine Learning For Absolute Beginners: A Plain English Introduction\nOliver Theobold (2017)\n\nA curveball, maybe, as we realize that those reading this list may have experience in the field, however, Machine Learning for Absolute Beginners walks through ML history and works in plain english with no coding experience necessary. What exactly will you be learning? The very basics including, decision trees, regression analysis, data reduction, k-means and more, giving you a great underlying understanding of the building blocks used in Machine Learning and how they can be used. Finally, some career advice with Oliver talking you through career options and how best to utilize the ML knowledge just picked up post-read.\n\n\n5. Mathematics for Machine Learning\nMarc Peter Deisenroth (2020)\n\nThis textbook puts the normally disparate course style of teaching in Mathematics to shame, combining together all of the fundamental mathematical tools needed to understand machine learning, including linear algebra, analytic geometry, matrix decompositions, vector calculus, optimization, probability and statistics. These concepts are then used to derive four central machine learning methods: linear regression, principal component analysis, Gaussian mixture models and support vector machines, giving a brilliant starting point for those entering the field and those looking for a refresher. Alongside the textual information, this book also includes examples and tests to ensure the reader's understanding.\n\n\n6. Pattern Recognition and Machine Learning\nChristopher M. Bishop (2007)\n\nChristopher Bishop’s Pattern Recognition and Machine Learning presents approximate inference algorithms that permit fast approximate answers in situations where exact answers are not feasible. The first of its kind on pattern recognition to present the Bayesian viewpoint, uses graphical models to describe probability distributions, which at the time, was not evident in any other ML text. Unlike some of the other inclusions on this list, familiarity with multivariate calculus and basic linear algebra is required, and some experience in the use of probabilities would be helpful though not essential.\n\n\n7. Probabilistic Graphical Models: Principles and Techniques\nDaphne Koller \u0026 Nir Friedman (2009)\n\nA general framework for constructing and using probabilistic models of complex systems that would enable a computer to use available information for making decisions. This book isn’t for beginners, nor for the faint of heart as it dives right into probabilistic graphical models in detail, including Bayesian and Markov Networks, inference, and learning from complete / incomplete data. If you want to get the most out of this book, there’s an option to attend Daphne Koller’s lectures on Probabilistic Graphical Models at Stanford University, on Coursera. Fun fact, Koller is actually one of the founders of Coursera, an online education platform.\n\n\n8. Machine Learning: A Bayesian and Optimization Perspective (Net Developers)\nSergios Theodoridis (2015)\n\nThe book builds carefully from the basic classical methods to the most recent trends of the time, including chapters on pattern recognition, statistical/adaptive signal processing, statistical/Bayesian learning, as well as short courses on sparse modeling, deep learning, and probabilistic graphical models. All of the major techniques you’ll need to know prior to working in the field are covered, including Mean/Least-Squares regression and filtering, Kalman filtering, stochastic approximation and online learning, Bayesian classification, decision trees, logistic regression and boosting methods. Alongside all of the theoretical text, this book includes case studies, code to be experimented with and more.\n\n\n9. Machine Learning: A Probabilistic Perspective\nKevin P. Murphy \u0026 Francis Bach (2012)\n\nKevin describes this text as a comprehensive introduction to machine learning methods that use probabilistic models and inference as a unifying approach. This overview text combines breadth and depth, offering necessary background material on such topics as probability, optimization, and linear algebra as well as discussion of recent developments in the field, including conditional random fields, L1 regularization, and deep learning. Rather than providing a cookbook of different heuristic methods, the book stresses a principled model-based approach, often using the language of graphical models to specify models in a concise and intuitive way. Alongside this, the software platforms used in examples are freely available online. Unlike some of the other texts in this list, Machine Learning: A Probabilistic Perspective is suitable for upper-level undergraduates, giving an ideal introduction to ML and Mathematical formulas.\n\n\n10. Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems\nAurelien Geron (2019)\n\nIn the last few years, breakthroughs in Deep Learning have boosted the entire field of machine learning. Now, even programmers who know close to nothing about this technology can use simple, efficient tools to implement programs capable of learning from data. Through minimal use of theory and maximum practical examples, this text helps you gain an intuitive understanding of the concepts and tools for building intelligent systems. You'll learn a range of techniques, starting with simple linear regression and progressing to deep neural networks. With regular examples throughout, it’s a great book to not only grasp how and why it all works, but to also test it yourself.","seo_title":"10 Must-Read Machine Learning Books","slug":{"_type":"slug","current":"10-must-read-machine-learning-books"},"tags":[{"_key":"R9BEAeeH","label":"Machine Learning","value":"Machine Learning"},{"_key":"OUl8Z7DG","label":"Best of","value":"Best of"}],"title":"10 Must-Read Machine Learning Books"},{"_createdAt":"2020-12-15T21:43:21Z","author":{"_id":"a3099d34-9595-4978-b517-e508196414c1","avatar":{"_type":"image","asset":{"_ref":"image-6643136d6c33f77b8e49366c166642ca5dafba8d-500x500-webp","_type":"reference"}},"bio":"Frédéric is a researcher and team leader with over 15 years of R\u0026D experience in machine learning, AI, NLP, speech recognition, and computer vision. Currently Head of AI at Sama, he has worked on building ML-based products in multiple industries from Healthcare to Retail, in large companies and startups. He cares about the impact of technology, and outside of work, you can often see him on a bicycle or on skis.","name":"Frederic Ratle","slug":{"_type":"slug","current":"frederic-ratle"}},"estimatedReadingTime":8,"featured_image":{"_type":"image","asset":{"_ref":"image-6abd2f846119ba50a8325787f99ec3aeec68ffff-1076x605-png","_type":"reference"}},"meta_description":"In this article we summarize an approach that we have developed to speed up polygonal instance segmentation using machine learning.","openGraphImage":null,"plaintextBody":"At Sama, Vector Annotation of objects using polygons is a task that our expert annotators spend a great deal of time on. This is especially true for projects involving autonomous vehicles, where it is typical to apply instance segmentation to label scenes comprising hundreds of frames, each with multiple objects (vehicles, pedestrians, traffic signs, etc.) like you see in Figure 1.\n\nFigure 1. Example of Polygonal Annotation in Sama.\n\n\n\nIn this post, we summarize an approach that we have developed to speed up polygonal instance segmentation using machine learning. This approach was presented earlier this year at the CVPR Workshop on Scalability in Autonomous Driving, and the ICML Workshop on Human-in-the-Loop Learning.\n\nFew-Click Annotation\n\nBuilding instance segmentation Deep Learning (DL) models for autonomous vehicles requires a significant amount of labeled data. The use of Machine Learning (ML) for producing pre-annotations to be reviewed by human annotators, whether in an interactive setting or as a pre-processing procedure, is a very popular approach for scaling up labeling while controlling the costs.\n\nMultiple approaches have been suggested for machine-assisted instance segmentation. These typically consist of a DL-based segmentation of the object(s) integrated into a human-in-the-loop system. The human can interact with the system by correcting the model output, initializing the model with one or several clicks, or a combination of those steps. Examples of such systems include Polygon-RNN++ [1], DELSE [7], DEXTR [3], and CurveGCN [2]. Those systems all present good results, but some open questions remain:\n\nDo these methods perform well when a production-level accuracy is required, as when working for a customer project?\n\nDoes the choice of annotation tool influence the results? The gains to be made by using ML depend on how difficult it is for humans to draw polygons in the provided UI. Here we used our optimized drawing tool for polygons, which is part of our labeling platform.\n\nML integration is not usually approached from a human-centric perspective. Beyond the optimization of traditional metrics like IoU, what interactions are most desirable and how should we present the output of the model to annotators?\n\nOur method relies on combining the well-known DEXTR [3] approach with a raster-to-polygon algorithm, to make the result more easily editable. This is not unlike what other tools (such as CVAT) have implemented, though we have optimized this approach for our specific use cases using A/B testing.\n\nThe Model\n\nOur instance segmentation model is based on the well-known Deep Extreme Cut (DEXTR) approach [3], along with a raster-to-polygon conversion algorithm that yields high quality polygons whose vertices are sampled in a way that reproduces human drawing patterns. The model uses the few clicks provided by human annotators at inference time. The steps are described in Figure 2.\n\nFigure 2. An overview of the approach.\n\n\n\nRegarding the model itself, we adopted a custom version of the UNet [5] along with an EfficientNet backbone [6] (instead of the ResNet backbone used in the paper).\n\nIn our experience, for human annotators to produce good instance segmentation masks efficiently, a polygon annotation tool should be used. As such, we needed to convert the raster masks produced by our model to high quality polygons. To add to the challenge, humans tend to produce sparse polygons, adding vertices only when necessary. We therefore adopted a raster-to-polygon procedure that minimizes the number of output vertices.\n\nA/B Testing the Approach\n\nAt Sama, we use A/B testing as much as possible to systematically refine and improve our new features. To this end, we have developed a flexible testing infrastructure that can ingest and aggregate data from multiple internal processes and is made available to anyone within the organization.\n\nThis framework measures the statistical impact of proposed changes on our efficiency metrics (such as drawing time or shape adjustment time). The significance of observed differences on a given efficiency metric is evaluated using statistical tests.\n\nToy A/B Tests\n\nWe conducted an A/B test of the method using a synthetic automotive dataset called SYNTHIA-AL [8]. The dataset's images and corresponding annotations were generated from video streams at 25 frames per second (FPS). Figure 3 shows SYNTHIA image examples, along with their segmentation (done manually and with the Few-Click tool).\n\nFigure 3. SYNTHIA example images, along with their manual and semi-automated annotations.\n\n\n\nThe test, applied only to motor vehicles, reproduced realistic annotation guidelines, namely:\n\nThe drawn polygon needs to be within 2 pixels of the edge of the vehicle.\n\nAll vehicles down to 10 pixels (height or width) need to be annotated.\n\nFollowing this test, we found a nearly 3-fold reduction in annotation time. On the other hand, we also found that on some of the more complex shapes, annotators were spending quite some time manually adjusting the ML output. DEXTR's authors originally showed that the segmentation can be improved with additional clicks beyond the four initial ones. We therefore extended our few-click tool to allow online refinement of the polygons by considering modifications to their vertices as extra clicks. At train time we simulated the corrective clicks by considering the point of greatest deviation between predicted mask and ground truth as illustrated in Figure 4.\n\nProblem: DEXTR’s 4 extreme clicks are not always sufficient.\n\nObservation: DEXTR trained on 4 clicks benefits from additional clicks.\n\nSolution: Fine-tune DEXTR model with additional clicks for hard samples as established by IoU at train time.\n\nFigure 4. Integrating additional clicks in the training process.\n\n\n\nUsing this method, annotators are able to re-trigger the model inference with an additional click, instead of manually adjusting the output. We proceeded to a second toy A/B test, and results showed that we could obtain a theoretical efficiency gain of up to 3.5x on vehicles using the improved method.\n\nDownload our paper on Human-Centric Efficiency Improvements in Image Annotation for Autonomous Driving here and stay tuned to hear more about our latest advances!\n\n\n\nReferences\n\nAcuna, D., Ling, H., Kar, A., and Fidler, S. Efficient annotation of segmentation datasets with polygon-rnn++. In CVPR, 2018.\n\nLing, H., Gao, J., Kar, A., Chen, W., and Fidler, S. Fast interactive object annotation with curve-gcn. In CVPR, 2019.\n\nManinis, K.-K., Caelles, S., Pont-Tuset, J., and Van Gool, L. Deep extreme cut: From extreme points to object segmentation. In Computer Vision and Pattern Recognition (CVPR), 2018.\n\nPapadopoulos, D., Uijlings, J., Keller, F., and Ferrari, V. Extreme clicking for efficient object annotation. In ICCV, 2017.\n\nRonneberger, O., Fischer, P., and Brox, T. U-net: Convolutional networks for biomedical image segmentation. CoRR, abs/1505.04597, 2015. URL http://arxiv. org/abs/1505.04597.\n\nTan, M. and Le, Q. V. Efficientnet: Rethinking model scaling for convolutional neural networks. CoRR, abs/1905.11946, 2019. URL http://arxiv.org/ abs/1905.11946.\n\nWang, Z., Acuna, D., Ling, H., Kar, A., and Fidler, S. Object instance annotation with deep extreme level set evolution. In CVPR, 2019.\n\nZolfaghari Bengar, J., Gonzalez-Garcia, A., Villalonga, G., Raducanu, B., Aghdam, H. H., Mozerov, M., Lopez, A. M., and van de Weijer, J. Temporal coherence for active learning in videos. arXiv preprint arXiv:1908.11757, 2019.\n\nThis post was written by Frederic Ratle and Martine Bertrand.","seo_title":"Fast Vector Annotation with Machine Learning Assisted Annotation","slug":{"_type":"slug","current":"fast-vector-annotation"},"tags":[{"_key":"fJGSmFCx","label":"Vector Annotation","value":"Vector Annotation"},{"_key":"21OOfbwx","label":"Polygons","value":"Polygons"},{"_key":"SPABaoXN","label":"Sama Engineering","value":"Sama Engineering"},{"_key":"O5nmmFbm","label":"Featured","value":"Featured"}],"title":"Fast Vector Annotation with Machine Learning Assisted Annotation"},{"_createdAt":"2020-12-01T19:25:19Z","author":{"_id":"26fb3cc4-608d-40e1-bb4d-955bceda232a","avatar":{"_type":"image","asset":{"_ref":"image-f1fd7fbcc4633299cdbedddba22cb44e24f17317-518x518-svg","_type":"reference"}},"bio":"From self-driving cars to smart hardware, Sama fuels AI. Founded over a decade ago, we’re experts in image, video and sensor data annotation and validation for machine learning algorithms in industries including automotive, navigation, AR/VR, biotech, agriculture, manufacturing, and e-commerce. Our staff are driven by a mission to expand opportunity for low-income people through the digital economy, and our social business model has helped over 50,000 people lift themselves out of poverty.","name":"Sama Team","slug":{"_type":"slug","current":"sama-team"}},"estimatedReadingTime":3,"featured_image":{"_type":"image","asset":{"_ref":"image-c30da58e631de99c9639604c9831763911b5860f-6240x3120-jpg","_type":"reference"}},"meta_description":"Samasource is thrilled to announce that Wendy Gonzalez is our new Chief Executive Officer.","openGraphImage":null,"plaintextBody":"Today, the Sama Board of Directors, leadership team and employees are thrilled to announce that Wendy Gonzalez is our new Chief Executive Officer.\n\nHaving served as President and COO at Sama since 2018, Wendy stepped into the role of Interim CEO when our CEO and founder, Leila Janah, passed away in January of 2020 following a battle with epithelioid sarcoma.\n\nOver the past year, Wendy has successfully led the Sama team through the uncertainty of 2020 while achieving significant organizational milestones along the way. As a company, we’ve continued to develop our leading training data platform, including launching Machine Learning Assisted Annotation which has improved annotation efficiency by over 300% to regularly deliver quality SLAs between 96-99%.\n\nIn addition to recruiting an engineering workforce of more than 30% Ph.Ds., under Wendy’s leadership, Sama has achieved a three-year revenue growth of 134%, gaining recognition on the Inc. 5000 list as one of America's fastest-growing private companies.\n\nWendy has over two decades of managerial and technology leadership experience for companies including EY, Capgemini Consulting and Cycle30 (acquired by Arrow Electronics), and is an active Board Member of the Leila Janah Foundation. Wendy’s commitment to actualizing Leila's vision at Sama is unwavering.\n\n\"We have been determined to find the right person to carry forward Leila's vision and commitment to Sama, and that woman is Wendy,\" said Ben Metcalfe, Member of Sama's Board of Directors and Board Advisor at Ridge Ventures. \"Following a thorough recruitment process, we're certain that Wendy is the right person to lead Sama through this next stage.\"\n\nIn her new role, Wendy will drive the technology, innovation and growth that Leila began. As CEO, she becomes a part of a small group of female leaders within the male-dominated AI industry and will work not only to prioritize innovation but empower people around the world to be part of the growing AI economy.\n\nOver the years at Sama, Wendy has become a trusted mentor and thought leader. As a team, we look forward to being a part of her success.\n\nCongratulations, Wendy!\n\nRead more about the news in our press release here and follow Wendy on Twitter and LinkedIn to stay up-to-date on future company announcements.","seo_title":"Meet Our New CEO: Wendy Gonzalez","slug":{"_type":"slug","current":"meet-our-new-ceo-wendy-gonzalez"},"tags":[{"_key":"PZYp5HYu","label":"Company News","value":"Company News"},{"_key":"Lx1s1Tlt","label":"Wendy Gonzalez","value":"Wendy Gonzalez"},{"_key":"BmcJBjt4","label":"Leila Janah","value":"Leila Janah"}],"title":"Meet Our New CEO: Wendy Gonzalez"},{"_createdAt":"2020-11-23T16:48:22Z","author":{"_id":"10ead718-57e1-41a8-b846-da3c81cc323a","avatar":{"_type":"image","asset":{"_ref":"image-a4c79da81bb1e23ce10fba84ea2cba5efe67a2a5-200x200-webp","_type":"reference"}},"bio":"Currently a Director of Product Management at Sama, Saul is passionate about the intersection of technology and social impact. He manages Sama’s data labelling products to ensure high quality training data efficiently and reliably reaches our customers. Experienced in both product and professional services, Saul is a proven leader who takes a data driven approach to expanding Sama’s capabilities and features. When not at work, you can usually find Saul enjoying the outdoors and spending time with his family.","name":"Saul Miller","slug":{"_type":"slug","current":"saul-miller"}},"estimatedReadingTime":3,"featured_image":{"_type":"image","asset":{"_ref":"image-0ea3313b4d00bb103e8441964eeb3968dfee2947-1144x644-png","_type":"reference"}},"meta_description":"Announcing our support for custom keypoint shapes in SamaHub, our training data platform trusted by the world's leading AI teams, for vector image and video annotation.","openGraphImage":null,"plaintextBody":"As a leader in high-quality training data, Sama supports clients across various use cases and applications. The ability to identify specific keypoint landmarks and track their relationship to one another is unlocking some of the most interesting developments in computer vision technology. This includes motion tracking like human pose identification for virtual fitness trainers or sports analytics. It also includes facial landmarks for emotion analysis, facial verification security, or driver alertness detection. It could also include hand gesture control for AR/VR or sign language transcription.\n\nWe are thrilled to announce our support for custom keypoint shapes in our training data platform trusted by the world's leading AI teams, for vector image and video annotation. While Sama has long supported other vector shapes like bounding boxes, polygons, and cuboids, these keypoint use cases require custom shapes that have a predefined number and order of points. Sama can now be configured to support skeletons, hands, eyelids or any other complex custom shape. Each keypoint can have its own label, color, and connection to other points. Multiple keypoint shapes are supported on the same annotation project.\n\n\nOptimizing for Quality and Efficiency\n\nThis new capability optimizes quality and efficiency in producing ground truth training data for our clients. Our expert annotators have a facilitated drawing experience where the shape builds itself as they annotate each point. The correct label for each point is inferred from the order that it is placed so no extra time is spent assigning labels to each point. We see quality improvement from visualizing the annotations as a cohesive connected shape instead of as free standing individual points. The custom keypoint shape also always has the same number and order of points—this means that no points could ever be omitted or mislabeled. In addition to the quality benefits, our A/B testing showed an 27% decrease in annotation time on a skeleton image annotation workflow over individual points.\n\nCompleted annotation data is returned to our clients in a structured delivery format that contains information about each point, its relative order in the array and its connection to other points.\n\nIf you're currently working on a computer vision algorithm that requires high quality keypoint annotations on images or videos, make sure to read more on our platform page or get in touch with an expert to discuss your training data needs.","seo_title":"Custom Keypoint Shapes for Vector Image \u0026 Video Annotation","slug":{"_type":"slug","current":"custom-keypoint-shapes"},"tags":[{"_key":"T7HkzkJx","label":"Product","value":"Product"},{"_key":"j9XlG6o5","label":"Keypoints","value":"Keypoints"},{"_key":"o2UoWoaC","label":"Video Annotation","value":"Video Annotation"},{"_key":"3GbpMBcq","label":"Vector Annotation","value":"Vector Annotation"}],"title":"Custom Keypoint Shapes for Vector Image \u0026 Video Annotation"},{"_createdAt":"2020-11-18T19:05:07Z","author":{"_id":"88e43c38-2712-41d4-9df3-8656c163c4db","avatar":{"_type":"image","asset":{"_ref":"image-af1346669e45d7264a36f051c18aea76061725bb-1895x1894-webp","_type":"reference"}},"bio":"Kyra is passionate about world-changing tech and sustainability, and happiest when these come together (looking at you, Sama). Stereotypical Dutch, she enjoys urban cycling and eating stroopwafels.","name":"Kyra Harrington","slug":{"_type":"slug","current":"kyra-harrington"}},"estimatedReadingTime":9,"featured_image":{"_type":"image","asset":{"_ref":"image-3651ed4b3fa5305116eb19a08c7ca1ac4b1ce130-900x506-png","_type":"reference"}},"meta_description":"12 Influential Women Working in Machine Learning, 2020.","openGraphImage":null,"plaintextBody":"Global spend on AI is predicted to be $98 Billion by 2023, up from 37.5 Billion in 2019. Maybe not unexpected for those witnessing it up close, but a whopping growth trajectory nonetheless. While machine learning models strive to mirror and predict real life as closely as possible, the people behind these models don't represent the real world. Despite this rapid forecasted growth, women still only make up a 12% of the ML workforce.\n\nAt Sama we believe in inclusive AI that benefits everyone, and believe highlighting role models is a key part in this work. With that in mind, we put together a celebratory list of some of the women we look up to and have spearheaded development in AI and Machine Learning in 2020. While it was near impossible to narrow down our list, we highly encourage you to connect with them and follow their incredible work going into 2021.\n\n\n\nFeryal Behbahani, Research Scientist, DeepMind\n\nFeryal obtained her BEng from Heron-Watt University in 2006, later going on to receive her MSc in Artificial Intelligence and Doctor of Philosophy in Machine Learning from Imperial College London. Post graduation, Feryal worked as a mentor at OpenAI and as a Research Scientist for Latent Logic (now Waymo). In 2019, Feryal started working as a Research Scientist at DeepMind, alongside volunteering at the Women in Machine Learning initiative as a director. Freyal’s work is currently focussed on Reinforcement Learning.\n\n\n\nMartine Bertrand, Lead AI, Sama\n\nThe desire to understand the universe led Martine to study physics at the University of Ottawa where she ultimately completed a Ph.D. in 2012. She then held Post-Doctoral Fellowships at the University of Carleton and her alma mater before undertaking a career as an industrial Research Scientist. She tackled challenges in computational chemistry at the Chemical Computing Group, natural language processing at Stradigi AI, and medical imaging at Imagia before joining Sama as Lead AI where she now steers ML R\u0026D efforts and guides the development of the MLOps infrastructure.\n\n\n\nJenny Sy, Data Scientist, USA for UNHCR\n\nCurrently working as a Data Scientist at the UNHCR, an organization which protects refugees and empowers them with hope and opportunity, Jenny is focussed on building the organizations analytics database, developing key department metrics, conducting research and more. Jenny obtained her B.Sc from Ateneo de Manila University and her MBA in Business Administration from China Europe International Business School. Alongside working at UNHCR, Jenny also volunteers as a Treasurer for the Women in Machine Learning organization, supporting Women in STEM fields.\n\n\n\nJulia Kroll, Data \u0026 ML Engineer, Amazon\n\nJulia Kroll is a Data and ML Engineer at Amazon, currently advising engineers at enterprise companies on migrating to and innovating with the AWS cloud. Julia also works on implement performant, scalable, and secure solutions on AWS, specializing in big data, analytics, and machine learning applications. Julia has also worked as a Data Engineer at Alexa artificial intelligence, following her role as a software engineer at HubSpot.\n\n\n\nKallirroi Dogani, Machine Learning Engineer, Facebook\n\nEarlier this year, Facebook gained a brilliant Machine Learning engineer in Kallirroi Dogani. Having previously worked as a ML Scientist at ASOS and Data Scientist at Tractable and Workable. Kallirroi obtained her second MSC in Artificial Intelligence from the University of Leuven, having received her first a year earlier from the University of Athens in Advanced Information Systems.\n\n\n\nLucy Wang, Machine Learning Engineer, Twitter\n\nCurrently working as a Machine Learning Engineer at Twitter, Lucy is focussed on the use of Machine Learning in Healthcare. Lucy previously held positions of Staff and Senior Data Scientist as Buzzfeed having earlier graduated from Columbia Engineering with an M.S in Computer Science. Aside from ML, Lucy holds interests in Natural Language Processing and Deep Learning.\n\n\n\nTobi Bosede, Founder \u0026 CEO, Ilekun Health\n\nTobi obtained a BA in Mathematics from the University of Pennsylvania and MSE from John Hopkins University in Applied Mathematics and Statistics before beginning her career as a Software Engineer at JPMorgan Chase. In the years since, Tobi has held positions as a Data Scientist at Sprint, Researcher at John Hopkins University and Lead in ML at Capital One, all prior to becoming the founder of Ilekun Health, a smart technology company in the healthcare space. Ilekun Health is a technology company that gleans insight around provider quality, services offered, and price from a complex deluge of unstructured health data using artificial intelligence (AI)—currently raising initial pre-seed.\n\n\n\nTian Su, Director of Machine Learning, Walmart\n\nAs an experienced Data Scientist, Tian is currently working as Director of ML at Walmart. Having previously held positions of Senior Data Scientist \u0026 Head of AI/ML at 7-Eleven, Tian has been heavily focussed on personalization and delivery for customers in the CPG market. Dr Tian holds considerable experience and skill in Advanced Analytics, Data Mining, Statistical Modeling, Machine Learning, Databases and Artificial Intelligence. She also boasts a strong research background with a Ph.D. from Yale University and Master’s Degree focused on Computer Science from Georgia Institute of Technology.\n\n\n\nNicole Barberis, Machine Learning \u0026 Quantum ML, IBM\n\nNicole currently works as a Deep Learning and Quantum ML Developer for IBM in the US, aiding in the development of IBMs python solution. Nicole is a big believer in doing quantum machine learning (QML) research as she states this evolving field will eventually complement your modern suite of analytics solutions (machine learning, deep learning, etc.) Nicole worked at IBM for nine years as a Data Scientist and Information Security Analyst, before landing at her current position after two years at Bloomberg as a Data Scientist. Nicole received her MS in Applied Statistics from the University of Wyoming.\n\n\n\nSaeedeh Salimianrizi, Applied ML Scientist, Amazon\n\nHaving held several data science positions at companies including Verisk Analytics, Simarian and Farmers insurance, Saeedeh currently works as an Applied ML Scientist at Amazon in San Francisco. For the past two years, Saeedeh has been improving Amazon’s augmented reality pipeline acceptance rate using CNNs as well as building a CNN-based solution with 95% accuracy, eliminating the need for manual data annotation for shoe vendors. Saeedeh received her MSc in Systems Engineering from Boston University having also studied Industrial Engineering at the University of Tehran.\n\n\n\nQian (Wendy) Xiong, Machine Learning Engineer, Google\n\nAfter achieving a 3.95 GPA in her Statistics PhD, Qian stayed at the University of Colorado State for five years, first as a statistical consultant and latest a graduate teaching assistant. In 2018, Qian moved out of the educational setting altogether, starting as a Data Scientist/ML Engineer at the Expedia Group, working on conversational AI to provide intelligent and personalized automatic customer service. Having spent 18 months at Expedia, Qian moved to Google in April of this year. Qian is Proficient in Python (tensorflow/keras/pandas/scikit-learn/numpy), AWS, Linux, R and SQL. Hands-on Big Data experience with Spark.\n\n\n\nGalina Malovichko, PhD, Applied Machine Learning Scientist, Lyft\n\nGalina obtained her PhD from UC Davis in Condensed Matter and Material Physics, having previously received an MS in Physics from the Moscow Institute of Physics and Technology. Whilst studying, Galina worked as a PhD Student Researcher and Teaching Assistant, later moving to Lyft in 2018. Initially working as a Data Scientist analyzing new features for machine learning models, predicting Lyft rides ETA, Galina later moved on to be an Applied ML Scientist, a position she has held for the last two and a half years. In her current role, Galina has built an ML stack to predict travel times, built traffic detection models and more.","seo_title":"12 Women in Machine Learning to Watch","slug":{"_type":"slug","current":"12-women-in-machine-learning-to-watch"},"tags":[{"_key":"Fysfdyt0","label":"Machine Learning","value":"Machine Learning"},{"_key":"nnpdrksh","label":"Best of","value":"Best of"},{"_key":"2MmoF9VK","label":"Women in AI","value":"Women in AI"}],"title":"12 Women in Machine Learning to Watch"},{"_createdAt":"2020-11-16T23:15:17Z","author":{"_id":"f972de8a-10c1-45e3-97c9-ac490eaceabe","avatar":{"_type":"image","asset":{"_ref":"image-4aa17073cfd70d2e8f7d8ed85325c14cb1519577-692x691-jpg","_type":"reference"}},"bio":"Loic has over 20 years of industry experience in the Cloud services and AI industry. At Sama he works as the VP of Research \u0026 Development. His experience includes Fortune 500 Companies such as Salesforce.com, Unity Technologies, and AT\u0026T where he led the development of large scale AI, data analytics, and cloud solutions. Loic received his MS in computer science from UTBM, France.","name":"Loic Juillard","slug":{"_type":"slug","current":"loic-juillard"}},"estimatedReadingTime":3,"featured_image":{"_type":"image","asset":{"_ref":"image-8b71517603a306e0f80ea070e3d0d532f0039105-1024x512-png","_type":"reference"}},"meta_description":"Sama was a partner of the McGill Engineering Hackathon, the largest annual hackathon run by the McGill Electrical, Computer, and Software Engineering StudentÃƒÂ¢Ã¢â€šÂ¬Ã¢â€žÂ¢ Society.","openGraphImage":null,"plaintextBody":"For the second consecutive year, Sama was a Terabyte partner of the McGill Engineering Hackathon, the largest annual hackathon run by the McGill Electrical, Computer, and Software Engineering Student’ Society. This as part of our close partnership with McGill University and the broader Montreal Machine learning Technology community.\n\nIn this year defined by COVID-19, the CodeJam team opted for the very fitting “Digital by Default” theme. Staying on topic, we proposed our very own challenge with an “Online Retail and Shopping Smart App”, wherein students would get to interact with a custom fashion segmentation API trained on our iMaterialist open dataset.\n\nThe participation was incredible! Out of 27 teams, 9 tackled our challenge. The submissions were split into two categories:\n\nRecommendation Engine: “I have seen someone wear this, where can I find it?”\n\nThe Virtual Fitting Room: “How would this look on me”\n\nMost recommendation approaches involved extracting one or multiple pieces of clothing using the segmentation API and querying an image repository for similar items. The models were producing good recommendations when the piece of clothing was well defined. Results were less accurate when the quality of the source image segmentation was approximate. A number of factors such as the type of clothes, occlusion (hair, jewelry, etc.), and ambient picture attributes affect segmentation and produce an approximate product match.\n\n\n\nMy Wardrobe\n\nTeam “GradientBoys” took on the task of virtually showing clothes on a subject, a virtual fitting room of sort. They implemented a complex pipeline that involved segmenting the subject picture (person looking to try the clothes), extracting the style of clothing mask, and segmenting the article of clothing from the library. This was followed by clever usage of the OpenPose model for keypoint identification that allowed to extract a set of local and global distortions to modify the new piece of clothing to fit the subject. Completely taking the in-person shopping experience out of the equation, trying on clothes has never been this smart. Perhaps even more impressive considering it was built in less than 36 hours.\n\nAwesome work, team GradientBoys, and thank you McGill, and all of CodeJam Student Execs for organizing this amazing event. Looking forward to the years to come!","seo_title":"Code.Jam(2020)-McGill Hackathon: and the winner is A Virtual Fitting Room","slug":{"_type":"slug","current":"codejam-2020-mcgill-hackathon"},"tags":[{"_key":"Fd3ZhNBx","label":"Sama Engineering","value":"Sama Engineering"},{"_key":"3M2wKXav","label":"Events","value":"Events"}],"title":"Code.Jam(2020)-McGill Hackathon: and the winner is A Virtual Fitting Room"},{"_createdAt":"2020-09-24T16:19:34Z","author":{"_id":"80db3356-b402-44cb-a717-2e0f9c9fa3e4","avatar":{"_type":"image","asset":{"_ref":"image-399295e959574e371fe2ba7bfd55ba4aee8a589d-320x320-jpg","_type":"reference"}},"bio":"Heather is passionate about bringing world-changing technologies to market and using supply chain purchasing power for good. She is a data-driven strategist experienced in developing and leading go-to-market, communications, and sustainability initiatives at start-ups and multi-national organizations. Heather is most happy when she’s growing companies that make a positive impact, enjoying the outdoors, and spending time with her family.","name":"Heather Gadonniex","slug":{"_type":"slug","current":"heather-gadonniex"}},"estimatedReadingTime":4,"featured_image":{"_type":"image","asset":{"_ref":"image-2277f59b40f900b74161027781351788ae996308-2132x1066-jpg","_type":"reference"}},"meta_description":"We're humbled and honored to announce that, as of today, Sama is the first AI company to receive the prestigious B Corp certification.","openGraphImage":null,"plaintextBody":"I’ll never forget the first time I met Leila Janah. She was interviewing me and spoke excitedly about the potential of Sama achieving B Corp certification, one of her main priorities for the company. In the next breath, she talked about dominating the training data industry and catapulting Sama to claim its much-earned spot in the AI landscape.\n\nThis introduction was just the first of many conversations we would later have on the importance of sustainability, impact, and responsible hyper-growth.\n\nI’m deeply humbled and honored to announce that, as of today, Sama is among the first AI companies to receive the prestigious B Corp certification. The achievement comes on the heels of completing our three-year Randomized Control Trial (RCT), which demonstrated the actualized impact of our hybrid model and continued commitment to fusing purpose and profit, both internally and externally.\n\n\n\n“This achievement is the result of years of hard work and dedication from our entire Sama community. By securing B Corp certification and completing our RCT, we’re not only carrying Leila’s passion and commitment to impact, but we’re making a tremendous stride within the AI industry,” says Wendy Gonzalez, CEO of Sama. “I’d like to extend immense thanks to everyone who played a part in making Leila’s dream a reality.”\n\n\n\nB Corp certification is a measurement certified by B Lab, a non-profit company that measures a company’s social and environmental performance. We received a score of 98.5, excelling in the ‘Workers' and 'Community Impact' criteria, and placing us well above the 80 points necessary to achieve B Corporation status. Of the 3,500+ Certified B Corporations, Sama is the first and only AI company recognized and joins companies such as WeTransfer, Patagonia, Hootsuite, and Ben \u0026 Jerry’s, all who use business as a force for good.\n\nThree years ago, Leila kicked off what would become a rigorous journey towards completing a full RCT. This in-depth trial, often referred to as the \"gold standard\" in research, evaluated our training and employment effectiveness towards our mission of creating a sustainable pathway out of poverty. It studied three groups of individuals from similar socioeconomic backgrounds that were randomly assigned to receive either training through Sama (Group 1), training and the opportunity for employment at Sama (Group 2), or neither training nor the opportunity for employment (Control Group).\n\nOne of the trial’s main findings is that individuals who are trained and employed by Sama make higher levels of employment and earnings (Group 2). This study and subsequent completion of the RCT further validated the training programs we’ve created at Sama, an impact model that has helped 50,000 people lift themselves out of poverty, increased wages by 4x, and provided over 11,000 hours of training. \n\nBecoming B Corp certified and completing an RCT in tandem is a direct example of how our team at Sama is leveraging AI to address critical societal issues. This year has proved that making a real impact and having a mission-driven business model is what truly matters in driving the bottom line. We believe that receiving B Corp certification should be the new metric companies strive for in today’s day and age. It’s not just about chasing after unicorn status anymore, it’s about creating a successful company while creating meaningful change. \n\nWe’ve accomplished our goals in large part as a result of our supportive community, but it doesn’t stop here. We’re looking forward to continuing our commitment to making a lasting impact on the world and improving the lives of as many as possible.\n\nDue to our hardworking team and our supportive investors, friends and family, we’ve been able to follow through on the vision and legacy created by our founder, Leila Janah.\n\nOn behalf of the entire Sama team, thanks for taking this journey with us.","seo_title":"From Dreams to Reality: Our Journey to Becoming a Certified B Corporation","slug":{"_type":"slug","current":"we-are-a-b-corp"},"tags":[{"_key":"hZTYAcKb","label":"Company News","value":"Company News"},{"_key":"DDJ2bEmS","label":"Ethical AI","value":"Ethical AI"},{"_key":"oKWZS5Lg","label":"Impact","value":"Impact"},{"_key":"AuzJCOHb","label":"Leila Janah","value":"Leila Janah"}],"title":"From Dreams to Reality: Our Journey to Becoming a Certified B Corporation"},{"_createdAt":"2020-09-04T15:25:34Z","author":{"_id":"88e43c38-2712-41d4-9df3-8656c163c4db","avatar":{"_type":"image","asset":{"_ref":"image-af1346669e45d7264a36f051c18aea76061725bb-1895x1894-webp","_type":"reference"}},"bio":"Kyra is passionate about world-changing tech and sustainability, and happiest when these come together (looking at you, Sama). Stereotypical Dutch, she enjoys urban cycling and eating stroopwafels.","name":"Kyra Harrington","slug":{"_type":"slug","current":"kyra-harrington"}},"estimatedReadingTime":6,"featured_image":{"_type":"image","asset":{"_ref":"image-db6c1732497afdfb78c26af05cbd6a1641fe3258-2132x1066-png","_type":"reference"}},"meta_description":"AI is often framed as something that’ll change our future, but many people aren’t aware of quite the extent to which AI currently used in society and everyday life. ","openGraphImage":null,"plaintextBody":"AI is often framed as something that’ll change our future, but many people aren’t aware of quite the extent to which AI already used in society and everyday life. While it’s important to recognize that AI is still very much in its infancy in regard to large-scale change, there have been incremental advancements in recent years, which have somewhat gone under the radar for those not regularly perusing the latest AI news or working in said fields. To explore just how prevalent AI is in our everyday lives, we have collected five spaces in which AI is shaping consumer behavior and practices.\n\n1. Netflix\nThe entire catalogue of movies and shows at Netflix is ranked, customized and ordered for each individual user in a personalized manner using Machine Learning (yes, you can blame your roommate for messing up the algorithms and data). Through the use of Machine Learning, Netflix is able to forecast, and facilitate your next series binge with five key stages to their recommendation model: ranking \u0026 layout, similarities, evidence and search, model improvement and exploit learning. In short, Netflix uses customer engagement to find similarities and patterns in your watching data, alongside the categories you most often visit and even the artwork to which you’re drawn to. These hypotheses are then A/B tested for accuracy and developed to enhance user experience.\n\n2. Virtual Assistants\nOk, so this may seem like an obvious inclusion on this list, but many often don’t realize what it is that really helps to power Siri, Cortana and their industry colleagues. The seemingly simple command to action transaction is not as simple as it seems, with mere short sentences going through a variety of processes including recognition of wake words, speech processing, natural language understanding, text-to-speech and more. Many will remember that the initial virtual assistant offerings would often have many misspelled words, as well as an inability to understand the message and the obvious struggles with accents. However, projections for virtual assistants suggest that the already 20% voice search KPI on phones is set to increase, alongside adoption of VA into greater devices including cars and wearables. With the gradual implementation of VA into personal devices, it’s easy to overlook how far they have actually come!\n\n3. Online Shopping\nAI being used in e-commerce and online shopping is suggested to reach revenues of over $30 billion by 2025, which poses the question: how is it being implemented? Well, prediction of customer behavior is a crucial aspect of its success, auto-populating websites through algorithms made up of history, third party data, content data and other information to offer the necessary reference to the user. Alongside these examples, AI is also being used in the collection of data from consumers, listening to feedback, automation of review emails and product suggestions for retargeting. Alongside personalization and product recommendation, e-commerce retailers are now utilizing chatbots to provide 24/7 assistance, made possible through the development of self-learning capabilities and NLP. Interested in reading more? Check out our E-Commerce Guide.\n\n4. Education\nThere have been many early applications of AI in education, including, but not limited to grading, plagiarism detection, personalization and more. Recent developments in Machine Learning have seen companies like Gradescope using AI to develop assisted grouping techniques of handwritten answers from students. These groupings work on a points-based rubric allowing for accurate marking in seconds, which in-turn, can free up time for teachers and allows for greater lesson planning time, to name but a few benefits. Current AI algorithms are also being used to assist in the fight against plagiarism. Tools including Turnitin are using AI to locate copied sentences, structuring and other stylometrics alongside monitoring the creation and moderation dates of work. The future of AI in education, according to many, lies in the development of heavily tailored courses and classes for students with specific needs, which may not be as far off as you’d think with education startups receiving over 20 billion dollars worth of funding by the end of 2018.\n\n5. Ride Sharing\nRidesharing apps have been reported to use a variety of algorithms both for the benefit of the customers and organizations. It was only as recent as 2017 that Uber had admitted to adapting their pricing for rides based on customer data collected on socioeconomic factors of users. Seen as one of the first major adopters of AI, services like Uber have suggested that AI is central to almost every aspect of their business including fraud detection, risk assessment, safety processes, marketing spend and allocation, matching drivers and riders, route optimization and more. Ridesharing apps also use personal data in an anonymized and aggregated form to closely monitor which features of the service are used most. Through this, Uber and their competitors are able to analyze usage patterns and to determine where they should further focus developments.\n\nThese subtle implementations of AI across industries and in businesses may come as a surprise, but we have no doubt that over the next few years, AI adoption will not only become widespread, but increasingly publicized.","seo_title":"5 Examples of AI You Didn't Know You Used","slug":{"_type":"slug","current":"ai-you-didnt-know-you-used"},"tags":[{"_key":"uZeu8hE2","label":"AI","value":"AI"},{"_key":"oYVzsLww","label":"Best of","value":"Best of"}],"title":"5 Examples of AI You Didn't Know You Used"},{"_createdAt":"2020-08-26T18:18:07Z","author":{"_id":"80db3356-b402-44cb-a717-2e0f9c9fa3e4","avatar":{"_type":"image","asset":{"_ref":"image-399295e959574e371fe2ba7bfd55ba4aee8a589d-320x320-jpg","_type":"reference"}},"bio":"Heather is passionate about bringing world-changing technologies to market and using supply chain purchasing power for good. She is a data-driven strategist experienced in developing and leading go-to-market, communications, and sustainability initiatives at start-ups and multi-national organizations. Heather is most happy when she’s growing companies that make a positive impact, enjoying the outdoors, and spending time with her family.","name":"Heather Gadonniex","slug":{"_type":"slug","current":"heather-gadonniex"}},"estimatedReadingTime":3,"featured_image":{"_type":"image","asset":{"_ref":"image-10bbf2e602804aa2ffe6104a315daa9713a53334-2000x1000-png","_type":"reference"}},"meta_description":"Through our work with Vulcan, we have been awarded the winner of the 2020 AI Breakthrough Awards for the Best Image Processing Solution.","openGraphImage":null,"plaintextBody":"As the leading company providing high-quality training data and model validation software and solutions to more than 25% of the Fortune 50, we have always taken pride in our ability to ensure smart AI starts with quality data. Without high-quality, diverse and accurate data sets, AI just doesn’t work.\n\nToday, we are honored to announce that through our work with Vulcan, we have been awarded the winner of the 2020 AI Breakthrough Awards for the Best Image Processing Solution. \n\n“In one of the toughest years yet, our team has consistently prevailed and furthered our mission of providing high-quality data for some of the world’s largest tech companies,” says Wendy Gonzalez, Interim CEO of Sama. “It’s because of their persistence and dedication that we have the opportunity to celebrate this win among many others this year. We’re thankful for this recognition and are inspired to continue our work in driving AI forward.”\n\nIn 2019, the UN reported that nature is declining globally at rates unprecedented in human history. We are causing a devastating impact on global biodiversity, with nearly one million animal and plant species threatened by extinction. In an effort to mitigate this, our expert annotators applied our technology to advance AI algorithms, powering one of the leading wildlife conservation efforts with Vulcan, a project and investment company that uses data to make a positive impact on endangered species, climate change, ocean health, and more.\n\nThrough our work with Vulcan, we were able to address the current threat of wildlife trafficking while collecting data that will inform long-term strategies to protect endangered species and ensure stable and thriving generations. Through our technology, Vulcan has significantly improved its turnaround times to process training data, allowing algorithms to thrive without compromising on quality. To date, we have labeled nearly one million images for Vulcan, achieving above industry-standard quality SLA of 95% in support of their efforts. Together, we are combating large-scale poaching that contributes to extinction, while supporting a more sustainable ecosystem through image processing solutions.\n\nThe annual awards program honors AI excellence, innovation and success in a range of AI and machine learning related categories, including AI and Machine Learning Platforms, Smart Robotics, Business Intelligence and Analytics, Natural Language Processing, industry-specific AI applications and many more. This year they saw over 2,000 nominations globally.\n\nWe’re honored to have our work with Vulcan recognized by this prestigious program and are inspired to continue working towards our mission every day.\n\nCheck out the complete list of AI Breakthrough Award winners here: https://aibreakthroughawards.com/2020-winners/ ","seo_title":"Sama Wins 2020 Artificial Intelligence Breakthrough Award for Best Image Processing Solution","slug":{"_type":"slug","current":"artificial-intelligence-breakthrough-award"},"tags":[{"_key":"Z4dKhKCG","label":"Company News","value":"Company News"},{"_key":"r9kvSdJK","label":"Ethical AI","value":"Ethical AI"},{"_key":"rU8MHfKu","label":"Awards","value":"Awards"},{"_key":"uy3WXjRk","label":"Use Cases","value":"Use Cases"},{"_key":"0Qr5y568","label":"Data Quality","value":"Data Quality"}],"title":"Sama Wins 2020 Artificial Intelligence Breakthrough Award for Best Image Processing Solution"},{"_createdAt":"2020-08-24T22:30:00Z","author":{"_id":"26fb3cc4-608d-40e1-bb4d-955bceda232a","avatar":{"_type":"image","asset":{"_ref":"image-f1fd7fbcc4633299cdbedddba22cb44e24f17317-518x518-svg","_type":"reference"}},"bio":"From self-driving cars to smart hardware, Sama fuels AI. Founded over a decade ago, we’re experts in image, video and sensor data annotation and validation for machine learning algorithms in industries including automotive, navigation, AR/VR, biotech, agriculture, manufacturing, and e-commerce. Our staff are driven by a mission to expand opportunity for low-income people through the digital economy, and our social business model has helped over 50,000 people lift themselves out of poverty.","name":"Sama Team","slug":{"_type":"slug","current":"sama-team"}},"estimatedReadingTime":2,"featured_image":{"_type":"image","asset":{"_ref":"image-0434b713b7dcba1cdf98eff0eb50d7cb3d5fb008-1500x908-jpg","_type":"reference"}},"meta_description":"Samasource Honored on Inc. Magazine’s Annual List of America’s Fastest-Growing Private Companies—the Inc. 5000","openGraphImage":null,"plaintextBody":"What! We're honored and excited to share that we've made Inc. Magazine’s Annual List of America’s Fastest-Growing Private Companies, the most prestigious ranking of the nation’s fastest-growing private companies, coming in at No. 2952.\n\n“We’re honored to have been recognized by Inc. as one of the fastest-growing private companies in the US,” said Wendy Gonzalez, President and Interim CEO of Sama. “While this year has been challenging in many ways, it has been a year of growth for AI technologies and the high-quality data that powers them. This recognition is a testament to the hard work and dedication our team has demonstrated throughout the last decade. I’m proud of how far we’ve come and look forward to the advancements that lie ahead.”\n\nNot only have the companies on the 2020 Inc. 5000 been very competitive within their markets, but the list as a whole shows staggering growth compared with prior lists as well. The 2020 Inc. 5000 achieved an incredible three-year average growth of over 500 percent, and a median rate of 165 percent. The Inc. 5000’s aggregate revenue was $209 billion in 2019, accounting for over 1 million jobs over the past three years.\n\n“The companies on this year’s Inc. 5000 come from nearly every realm of business,” says Inc. editor-in-chief Scott Omelianuk. “From health and software to media and hospitality, the 2020 list proves that no matter the sector, incredible growth is based on the foundations of tenacity and opportunism.”\n\nWhat a great way to take stock of business and continue our journey together with renewed ambitions.\n\nComplete results of the Inc. 5000, including company profiles and an interactive database that can be sorted by industry, region, and other criteria, can be found here.","seo_title":"We made the Inc. 5000 list!","slug":{"_type":"slug","current":"we-made-the-inc.-5000-list"},"tags":[{"_key":"G24pVOQf","label":"Company News","value":"Company News"},{"_key":"96XHFEEw","label":"Awards","value":"Awards"}],"title":"We made the Inc. 5000 list!"},{"_createdAt":"2020-07-17T18:31:06Z","author":{"_id":"88e43c38-2712-41d4-9df3-8656c163c4db","avatar":{"_type":"image","asset":{"_ref":"image-af1346669e45d7264a36f051c18aea76061725bb-1895x1894-webp","_type":"reference"}},"bio":"Kyra is passionate about world-changing tech and sustainability, and happiest when these come together (looking at you, Sama). Stereotypical Dutch, she enjoys urban cycling and eating stroopwafels.","name":"Kyra Harrington","slug":{"_type":"slug","current":"kyra-harrington"}},"estimatedReadingTime":4,"featured_image":{"_type":"image","asset":{"_ref":"image-9e345dfd7e3354b64d6ad3e61e5b723a0650a32c-1200x628-jpg","_type":"reference"}},"meta_description":"From accidental Alexa purchasing to bias in recruitment, we have gathered 5 AI fails from the last few years.","openGraphImage":null,"plaintextBody":"AI is often hailed as the ‘next big thing’ and an answer to all of our problems. We’re certainly betting on it: global spend on AI is predicted to be a whopping $98 Billion by 2023, up from 37.5 Billion in 2019. But we’re still in the early days when it comes to fulfilling the promise of AI and one of the reasons is training data—the annotated data a machine needs to learn to see or hear. Training data is essential to the development of any machine learning model. A global survey of Data Scientists, AI Experts and Stakeholders revealed that 8 out of 10 AI projects fail and 96% run into problems with data quality and labeling. \n\nEven applications now leading the scene have had their fair share of mishaps along the way. We’ve listed some examples of when AI might not have been performing at its best.\n\n\n\n1. The Somewhat Vulgar Virtual Assistant\n\nIt only took 24 hours for Tay, Microsoft's new conversational understanding chatbot, to start tweeting some extremely insensitive and offensive material online. The initial idea was to have Tay learn through ‘casual conversation’ with fellow Twitter users, which perhaps, at that time, was too far-fetched. While a majority of the 100k+ tweets were not considered to be eyebrow-raising, a selection of mimicking and extremely offensive posts saw Tay taken down after just one day. Unfortunately, the follow-up chatbot, ‘Zo’ resulted in the same fate, albeit after five months of live interaction.\n\n\n\n2. IBM Watson for Oncology\n\nMany see AI as the future of medicine, but it’s been suggested that IBMs Watson for Oncology initially over-promised and underdelivered with misdiagnosis, incorrect drug treatment offerings and unsafe judgement calls on patients. While there was and still is promise, the current ‘messy’ state of healthcare systems and mismatch between both the working and learning styles of doctors and machines became evident. Watson Oncology, however, continues to improve, with memory banks of every rare disease, increasing knowledge and a lack of cognitive bias potentially held by long serving medical professionals.\n\n\n\n3. Amazon AI Bias Recruitment\n\nNow scrapped, the once used AI recruiting tool from Amazon was found to be perpetuating the gender gap in tech jobs. The initial hope for a perfect filtration system for the top applications, was soon found to host significant bias towards hiring men due to being trained on resumes submitted in previous years—which were predominantly from male applicants. The use of historical data which facilitated existing bias has seen great advancements in regard to hiring diverse talent, but continues to have a long road ahead.\n\n\n\n4. Purchasing on Alexa\n\nResearch suggests that by 2023 over eight billion voice assistants will be present in consumers' lives. We predict many of these will be subject of, or susceptible to, some humorous moments while in development. We have seen doll houses ordered through ads on television (oops!), parrots casually chatting with Alexa, hacking issues, and all sorts. There is no doubt that the virtual assistant has further developed to a more accurate level since, but we’re excited for the future of viral YouTube videos to come.\n\n\n\n5. Insurance company uses social media data to issue rates, shows bias\n\nAdmiral Insurance, a popular company for first-time drivers, aimed to embed AI to analyse the Facebook data of those applying for insurance as they claimed there was a ‘proven link’ between personality and their driving style or ability. Facebook was quick to stop any advancements in this due to underlying ethical concerns, data privacy and potential bias of models.\n\nWhile AI carries a lot of promise, it also carries a lot of hype. When it comes to AI, your model is only as good as the data it’s trained on, because here applies: garbage in, garbage out.","seo_title":"The Training Data Challenge: 5 AI Fails","slug":{"_type":"slug","current":"the-training-data-challenge-5-ai-fails"},"tags":[{"_key":"3firrVWY","label":"AI","value":"AI"},{"_key":"93GN7laV","label":"Best of","value":"Best of"},{"_key":"WeyhHs1O","label":"Data Quality","value":"Data Quality"}],"title":"The Training Data Challenge: 5 AI Fails"},{"_createdAt":"2020-06-29T19:47:48Z","author":{"_id":"1a59f036-e3fe-4f02-9a34-688ce45de143","avatar":{"_type":"image","asset":{"_ref":"image-7d8f236ba010dd4927d0c5a93368bdce1f712843-390x390-webp","_type":"reference"}},"bio":"Currently a Project Manager at Sama, Taylor Rouleau has a passion for ensuring ethical and sustainable practices in tech. After 5 years leading production teams for our customers, Taylor's expertise is applied internally in our Project Management Office. She heads up efforts to maintain our industry-leading data training processes with a special focus on Security \u0026 Compliance.","name":"Taylor Rouleau","slug":{"_type":"slug","current":"taylor-rouleau"}},"estimatedReadingTime":6,"featured_image":{"_type":"image","asset":{"_ref":"image-3faf334fa409d858100d8be69e4f3a1c87e85922-4096x2732-png","_type":"reference"}},"meta_description":"We explain the various types of machine learning algorithms including supervised, unsupervised and reinforcement learning, as well as business use cases.","openGraphImage":null,"plaintextBody":"Many will recognize the term AI or Artificial Intelligence, understanding that this broad term applies to almost any technique which allows for computers to mimic human behavior. Machine Learning is a subset of AI which includes abstruse statistical techniques, supporting gradual task improvement through experience gained. These broad sets of algorithms are used to extract useful models from raw data which are in turn used for a variety of mining tasks \u0026 synthetic tasks. In this blog we aim to explain both the various types of machine learning algorithms including supervised, unsupervised and reinforcement learning, as well as highlighting its business use examples. Many of the terms used in the blog can be further understood through Google’s extensive ML glossary, found here.\n\n\n\n“Machine learning research is part of research on artificial intelligence, seeking to provide knowledge to computers through data, observations and interacting with the world. That acquired knowledge allows computers to correctly generalize to new settings”\n\n- Yoshua Bengio\n\n\n\nSupervised Learning - Definition, benefits \u0026 limitations\n\nRecognized as the most common type of Machine Learning, supervised learning algorithms are designed to learn through example, hence the term ‘supervised’. To achieve this, the algorithm uses provided input and output data. This provided data is labeled to provide a base for future data processing. Using this data, the goal is to produce an accurate mapping function which in-turn allows for prediction of the desired output. Supervised learning is then further compartmentalized into a range of algorithms, including, but not limited to decision trees, logistic regression \u0026 support vector machines. Of course, as with many facets of AI, supervised learning has both advantages and disadvantages. Firstly, supervised learning is a simple process to understand and is extremely useful in classification problems. That said, supervised learning is not ‘real-time’ data, meaning that there will be delays in results required. Alongside this, supervised learning requires substantial computation time for training and is considerably more complex in comparison to unsupervised learning due to the need for labeling all inputs.\n\nUnsupervised Learning - Definition, benefits \u0026 limitations\n\nUnsupervised learning refers to the process of giving an algorithm no labeled data and leaving it to structure its own output. Through this lack of labeling, models using unsupervised learning can suggest subtle trends that would otherwise be unfound, especially when using semi-supervised learning. Unsupervised Learning can be seen as extremely beneficial, as it then becomes possible to uncover previously unknown patterns in data. The downside? Unsupervised learning results make it hard to find meaning in the data due to the lack of answer labels and this also makes it harder to compare to supervised learning tasks. Applications of unsupervised learning include clustering, anomaly detection and association mining.\n\nBefore moving on to reinforcement learning, it is important to also address semi-supervised learning. Semi-supervised learning sits between the two aforementioned methods, using a mixture of both tagged and untagged data to fit models. This technique is best suited for a large amount of data with both tagged and untagged sections. An example of this? Amazon’s Alexa! Jeff Bezos has previously spoken very highly of semi-supervised learning, suggesting that the reduced amount of labeled data needed to achieve the same accuracy improvement by 40 times.\n\n\n\nReinforcement Learning - Definition, benefits \u0026 limitations\n\nA recent buzzword, reinforcement learning is a technique used to aid the development of 'learning' in an environment, through the process of trial and error. This in turn, uses 'feedback' to correct itself. Unlike the above, whereby feedback provided to the agent is a correct set of actions for performing a task, Reinforcement Learning uses reward and punishment as signals for positive and negative behavior, with the goal to find a suitable action model that would maximize the total cumulative reward of the agent. This feedback design acts as a motivational factor for the RL-agent, whereby an understanding of outcome pushes the agent to learn the method of maximizing accumulated rewards over time. Applications of reinforcement learning are often seen in robotics for industrial automation, for data processing and in the creation of training systems. This technique has many positives, including being seen to solve various complicated problems which cannot be solved with conventional techniques including robotic movement and video game completion, similar to human problem solving in regard to process and repeat and the lessening in the potential for repeat mistakes.\n\nA challenge that should be recognized when looking at RL is the time it takes to generate data. This seems to be commonplace in the majority of keynote discussions surrounding different algorithms and subsets of AI. Alongside this, it is suggested that RL assumes the world is Markovian, which it is not. The Markovian model describes a sequence of possible events in which the probability of each event depends only on the state attained in the previous event.\n\nMachine Learning, in its many forms, is seen as a staple in AI, with the potential to scale, a key factor in the growth of intelligent machines. A number of industries utilizing large swathes of data already recognize the potential of ML, as it’s regularly used in financial services, healthcare, government processes, transportation, and more. Using ML not only increases the potential effectiveness of the product, but can aid in a competitive advantage over those proceeding without.","seo_title":"What Are the Types of Machine Learning?","slug":{"_type":"slug","current":"types-of-machine-learning"},"tags":[{"_key":"xTN3uVzu","label":"Machine Learning","value":"Machine Learning"}],"title":"What Are the Types of Machine Learning?"},{"_createdAt":"2020-06-19T20:12:52Z","author":{"_id":"a009d418-aa96-47ac-a73a-fd2cd52c79d9","avatar":{"_type":"image","asset":{"_ref":"image-e0d717f753ba4876a6b0dbf9f125cf6c3d27e545-500x500-webp","_type":"reference"}},"bio":"Wendy Gonzalez is an executive passionate about building high-performing, high-functioning teams that develop and scale innovative, impactful technology. With two decades of managerial and technology leadership experience for companies including EY, Capgemini, Cycle30 (acquired by Arrow Electronics) and General Communications Inc, Gonzalez is currently the CEO of Sama, the provider of accurate data for ambitious AI, used by leading technology companies such as Walmart, Google, Nvidia and Getty. Before taking on her role as CEO, Gonzalez spent 5 years at Sama as COO, and is an active Board Member of the Leila Janah Foundation.","name":"Wendy Gonzalez","slug":{"_type":"slug","current":"wendy-gonzalez"}},"estimatedReadingTime":6,"featured_image":{"_type":"image","asset":{"_ref":"image-86e8e4063cb04eb424755f0cce2de16c50442a1c-2135x1068-png","_type":"reference"}},"meta_description":"We’ve decided to take action both inside our organization and in partnership with local organizations to drive toward a more equitable society.","openGraphImage":null,"plaintextBody":"Like many people, I have been deeply saddened, frustrated, and outraged by the violence against Black people in the United States. This systemic challenge we face requires not only immediate action. It requires a change in the way we think, see, and operate.\n\nTrue change happens with purposeful action and in the decisions we make every day. It takes investment for the long term, even when there is pressure to perform in the short term. As I’ve been telling my teams as we organize to contribute the fight for justice for Black people, “We are in a marathon, not a sprint.”\n\nI also firmly believe that change starts locally. It starts with us as individuals and within our communities.\n\nLastly, change needs to be measurable - not just this quarter or next, but on an ongoing basis. We need to hold ourselves accountable to ensure we’re making positive progress toward a more just, equitable society. Since our founding in 2008, we have treated our impact metrics with the same level of fidelity and reporting as we have our financials. We will apply the same rigor of measure to the Black Lives Matter engagement strategy and actions I’m committing to today.\n\nFollowing our company values, we’ve decided to take action both inside our organization and in partnership with local organizations to drive toward a more equitable society where Black people in America realize the same level of humanity as others.\n\nI’m committing to four core initiatives to support racial inclusion.\n\nFinancial Support: Earlier this week, we launched a corporate matching program to raise donations for three local organizations. Sama seeded this program with funds and is matching each employee donation 2:1. The three local organizations include:\n\nCode2040, a San Francisco based nonprofit activating, connecting, and mobilizing the largest racial equity community in tech to dismantle the structural barriers that prevent Black and Latinx technologists from fully participating in the tech industry.\n\nAI4ALL, an Oakland based non-profit lead by a Sama alum, opens doors to artificial intelligence for underrepresented talent through education and mentorship.\n\nColor Of Change, an Oakland based organization, is the nation’s largest online justice organization. They design campaigns powerful enough to end practices that hold Black people back, and champion solutions that move us all forward.\n\nHoodstock, a Montreal based organization, generates spaces for dialogue and mobilizing initiatives to eliminate systemic inequalities and develop supportive, inclusive, secure and dynamic communities.\n\nDiversity and Inclusion: While we have rigorous programs in place to address gender diversity and inclusion, and enjoy a cross-cultural mix of people within our organization, we found that our North American diversity representation could be improved. To achieve this goal, we’re committing to expand our gender diversity and inclusion program to specifically include inclusion metrics for Black and other people of color.\n\nInternal Education: We are committed to hosting internal education and engagement sessions covering topics such as institutional racism, racial bias in AI, diversity hiring best practices, and the importance of voting. We also started an internal forum for our teams to openly exchange ideas for action and provide additional support to each other.\n\nPartnerships: We are engaging with partners within the AI community committed to better understanding and taking action against AI Bias. We’re also engaging with organizations that support upskilling, mentoring, and recruitment to drive economic mobility and inclusion for Black Americans in the tech industry.\n\nMy hope is that everyone in our broader technology community will drive the change needed to make the United States a more equitable and inclusive country. We must stand together and bring humanity to our industry, and to our country.\n\n(In case you missed it, below is our original statement. )\n\nThe murder of George Floyd has reignited an international movement for equality and justice. It sparked a powder keg of emotions that have left many feeling frustrated, helpless, angry, outraged, and sad. We here at Sama stand by one of our core company values -- Humanity. Humanity for the countless number of Black lives who have been victims of police brutality. Humanity for every person that tries to survive daily in a system that routinely oppresses them. Humanity for those who so often do not receive it, all because of the color of their skin.\n\nThe world of tech so often speaks of creating a new, better world -- but we cannot do that if we do not stand for the rights of everyone. Humanity and compassion are necessary. In order to avoid bias in ourselves and the technology we produce, we must consider how daily actions, words, and thoughts contribute to a racist and prejudiced world.\n\nTo quote our late founder Leila Janah, “Don’t underestimate the ripple effect of what you do. These kinds of small actions undertaken in coordination by large numbers of people have toppled empires.” We stand in solidarity with those driving positive change. Here is a list of resources we found helpful.","seo_title":"Underscoring Our Commitment to Racial Justice and Equality","slug":{"_type":"slug","current":"our-commitment-to-racial-justice-and-equality"},"tags":[{"_key":"ceQUdDQq","label":"Company News","value":"Company News"},{"_key":"Ac6EgzO1","label":"Ethical AI","value":"Ethical AI"},{"_key":"s1l8UVKZ","label":"Leila Janah","value":"Leila Janah"}],"title":"Underscoring Our Commitment to Racial Justice and Equality"},{"_createdAt":"2020-06-10T16:27:24Z","author":{"_id":"80db3356-b402-44cb-a717-2e0f9c9fa3e4","avatar":{"_type":"image","asset":{"_ref":"image-399295e959574e371fe2ba7bfd55ba4aee8a589d-320x320-jpg","_type":"reference"}},"bio":"Heather is passionate about bringing world-changing technologies to market and using supply chain purchasing power for good. She is a data-driven strategist experienced in developing and leading go-to-market, communications, and sustainability initiatives at start-ups and multi-national organizations. Heather is most happy when she’s growing companies that make a positive impact, enjoying the outdoors, and spending time with her family.","name":"Heather Gadonniex","slug":{"_type":"slug","current":"heather-gadonniex"}},"estimatedReadingTime":5,"featured_image":{"_type":"image","asset":{"_ref":"image-7fa93d0a7942fc64939bb9f56f4aa294b7dbd8ad-1280x400-png","_type":"reference"}},"meta_description":"Maximizing Training Data in AI Securely","openGraphImage":null,"plaintextBody":"TL;DR:\n\nThe need for data to meet privacy \u0026 security requirements by law can often reduce the amount of training data available\n\nThe growth of popularity in AI has been mirrored by a growing number of concerns surrounding privacy, security and ethical use of data\n\nPII includes any information which could point toward identification, including, but not limited to social media information, IP addresses and more\n\nSama gives the ability for AI companies to scale training data at a faster pace without compromising quality, privacy or security\n\nWith the steady rise in both popularity and progress in Artificial Intelligence (AI) over recent years, many have been quick to address potential privacy and security concerns, with buzzwords like ‘ethics’ and ‘responsibility’ never too far from discussion. While the initial public perception of AI was “will automation steal my job”, steady progress has seen facets of AI and Machine Learning technology present in our living rooms, cars, phones and more, mostly without people knowing. That said, an important question has emerged: What level of trust can—and should—we place in these AI systems?\n\nIn an age of increasingly complex governmental data privacy requirements, it can be hard to understand not only the level of personal data that’s available, but also how this is protected, both in law (GDPR, Information Privacy law etc.), but also through the development of solution provider products.\n\nPersonally identifiable information (PII) is to be considered any information which could identify a specific individual. Of course, the wide definition of PII can create challenges, especially when searching for AI training data as it can cover anything from IP addresses, imagery, behavioral data, social media information, and more.\n\nWhy do we need such swathes of data? Well, a simple input/output equation suggests that more data equates to the ability for increased training and training environments. This, in turn, leads to models that are often increasingly accurate due to both the level of training and the various training scenarios it has been placed inside [1]. At this stage, it is also important to recognize the differentiation between both structured \u0026 unstructured data as well as supervised \u0026 unsupervised learning. You can read more on this here.\n\nTo surmise, the main challenges faced by those in need of large data training sets include, but are not limited to, the following:\n\nInability to utilize all owned data due to GDPR and CCPA privacy restrictions\n\nLack of anonymized video training data available for use\n\nPreviously used techniques, including pixelation of imagery reducing model performance for video analysis\n\nRequired manual human intervention\n\nRecent developments on our platform aim to address the above through the use of Vector Annotation, Semantic Segmentation, Lidar/3D Annotation, and Dynamic Labelling. These developments when coupled with ISO-certified data centers, vulnerability testing of systems, data storage encryption \u0026 GDPR compliance, not only ensure the highest level of internal security, but also the most dynamic and innovative data utilization service available.\n\nMany applications have previously struggled to keep personally identifying information safe across a variety of data sources, especially when street-level images, cars, retail places, and similar are discussed, however, Sama uses deep learning pre-annotation technology to anonymize data without the need for a human intervention! The process for this includes:\n\nData is run through our anonymizer technology service before any labeling occurs\n\nThis service automatically detects faces and license plates to obscure and does so until unrecognizable\n\nThis AI-generated content creates training data that looks like real-time data when people and vehicles are the primary objects of interest for the algorithm\n\nThe above, through the lack of human intervention further accentuates the privacy of PII data\n\nThis service then allows for increased test data at a faster pace, without compromising security, allowing you to scale your AI whilst also not compromising the trust of stakeholders\n\nWant to learn more? Check out our Anonymization webinar here.","seo_title":"Data Protection and Privacy for Training Data","slug":{"_type":"slug","current":"data-protection-and-privacy-for-training-data"},"tags":[{"_key":"j3OupYSm","label":"Security \u0026 Trust","value":"Security \u0026 Trust"},{"_key":"pW5yVuE3","label":"Training Data","value":"Training Data"},{"_key":"KuCG26mD","label":"Anonymization","value":"Anonymization"}],"title":"Data Protection and Privacy for Training Data"},{"_createdAt":"2020-05-12T16:08:52Z","author":{"_id":"a009d418-aa96-47ac-a73a-fd2cd52c79d9","avatar":{"_type":"image","asset":{"_ref":"image-e0d717f753ba4876a6b0dbf9f125cf6c3d27e545-500x500-webp","_type":"reference"}},"bio":"Wendy Gonzalez is an executive passionate about building high-performing, high-functioning teams that develop and scale innovative, impactful technology. With two decades of managerial and technology leadership experience for companies including EY, Capgemini, Cycle30 (acquired by Arrow Electronics) and General Communications Inc, Gonzalez is currently the CEO of Sama, the provider of accurate data for ambitious AI, used by leading technology companies such as Walmart, Google, Nvidia and Getty. Before taking on her role as CEO, Gonzalez spent 5 years at Sama as COO, and is an active Board Member of the Leila Janah Foundation.","name":"Wendy Gonzalez","slug":{"_type":"slug","current":"wendy-gonzalez"}},"estimatedReadingTime":3,"featured_image":{"_type":"image","asset":{"_ref":"image-08a4bf963b3d04e0e0017d67bf2b972531285856-2000x1000-jpg","_type":"reference"}},"meta_description":"We’re excited to introduce Chloe, an automated public chatbot service to support in the fight against the coronavirus.","openGraphImage":null,"plaintextBody":"For over a decade, we have developed mission-critical data for AI applications. As we continue to look for ways to improve the lives of many each day, we’re excited to introduce Chloe, an automated public chatbot service to support in the fight against the coronavirus. We’ve partnered with several leaders in artificial intelligence development including Dialogue Technologies, MILA, Nu Echo, along with support from Google, John Hopkins University and Dataperformers, to enhance this service.\n\nSince the start of the coronavirus pandemic, information has constantly been evolving. Most of us don’t have the time or mental capacity to check the news or medical reports on an hourly or daily basis. It’s not because we don’t want to be informed, but it's become nearly impossible to keep up, especially on top of everything we’re dealing with at home and at work.\n\nIt’s affecting the healthcare workers who are at the forefront of this fight and it’s also affecting individuals mental well being and causing anxiety to surge. That’s why we’ve decided to offer our services to help support an online system that will do it for you. Through Chloe, people can get the most up-to-date information instantly.\n\n“COVID-19 is impacting everyone at an alarming rate, that’s why we’re excited to work with such an astonishing group of professionals to help power such a vital service. Our team is honored to have been able to help make this happen.” - Wendy Gonzalez, CEO, Sama\n\nThe Chloe chatbot is available today and supports Canadians by becoming the first phase of getting care - both for your physical and mental wellbeing. Providing current and verified information about COVID-19 with clear answers to specific questions on the subject is the first step. At the end of the assessment, it will provide you resources that correlate with your medical inquiry. But we know that having a clear sense of mind doesn’t stop there. If the assessment indicates that you have a likelihood of having been exposed to the virus, Chloe will answer questions about the testing phase and monitor people in self-isolation to keep track of their condition.\n\nChloe is powered through natural language processing technology (NLP). Our team at Sama makes Chloe smarter by helping her understand the human language. We have real people behind the chatbot, annotating every data asset to help ensure that citizens are getting the most accurate information available in real-time.\n\nWe believe that it’s our obligation to use our industry-leading annotation solutions to help as many people as possible navigate these trying times. Our goal is to provide a safe service with truthful, current and accurate information for everyone. We hope that you’ll sign up and take advantage of this free service. Let’s work together to help alleviate our first responders and support our healthcare system.\n\nWe’re excited to help bring Chloe to market and are eager to contribute what we can in this fight against COVID-19.\n\nTo access Chloe, please visit covid19.dialogue.co.","seo_title":"Introducing Chloe: A ChatBot to Help You Get Accurate Health Information During COVID-19","slug":{"_type":"slug","current":"chatbot-during-covid-19"},"tags":[{"_key":"k5wnuMl8","label":"Company News","value":"Company News"},{"_key":"KjXPtdNx","label":"AI","value":"AI"},{"_key":"A3zPSdd2","label":"Training Data","value":"Training Data"},{"_key":"z3km6Gbw","label":"Use Cases","value":"Use Cases"},{"_key":"oNwQIM12","label":"Data Quality","value":"Data Quality"}],"title":"Introducing Chloe: A ChatBot to Help You Get Accurate Health Information During COVID-19"},{"_createdAt":"2020-04-28T17:30:19Z","author":{"_id":"80db3356-b402-44cb-a717-2e0f9c9fa3e4","avatar":{"_type":"image","asset":{"_ref":"image-399295e959574e371fe2ba7bfd55ba4aee8a589d-320x320-jpg","_type":"reference"}},"bio":"Heather is passionate about bringing world-changing technologies to market and using supply chain purchasing power for good. She is a data-driven strategist experienced in developing and leading go-to-market, communications, and sustainability initiatives at start-ups and multi-national organizations. Heather is most happy when she’s growing companies that make a positive impact, enjoying the outdoors, and spending time with her family.","name":"Heather Gadonniex","slug":{"_type":"slug","current":"heather-gadonniex"}},"estimatedReadingTime":3,"featured_image":{"_type":"image","asset":{"_ref":"image-3bb8e0adeda260278b213dc40d2819a792dfe6f8-3534x1767-png","_type":"reference"}},"meta_description":"Fast Company has recognized Samasource as a finalist in the AI and Data category as part of their 2020 World Changing Ideas Awards.","openGraphImage":null,"plaintextBody":"Since our founding in 2008, we’ve been driven by the idea that through technology, we can make the world a better place. Ever since, our team has continuously broken barriers as we became the trusted, high-quality training data and validation provider for 25% of the Fortune 50 companies.\n\nThanks to our team’s dedication to providing high quality training data and trustworthy technology, we’re honored that Fast Company has recognized us as a finalist in the AI and Data category as part of their 2020 World Changing Ideas Awards.\n\n“This recognition is a testament to our commitment to using technology to fuel positive impacts on our world,” says Wendy Gonzalez, President and Interim CEO of Sama. “We’re humbled to have received this recognition from Fast Company. This award further validates our industry-leading software technology and our team’s efforts to ensure only the highest quality standards.”\n\nFast Company recognized us as a finalist for our work with Vulcan, a project and investment company using data to make a positive impact on endangered species, climate change, ocean health, and more. Image recognition technology plays an important role in wildlife conservation and through our work together, we created an AI solution that was able to record and monitor African wildlife and threats of poaching, human-wildlife conflict and loss of habitat. Through that technology, we were able to combat large-scale poaching that contributes to extinction to support a more sustainable ecosystem. To date, we’ve labeled nearly one million images for Vulcan, achieving above industry-standard quality in support of their efforts.\n\n“There seems no better time to recognize organizations that are using their ingenuity, resources, and, in some cases, their scale to tackle society’s biggest problems,” says Stephanie Mehta, editor-in-chief of Fast Company. “Our journalists, under the leadership of senior editor Morgan Clendaniel, have uncovered some of the smartest and most inspiring projects of the year.”\n\nThe World Changing Ideas Awards is one of Fast Company’s major annual awards programs and is focused on social good, seeking to elevate finished products and brave concepts that make the world better. A panel of judges from across sectors choose winners, finalists, and honorable mentions from a pool of 3,000 this year and based their decisions on feasibility and the potential for impact. With a goal of awarding ingenuity and fostering innovation, Fast Company draws attention to ideas with great potential and helps them expand their reach to inspire more people to start working on solving the problems that affect us all.\n\nWe’re thankful to everyone who has made an impact and provided guidance to Sama - our teams, investors, customers and community - this award acknowledges us all.\n\nHead this way to check out the complete list of companies featured in Fast Company’s World Changing Ideas. ","seo_title":"Fast Company Names SamaÃƒâ€šÃ‚Â a 2020 World Changing Ideas Finalist in AI and Data","slug":{"_type":"slug","current":"world-changing-ideas-finalist"},"tags":[{"_key":"ZsUzaThh","label":"Company News","value":"Company News"},{"_key":"V1MkPdiw","label":"Ethical AI","value":"Ethical AI"},{"_key":"zh1YPgpW","label":"AI","value":"AI"},{"_key":"3r8B4ITq","label":"Awards","value":"Awards"}],"title":"Fast Company Names Sama a 2020 World Changing Ideas Finalist in AI and Data"},{"_createdAt":"2020-04-23T21:37:48Z","author":{"_id":"26fb3cc4-608d-40e1-bb4d-955bceda232a","avatar":{"_type":"image","asset":{"_ref":"image-f1fd7fbcc4633299cdbedddba22cb44e24f17317-518x518-svg","_type":"reference"}},"bio":"From self-driving cars to smart hardware, Sama fuels AI. Founded over a decade ago, we’re experts in image, video and sensor data annotation and validation for machine learning algorithms in industries including automotive, navigation, AR/VR, biotech, agriculture, manufacturing, and e-commerce. Our staff are driven by a mission to expand opportunity for low-income people through the digital economy, and our social business model has helped over 50,000 people lift themselves out of poverty.","name":"Sama Team","slug":{"_type":"slug","current":"sama-team"}},"estimatedReadingTime":5,"featured_image":{"_type":"image","asset":{"_ref":"image-04a6a4a0b08c6631b9ea6592f4d5e29e4ca09ba5-4496x3000-jpg","_type":"reference"}},"meta_description":"The traffic light problem for autonomous vehicles is critical for all vehicle safety, and unlike human-drivers, AVs rely solely on computer vision systems to navigate the world around us.","openGraphImage":null,"plaintextBody":"Solving the traffic light problem for autonomous vehicles is critical for all vehicle safety, not just autonomous vehicles. And unlike human-driven cars, AVs rely solely on their computer vision system and the data used to train them to navigate the world around us.\n\nCurrently, the best self-driving assistance systems incorrectly perceive something in their environment once every tens of thousands of hours. If that object is a traffic light, and the car gets it wrong, passengers, pedestrians, cyclists, etc., are all at risk. Here’s a look at the traffic light problem for autonomous cars from three perspectives. \n\n\n\n1. Traffic lights tell road users when to stop and go, but they only work when everyone follows the rules.\n\nTraffic lights are an engineered system, timed around traffic patterns. There are rules in place that tell cars when to stop and go, but the inherent human behavior of drivers, pedestrians and cyclists sometimes means these rules are loosely interpreted.\n\nThere are no physical barriers forcing road users to abide by traffic signals. They work because drivers follow the rules. AVs must also get the rules right, and that means countless hours of real-world exposure to the unspoken rules (or lack thereof) of the road.\n\nThe quality and accuracy of the data received from sensor packages must be precise, leaving no room for interpretation or inconsistencies. Also, license plates, faces or other personal identifying information (PII) may need to be anonymized to protect the privacy of people who might appear in the raw footage. \n\nEnsuring AVs learn the right rules requires unbiased, appropriately labeled and high-quality training data based on a range of driving scenarios.\n\n\n\n2. Traffic lights challenge both the vision system and the team developing the algorithms.\n\nBecause traffic lights are not a distance detection problem, AVs cannot use lidar or radar to navigate traffic signals. They must rely solely on their computer vision system to understand when to stop and go.\n\nThis can be difficult for both the vision system as well as the team developing the algorithm because the visibility of traffic lights may vary based on weather conditions like bright sunlight, rain, snow or fog. Similarly, not all intersections have traffic lights, so if the AV doesn’t detect one, that could be correct.\n\nThe treatment of out of service traffic signals can also vary. One city might use a plastic bag to indicate an out-of-service light while another city, or even county, might use masking tape or some other method to cover broken traffic signals.\n\nContext clues like head nods or hand signals help human drivers manage low visibility or the lack of a traffic light, but since AVs cannot register this supplemental visual information, machine learning and computer vision engineers must train the AV on such scenarios as they arise.\n\n3. Stock datasets aren’t enough to help AVs navigate traffic lights safely.\n\nThe volume and diversity of data required for autonomous vehicles is vast, and stock datasets cannot cover all use cases.\n\nDuncan Curtis, Sama VP of Product shares approximately 1,000 to 10,000 images are needed to deploy a solution onto a vehicle, and in order to launch a product publicly, approximately 10,000 to 100,000 images are needed per ADAS feature. That’s a lot of data, and the amount of data needed to account for edge cases like traffic lights in unique road conditions is unknown.\n\nSince AVs often need data from a specific camera or sensor package (because that’s what the car will use in production), generic stock data isn’t enough to help AVs navigate traffic lights safely. In addition to this, models need to be refreshed with ongoing training data as road rules, and the world around us changes.\n\nOur team of data labeling experts annotate over 4M tasks per month, and for AV use cases, we’ve achieved 99.5 percent quality SLAs with more than 100 tags per image. We’ve also successfully achieved full productivity within our labeling teams in as little as two to five days.\n\nIf you need ground truth data for your ML model, connect with our team for a virtual consultation and demo of our industry-leading data annotation platform.","seo_title":"The Traffic Light Problem for Autonomous Vehicles","slug":{"_type":"slug","current":"the-traffic-light-problem-for-autonomous-vehicles"},"tags":[{"_key":"ivLPY6Ly","label":"AI","value":"AI"},{"_key":"Liw4Inz5","label":"Training Data","value":"Training Data"},{"_key":"3LVFK315","label":"Data Annotation","value":"Data Annotation"},{"_key":"YhUSiZyI","label":"Data Quality","value":"Data Quality"},{"_key":"72HsQzsx","label":"Autonomous Transportation","value":"Autonomous Transportation"}],"title":"The Traffic Light Problem for Autonomous Vehicles"},{"_createdAt":"2020-04-22T23:33:05Z","author":{"_id":"80db3356-b402-44cb-a717-2e0f9c9fa3e4","avatar":{"_type":"image","asset":{"_ref":"image-399295e959574e371fe2ba7bfd55ba4aee8a589d-320x320-jpg","_type":"reference"}},"bio":"Heather is passionate about bringing world-changing technologies to market and using supply chain purchasing power for good. She is a data-driven strategist experienced in developing and leading go-to-market, communications, and sustainability initiatives at start-ups and multi-national organizations. Heather is most happy when she’s growing companies that make a positive impact, enjoying the outdoors, and spending time with her family.","name":"Heather Gadonniex","slug":{"_type":"slug","current":"heather-gadonniex"}},"estimatedReadingTime":3,"featured_image":{"_type":"image","asset":{"_ref":"image-01151de9b1aeaf0afe4d22613bf6b5537354c561-1386x808-png","_type":"reference"}},"meta_description":"Creating lasting social impact, understanding our carbon footprint and being environmentally sustainable are just few of our goals for sustainability and impact.","openGraphImage":null,"plaintextBody":"I woke up this morning thinking about Leila’s fierce commitment to sustainability.\n\nThe two of us spent quite a lot of time visioning how our team could expand Sama’s Give Work platform to incorporate environmental sustainability, and how the organization could play an even bigger role in creating a more equitable, just world.\n\n\n\nPHOTO: Leila Janah, Founder,Sama\n\n\n\nFrom nixing the use of water bottles at all of our offices to launching the Give Work Challenge, a full-scale business competition that rewards entrepreneurs for social and environmental innovation, Leila had a lot of ideas to weave triple bottom line principles (being financially profitable, creating lasting social impact, and being environmentally sustainable) into our business model. In addition to spearheading our impact model, she sparked a deep love of nature in the visitors to our East African offices by increasing their exposure to African nature and wildlife. One of her favorite topics to muse about over the past few years was how to calculate the positive climate impacts of hiring and empowering women in East Africa.\n\nOn this 50th anniversary of Earth Day, I want to take the time to remember Leila’s legacy by giving you a glimpse into a few of the sustainability activities we are working on here at Sama. Mid last year, we officially launched our sustainability program. Our first goals were to understand our carbon footprint (GHGe per GHG Protocol Corporate Standard, Scope 1 and 2), develop an environmentally preferable purchasing program for ongoing consumables, and create biophilic office environments that brought nature inside.\n\nPHOTO: Sama East Africa delivery center\n\n\n\nUltimately, Leila wanted to pursue B Corporation certification. She always wanted to lead by example, to show the world how business could be both profitable and a force for good.\n\nWhat better way to do this than provide the ultimate success story and demonstrate that a technology company can be committed to both providing the leading training data platform and to using business as a force for good?\n\n“One of the things I’m proudest of at Sama is our collaboration with conservation groups including the Nature Conservancy to use AI to track endangered species, combat poaching, and protect wilderness. Not only that, but we do it by employing women (more than 50% of our workforce!) from low-income backgrounds, which itself is a way to fight climate change.” - Leila Janah\n\n\n\nI’m thrilled that Sama is actively engaged with B Lab and is in the final stages of audit verification for our B Corporation certification. I couldn’t be prouder of our Nairobi based Sustainability Analyst, Robert Wahome, for ensuring we have all of the data in place needed to drive this initiative forward, including our first ever Carbon Footprint. In my heart, I know Leila is proud too.","seo_title":"Celebrating 50 Years of Earth Day: Honoring Leila Janah's Legacy","slug":{"_type":"slug","current":"celebrating-50-years-of-earth-day-honoring-leila-janahs-legacy"},"tags":[{"_key":"OFjdWc6T","label":"Ethical AI","value":"Ethical AI"},{"_key":"OtSjlWNo","label":"Impact","value":"Impact"},{"_key":"fWAUAQ8u","label":"Leila Janah","value":"Leila Janah"}],"title":"Celebrating 50 Years of Earth Day: Honoring Leila Janah's Legacy"},{"_createdAt":"2020-04-02T20:30:00Z","author":{"_id":"6dd25391-26af-42ff-a2a2-f0778ce1b002","avatar":{"_type":"image","asset":{"_ref":"image-4e1c47984bb16cb0370855b6272663d6744216d3-450x450-jpg","_type":"reference"}},"bio":"Currently a Senior Product Manager at Sama, Audrey guides cross-functional teams to create thoughtful product solutions. She has guided teams of designers and engineers at HUGE Inc. and NBCUniversal, and monitored user analytics at the Wall Street Journal. With a BA in history from Harvard, an MA in anthropology from Columbia and an MBA from UNC Chapel Hill KFBS, Audrey is passionate a using technology and data analytics facilitate social impact and environmental solutions through technology.","name":"Audrey Boguchwal","slug":{"_type":"slug","current":"audrey-boguchwal"}},"estimatedReadingTime":3,"featured_image":{"_type":"image","asset":{"_ref":"image-7eb4854393afff7be19269b7e8414936e411403f-5506x3671-jpg","_type":"reference"}},"meta_description":"SamaHub's video and 3D object tracking with frame level labeling assists companies in quickly building models that better reflect real-world behavior.","openGraphImage":null,"plaintextBody":"We live in an ever-changing world, where AI-enabled technology has become a new normal for society. To assist top organizations in their efforts to build smarter computer vision algorithms, we’ve rolled out a new release for video and 3D object-tracking in our leading data annotation platform.\n\nThere are a number of applications that use computer vision to track how the world, and the objects in it, change overtime. For a self-driving car to navigate safely, it needs to track other moving objects on the road and make predictions about their future movement, so it can plan its driving path.\n\nAR and VR applications like video games need to track the motion of individual people to create a seamless digital experience. These vision applications have something in common: they all seek to understand the change in position, behavior and characteristics of unique objects over time.\n\nObject tracking annotation offers object tracking capabilities for complex scenarios, including path planning, traffic light status, sentiment analysis, etc.\n\nThis frame-level labeling technology allows unique objects to be dynamically tracked across a video or a sequence of 3D point cloud data from a Lidar sensor. Change in position and pose are captured with annotation shapes like cuboids, polygons and bounding boxes.\n\nSama supports custom label taxonomies for both the main object class (person, car, etc.) and dynamic labeling for other object characteristics that change over time, such as visibility percentage of an object or a specific set of characteristics like emotions. Sama’s built-in automated interpolation between video frames helps ensure efficient, high quality labeled training data for a variety of object tracking use cases.\n\nFor over 10 years, Sama has delivered turnkey, high-quality training data and validation to train the world's leading AI technologies. Video and 3D object tracking are no exception, and this update for video object tracking annotation in 2D RGB video and 3D Lidar data will continue to assist organizations in quickly building models that better reflect real-world behavior.\n\nSama has deep expertise working with training data for object tracking use cases across a variety of industries including autonomous vehicles, AR/VR, retail and e-commerce, communications, media and entertainment, to name just a few.\n\nDownload our solution brief to learn more about our secure training data annotation platform, or contact our team here.","seo_title":"Object Tracking with Frame Level-Labeling","slug":{"_type":"slug","current":"object-tracking-in-samahub-with-frame-level-labeling"},"tags":[{"_key":"OjIX5QKU","label":"Product","value":"Product"},{"_key":"2ZGxEWq0","label":"Video Annotation","value":"Video Annotation"},{"_key":"iBmdS1VE","label":"Training Data","value":"Training Data"},{"_key":"uZYhm6jx","label":"Data Annotation","value":"Data Annotation"}],"title":"Object Tracking with Frame Level-Labeling"},{"_createdAt":"2020-03-26T16:49:42Z","author":{"_id":"6dd25391-26af-42ff-a2a2-f0778ce1b002","avatar":{"_type":"image","asset":{"_ref":"image-4e1c47984bb16cb0370855b6272663d6744216d3-450x450-jpg","_type":"reference"}},"bio":"Currently a Senior Product Manager at Sama, Audrey guides cross-functional teams to create thoughtful product solutions. She has guided teams of designers and engineers at HUGE Inc. and NBCUniversal, and monitored user analytics at the Wall Street Journal. With a BA in history from Harvard, an MA in anthropology from Columbia and an MBA from UNC Chapel Hill KFBS, Audrey is passionate a using technology and data analytics facilitate social impact and environmental solutions through technology.","name":"Audrey Boguchwal","slug":{"_type":"slug","current":"audrey-boguchwal"}},"estimatedReadingTime":3,"featured_image":{"_type":"image","asset":{"_ref":"image-70afee37d4e99e8fac42e758c436c2f61d991a7d-1904x797-jpg","_type":"reference"}},"meta_description":"Samasource is excited to launch the PII Data Anonymizer for video training data. This technology enables obscuring of sensitive information in training data.","openGraphImage":null,"plaintextBody":"Sama is excited to launch the PII Data Anonymizer as part of our platform for video training data. This technology enables obscuring of sensitive, personally identifying information (PII) in training data.\n\nIn light of new laws like GDPR and CCPA, it’s important for companies building AI and ML technologies to carefully manage data with PII information. Obscuring PII helps Sama and our customers work to protect privacy.\n\nSama’s PII Data Anonymizer helps make more data available to train AI by keeping personally identifying information safe across a variety of data sources: People in camera images from retail spaces and public places, street-level images of people and license plates captured by vehicles, smart city applications on public transit and more.\n\nApplications for anonymization range from autonomous transportation, detailed customer demographics, customer data like clothing and emotion, people counters, and security.\n\nThis deep learning pre-annotation technology allows Sama to obscure faces and vehicle license plates that appear in data without the need for any human intervention. That means that private information remains private and is never seen by another person.\n\n\n\nWhen Sama receives customer data, it can be run through our anonymizer technology service before any labeling occurs. The service would automatically detect faces and license plates and obscure them, as well as blur faces and license plates so they are not recognizable.\n\nAlternatively, it can replace faces and license plates with realistic computer-generated avatars. This AI-generated content creates training data that looks like real-time data when people and vehicles are the primary objects of interest for the algorithm.\n\nUnlike manual blurring, Sama’s PII Data Anonymizer is run without a human examining the data, which contributes to the privacy of PII data. It is built on deep learning and is run within our technology platform, ensuring that customer data never leaves Sama’s secure cloud environment.\n\nFrom pilots to multi-year projects, Sama securely trains and validates computer vision and NLP models. We work on a range of use cases ranging from e-commerce to autonomous transportation, manufacturing, navigation, retail, AR/VR, and biotech. If your goal is to quickly build smarter AI, contact our team to discuss your training data needs.","seo_title":"Keep it Secret, Keep it Safe: Announcing the PII Data Anonymizer","slug":{"_type":"slug","current":"keep-it-secret-keep-it-safe-announcing-the-pii-data-anonymizer"},"tags":[{"_key":"4C8kDRnD","label":"Product","value":"Product"},{"_key":"sCm7cDGc","label":"Security \u0026 Trust","value":"Security \u0026 Trust"},{"_key":"w6zwqXk4","label":"Anonymization","value":"Anonymization"}],"title":"Keep it Secret, Keep it Safe: Announcing the PII Data Anonymizer"},{"_createdAt":"2020-03-08T19:00:00Z","author":{"_id":"80db3356-b402-44cb-a717-2e0f9c9fa3e4","avatar":{"_type":"image","asset":{"_ref":"image-399295e959574e371fe2ba7bfd55ba4aee8a589d-320x320-jpg","_type":"reference"}},"bio":"Heather is passionate about bringing world-changing technologies to market and using supply chain purchasing power for good. She is a data-driven strategist experienced in developing and leading go-to-market, communications, and sustainability initiatives at start-ups and multi-national organizations. Heather is most happy when she’s growing companies that make a positive impact, enjoying the outdoors, and spending time with her family.","name":"Heather Gadonniex","slug":{"_type":"slug","current":"heather-gadonniex"}},"estimatedReadingTime":3,"featured_image":{"_type":"image","asset":{"_ref":"image-8556bc7f46d3029f92f018877f7fe103dd60b86f-2048x1365-jpg","_type":"reference"}},"meta_description":"Sama's commitment to an ethical AI supply chain enables us to close the gender pay gap, by paying a living wage to all of our employees.","openGraphImage":null,"plaintextBody":"For centuries, women have made history as innovators, inventors and trailblazers. I’m grateful that everyday I get to witness history in the making as part of a female-led tech company that provides industry leading, high quality training data technology and solutions that are underpinned by an ethical AI supply chain.\n\nIt’s this commitment to an ethical supply chain that enables us to close the gender pay gap by paying a living wage to all of our employees.\n\nThe AI supply chain is the combination of the human workforce and advanced technology solutions that help bring machine learning based systems to market. To operationalize AI, everything from data labeling to model development needs to be in sync, and as it is with any supply chain, ethics and sustainability come into play. \n\nBehind every Al, there’s a person deciding what data is needed and the best model to use. There’s also someone guiding AI strategy, developing the tech to support AI pipelines, and so on.\n\nData annotation is a key part of making machine learning possible, and conversations around ethical AI supply chains are vital to have now if we want to ensure the labeling industry is a positive force.\n\nAccording to our baseline impact survey data, prior to joining Sama, female workers in our global delivery centers earned 70 cents for every dollar earned by men, despite having comparable educational backgrounds.\n\nWe found that gender discrimination—especially in the technology sector—and the challenge of being a primary family caretaker make it difficult for many women to get into and stay in the formal employment sector in Kenya and Uganda.\n\nWomen are highly driven and eager to align their education with real-world opportunities, yet a recent report from the World Economic Forum found that if progress continues at its current rate, it would take roughly 95 years for sub-Saharan Africa to achieve gender parity.\n\nThis number is astonishing, but there is a path forward, and it involves equipping younger generations with the skills needed to succeed in the growing digital economy.\n\nAt Sama, our workers receive digital literacy training, as well as specialized training on artificial intelligence and our advanced training data platform. Also, in addition to connecting women and youth to dignified digital work, workers gain access to life and professional skills development opportunities, to further advance their careers.\n\nWhen women succeed, everyone benefits, and closing the gender gap at work is key to achieving at least five of the sustainable development goals set by the UN. In the last decade, Sama has positively impacted over 50,000 lives, and we’re proud to have achieved gender parity in our global work centers, by ensuring our workers earn equal pay for equal work. \n\nIf you’re interested in learning more about our impact, read our story here, or work with us on training data and model validation, to start making a positive global impact one bounding box at a time.","seo_title":"How Sama is Closing the Gender Pay Gap for Women in the AI Supply Chain","slug":{"_type":"slug","current":"how-samasource-is-closing-the-gender-pay-gap-for-women-in-the-ai-supply-chain"},"tags":[{"_key":"daz4fDnC","label":"Ethical AI","value":"Ethical AI"},{"_key":"wLYQCPhS","label":"Women in AI","value":"Women in AI"},{"_key":"EiPsNBnG","label":"Impact","value":"Impact"}],"title":"How Sama is Closing the Gender Pay Gap for Women in the AI Supply Chain"},{"_createdAt":"2020-02-28T21:00:00Z","author":{"_id":"b4d4d096-8187-41fa-a386-87ce949c9915","avatar":{"_type":"image","asset":{"_ref":"image-45005425be54d2d450b1aa53f7d9435477531eb3-1664x1758-jpg","_type":"reference"}},"bio":"Liliosa is the Impact and Marketing Manager at Sama.","name":"Liliosa Mbirimi Muturi","slug":{"_type":"slug","current":"liliosa-mbirimi-muturi"}},"estimatedReadingTime":6,"featured_image":{"_type":"image","asset":{"_ref":"image-7e68dc7b894694d301fe5feff694b3df88f8beda-2048x1365-jpg","_type":"reference"}},"meta_description":"As AI adoption expands, untapped communities are finding work at the cutting edge of AI. Here are 4 ways AI makes a positive impact on communities in East Africa.","openGraphImage":null,"plaintextBody":"In a small town in Northern Uganda, Ocem turns on his lights at 6am to get ready for work. It’s his second year working at Sama, and before becoming an AI trainer, he was a student at the local university, moonlighting as a farmer.\n\nWorking in AI was not a path Ocem imagined until he was recruited to work at Sama. In fact, like many students, his greatest uncertainty was whether he could put his degree to good use at all, post graduation.\n\nOcem’s story is like many other young professionals in East Africa, but as AI adoption explodes, these talented, untapped communities are finding work they never thought possible, at the cutting edge of AI.\n\nHere are four ways AI makes a positive impact on communities in East Africa.\n\nGainful Employment\n\nA report by the World Bank shared that 60 percent of the unemployed in Africa are youth, who after graduating from University will take on average, one year to find employment in their field of study. \n\nI believe this is due to the lack of formal career opportunities within the region, making it difficult for the employment industry to support the volume of college graduates entering the workforce.\n\nAs a result, talented graduates find work in menial jobs that often don’t provide a sustainable income, but Sama and other tech companies are working to fix that.\n\nThe explosion of AI has birthed the demand for high-quality, ground truth data for artificial intelligence, and with training, recent graduates like Ocem can find meaningful work in the growing digital economy.\n\nDriven by a mission to expand opportunity for low-income people, Sama's social impact business model has helped over 50,000 people move themselves out of poverty through digital work.\n\nWorkers in our East Africa centers are paid a living wage and competitive benefits. They also receive ongoing digital skills training and have access to professional development opportunities that further help increase the purchasing power of their communities, potentially ending a long cycle of poverty.\n\nSkills and Knowledge\n\nAs more tech companies expand operations to Africa, and more people start gaining on-the-job experience working in machine learning and AI, the positive impact AI makes on communities in East Africa will only expand.\n\nI’ve personally witnessed how energized the youth in East Africa are about the intricacies of enriching data for AI. For many, this budding tech boom has motivated them to enroll in courses centered around data sciences and AI.\n\nAlso, the Deep Learning Indaba, an annual meeting for AI enthusiasts in Africa has brought together a growing number of youths for week long discussions, debates and research on machine learning and AI.\n\nWith a reliable IT infrastructure, paired with increased trust in the skill level of youth workers in the region, East Africa has the potential to become a major hub for AI training data.\n\nAt Sama, our AI training classes fill up weekly, and this level of investment in skills development for young professionals is what’s needed to continue developing the talent pool. \n\nDiversity and Inclusion\n\nUnesco reported that 30% of the tech-workforce in sub-Saharan Africa are women, and despite the challenges of gender discrimination in the technology sector, the percentage of female workers is steadily growing. \n\nWomen in Africa have often been set apart from the tech industry because culturally, they were considered primary caretakers, expected to focus on their family, not their career. Now, the vast opportunities in AI are helping to change that.\n\nWe actively recruit women and youth to work in our East Africa centers, providing in-depth technical training, so underserved communities can pursue a career in AI. From scholarship programs to nursing rooms, we’ve also made it a priority to establish an office culture conducive to the success of women workers.\n\nOur baseline survey data found that women earned 70 cents for every dollar earned by men, before joining Sama. By ensuring we achieve gender parity in pay, while simultaneously paying a living wage, our workers can support themselves and their families sustainably.\n\nData Privacy and Protection\n\nAs more populations in Africa adopt mobile technology and digital apps, their exposure to AI-enabled technologies that collect data to improve experiences will only increase. These interactions can be as simple as product recommendations from an online store, or interactions with an automated messenger bot.\n\nGiven the rapid advancement of AI and the Internet of Things (IoT) , the Kenyan government recently passed a data protection law which complies with the European Union’s General Data Protection Regulation (GDPR). \n\nSimilar to GDPR, this legislation outlines restrictions on data handling and sharing, in an effort to regulate the processing of personal data and information in the country.\n\nThis commitment to increased digital security will no doubt have a positive impact on the citizens of Kenya, and in many ways, it sets the standard for data privacy and protection across the continent.\n\nMcKinsey Global Institute predicts that AI has the potential to deliver an additional economic output of $13 trillion by 2030. This economic growth, alongside the digital transformation of sub-Saharan Africa will have enormous positive impact on communities in East Africa.\n\nIf you’re interested in learning more about how Sama connects people to dignified digital work, you can read about our social business model here.","seo_title":"4 Ways AI Makes a Positive Impact on Communities in East Africa","slug":{"_type":"slug","current":"4-ways-ai-makes-a-positive-impact-on-communities-in-east-africa"},"tags":[{"_key":"AEPEpJXp","label":"Ethical AI","value":"Ethical AI"},{"_key":"tc6Hsfbm","label":"AI","value":"AI"},{"_key":"VmY4sBKG","label":"Impact","value":"Impact"},{"_key":"iLYfly0n","label":"Data Annotation","value":"Data Annotation"}],"title":"4 Ways AI Makes a Positive Impact on Communities in East Africa"},{"_createdAt":"2020-02-04T22:15:39Z","author":{"_id":"6dd25391-26af-42ff-a2a2-f0778ce1b002","avatar":{"_type":"image","asset":{"_ref":"image-4e1c47984bb16cb0370855b6272663d6744216d3-450x450-jpg","_type":"reference"}},"bio":"Currently a Senior Product Manager at Sama, Audrey guides cross-functional teams to create thoughtful product solutions. She has guided teams of designers and engineers at HUGE Inc. and NBCUniversal, and monitored user analytics at the Wall Street Journal. With a BA in history from Harvard, an MA in anthropology from Columbia and an MBA from UNC Chapel Hill KFBS, Audrey is passionate a using technology and data analytics facilitate social impact and environmental solutions through technology.","name":"Audrey Boguchwal","slug":{"_type":"slug","current":"audrey-boguchwal"}},"estimatedReadingTime":4,"featured_image":{"_type":"image","asset":{"_ref":"image-df3e88916d772584990f79eaa76fbbf00fb8c830-3872x2592-jpg","_type":"reference"}},"meta_description":"During the REWORK Deep Learning Summit, Sama shared how top organizations obtain secure, high-quality training data, fighting AI bias in the process.","openGraphImage":null,"plaintextBody":"I recently presented a talk at the ReWork Deep Learning Summit titled, “Fighting AI Bias: How to Obtain Secure, High-Quality Training Data,\" but I think it’s equally important this knowledge is also shared outside of the summit.\n\nBias can make its way into your model at any stage of the training data lifecycle, potentially compromising the accuracy and performance of your algorithms. And as more organizations develop their own AI and ML programs, the necessity of superior quality data is even more pertinent.\n\nImpact of Biased Data in Computer Vision\n\nAI bias can creep in at any stage of the training data lifecycle, and bias presents itself most commonly in three categories: dataset bias, training bias and algorithmic bias.\n\nDataset bias is as you might expect—the data does not provide enough information for the model to learn the problem, or it’s unrepresentative of reality in some way. Training bias is the result of poor quality or inconsistent data labeling, and lastly, algorithmic bias occurs when the algorithm itself makes poor predictions or produces poor results.\n\nModels trained on biased data not only produce inaccurate algorithms, they also present ethical, legal and safety problems. And in some cases, biased data in computer vision can perpetuate historical, negative stereotypes across race and gender.\n\nLeft unchecked, algorithms trained on biased data greatly impact the lives of people using the very technologies meant to enhance their everyday experiences.\n\n\n\nCountering Bias in Training Data\n\nCountering bias in training data starts by having an effective training data strategy.\n\nLast year at Embedded Vision Summit, I presented a talk on practical training data strategies to avoid bias, sharing four ways to mitigate unwanted bias in training data. \n\nI want to echo my thoughts here that an effective training data strategy makes for a strong defense against AI bias. Fighting bias in training data means determining your data needs, developing training rules to cover known uses cases, and diversifying data to cover edge cases.\n\nAs your model learns, countering bias means evolving rules and sourcing more data when needed—all while keeping apprised of legal and ethical sourcing considerations.\n\nObtaining Superior Quality Datasets\n\nTop organizations understand that if they want smarter models, they need ethically sourced, quality data (https://www.samasource.com/quality-scale). Your quality requirements might vary, depending on your model, but the fact remains that diverse, high-quality data helps counter AI bias.\n\nFor over a decade, hundreds of organizations, including 25% of the Fortune 50 have relied on Samasource to deliver secure, high-quality training data and model validation for machine learning.\n\nWe’ve helped organizations like Walmart improve their retail item coverage, and others like Vulcan Inc., improve turnaround time to process training datasets. We’ve even partnered with organizations like Cornell Tech to produce an open-source dataset of our own. \n\nHere are a few things to keep in mind when sourcing superior quality datasets:\n\nBe aware of local privacy and property laws as you collect data.\n\nEnsure you have legal user consent for data capture.\n\nStay informed of the security protocols of facilities processing your training data.\n\nWhen possible, stay informed of the working conditions of the workers labeling your data, and support that pay living wages and benefits.\n\nFrom pilots to multi-year projects, Samasource securely trains and validates computer vision and NLP models. We work on a range of use cases ranging from e-commerce to autonomous transportation, manufacturing, navigation, retail, AR/VR, and biotech, and if your goal is to build smarter AI, contact our team to discuss your training data needs. ","seo_title":"Fighting AI Bias by Obtaining High-Quality Training Data","slug":{"_type":"slug","current":"fighting-ai-bias-by-obtaining-high-quality-training-data"},"tags":[{"_key":"JOSMV7ei","label":"Events","value":"Events"},{"_key":"X1b43vej","label":"Training Data","value":"Training Data"},{"_key":"zoYY0bth","label":"Data Quality","value":"Data Quality"},{"_key":"8v2jDpFA","label":"AI Bias","value":"AI Bias"}],"title":"Fighting AI Bias by Obtaining High-Quality Training Data"},{"_createdAt":"2019-12-20T02:00:00Z","author":{"_id":"97dc2368-fffb-4c41-82aa-5a9cbe2ec670","avatar":{"_type":"image","asset":{"_ref":"image-af58425525bb33d8cffdc1f1b10f02bf1e4faf57-1916x2028-jpg","_type":"reference"}},"bio":"Sharon is the Content Marketing Manager at Sama where she's responsible for telling the story behind the company's impact sourcing mission and human-powered training data solutions. Sharon holds a MS in Integrated Marketing Communications and is passionate about helping social enterprises transform abstract concepts into results-driven marketing.","name":"Sharon L. Hadden","slug":{"_type":"slug","current":"sharon-l-hadden"}},"estimatedReadingTime":18,"featured_image":{"_type":"image","asset":{"_ref":"image-ade4117797bfbff2e90f48d6ad558ab992aca273-3423x2283-jpg","_type":"reference"}},"meta_description":"In this interview, we chat with Head of AI at Samasource about AI trends to expect in 2020, as well as frequently asked questions about AI and machine learning.","openGraphImage":null,"plaintextBody":"McKinsey Global Institute shared that among the top 5 limitations to adopt AI, two common challenges are labeling training data and obtaining datasets.\n\nIn this interview, Frédéric Ratle, Head of Artificial Intelligence at Sama answers frequently asked questions about AI and machine learning.\n\nRatle also shares AI trends you can expect to see in 2020, how Sama helps enterprise organizations overcome training data challenges, and his thoughts on how AI is shaping the world around us.\n\naudio embed\n\nTranscript: \"Ask Me Anything\" Part 2: 8 Answers to Your Questions About AI and Machine Learning\n\n(00:04) Hello, and welcome to the Ask Me Anything series by Sama, where we interview subject matter experts working in artificial intelligence. I'm your host, Sharon L. Hadden, an AI enthusiast and content marketing manager at Sama.\n\nFrédéric Ratle is the Head of Artificial Intelligence at Sama. He brings 15+ years of R\u0026D experience in machine learning, AI, NLP and computer vision to his role at the company. Frédéric holds a PhD in machine learning, has published numerous research papers and has experience bringing products to market across multiple industries, including healthcare, automotive, consumer electronics and retail.\n\n(00:56) Hi Frédéric. Thanks for joining me today. A little bit about my background, I worked at NVIDIA for a number of years, and it was my first intro into machine learning, deep learning, artificial intelligence. The number one question we were always asked is what's the difference between machine learning and artificial intelligence?\n\n(01:21) It's a really good question. Well, machine learning specifically is concerned with algorithms that can efficiently learn from data. For example, building a classifier that can learn to distinguish, let's say lion images from tiger images based on a set of labeled images is a typical machine learning problem, but it's a subset of AI because AI also includes approaches that aren't data-driven, which I refer to as symbolic approaches. For example, rule-based systems or knowledge or ontology based systems.\n\nThose are AI, but they're not specifically machine learning. Knowing this, the distinction has been a little bit blurred. So we tend to call everything AI. Another difference is that AI implies some kind of goal of mimicking human intelligence, while machine learning clearly is in engineering territory and really aimed at building data-driven decision making systems.\n\n(02:18) That's a great way to put it, Frédéric, and where does deep learning fit into the picture of all of this?\n\n(02:26) Right, so deep learning is a part of machine learning. It's a class of models in machine learning so it's also concerned with algorithms that can learn from data. But the difference with other kinds of models is that it's specifically concerned with so-called deep architectures. These are a model that stack multiple layers of representation and that is believed to lead to the model being able to learn more meaningful features as opposed to traditional shallow models like support vector machines, for example.\n\nIt's a field that has existed for a while actually. People have been trying to use deep neural networks since the 80s, I think, but it only started gaining some traction about 10 years ago, mostly because researchers have found mathematical tricks to optimize those models in an end-to-end way. And also thanks to the increasing availability of computational power, so really cloud computing.\n\n(03:26) Anytime I'm doing research around the history of AI, it's remarkable to see how long we've been using it, but have only just arrived at being able to maximize the technology. Let's talk a little bit about AI versus general artificial intelligence.\n\n(03:46) What we describe as a general artificial intelligence, sometimes is also called strong AI. It's the idea that a machine would be able to be trained and then, it would be able to learn any task that a human can learn.\n\nIt's more of an academic notion really, and I believe we're very far from that because we lack a lot of scientific knowledge about many mechanisms that underlie human intelligence and reasoning. The other kind of AI that you mentioned is the one that's been most successful, in my view. And people in academia typically refer to it as narrow AI.\n\nIt's the ability to really, for machines to mimic your very specific cognitive ability that's normally associated with humans, like speech for example, and in a way that is useful to us. This is mostly on the engineering side and most AI work in industry fall into that category. But that being said, a strong versus narrow AI or AI versus general AI, is really more of an academic debate in my view.\n\nThere's an article that was written last year by professor Michael Jordan from Berkeley, which I really liked, where he presented a framework to categorize different kinds of AI work in a way that is meaningful. I really encourage our listeners to take a look at it. It's called, Artificial Intelligence—The Revolution Hasn't Happened Yet. The three categories that he outlines, there are first what he calls the human imitative AI, which is mostly an academic field of work where people try to build an intelligence that somewhat resembles that of humans.\n\nThe second category is what he calls intelligence augmentation. And this is really an engineering domain that aims at augmenting human intelligence with things like web search, machine translation, and these are things that that really changed the way we interact with information.\n\nAnd the third category is called intelligent infrastructure. So everything around internet of things, sensors—these are basically systems that capture information about the world and try to make intelligent decisions based on that.\n\n(06:08) That really sounds like a crash course on AI, the way you've described it. I would love to know if you have any just examples of technology that isn't artificial intelligence. I know when you were describing kind of the difference between machine learning and AI, you talked a little bit about rule-based systems and I think often in movies media, AI is depicted as you know, anything smart is AI. Could you just share a few examples of technology that isn't AI? Maybe some things that are even commonly mistaken as AI?\n\n(06:49) Actually, I don't really want to single out a particular field because I think in every field there is room for research and development around something that is AI. I do want to point out that many fields of research and many fields of engineering are now called AI because as you say, the media talks a lot about it.\n\nEven practitioners in those domains are starting to talk about AI, but those domains often have a history of their own and most importantly, they have challenges and goals of their own. If you think, for example, the fields like data mining, like operations research, like control theory, even some parts of statistics for example.\n\nI don't think it's necessarily useful to call them out as AI in that sense because we can easily lose sight of what the goal of those disciplines is. For example, if you look at operations research, its goal really is to solve some very specific optimization problems in business. But if you call it an AI, it kind of blurs the notion of what the goal of that field is.\n\n(07:50)Well, thanks. Thanks for laying that out Frédéric. I'd love to talk more about your work at Sama, specific to our training data annotation platform.\n\n(08:02) Humans are much better than machines at recognizing and judging complex situations and use cases. Sama is a human-in-the-loop labeling platform where our customers can upload their data and receive annotations. We really want to make this platform more intuitive and more helpful to AI practitioners, in terms of tools that are available to slice and dice data sets, for example, to sample data sets for labeling. Because sometimes you have huge videos, but it's not exactly clear if you need to label the whole of it or whether you need to apply some smart way of just picking the frames that are important.\n\nWe support many types of annotation formats within that platform, but there are many features that are in the making, in my team, that will soon make their way into production.\n\n(08:57)Thanks for really pointing out that human-to-machine interaction. I think it often gets lost that humans help train AI. So thanks so much for pointing that out. Of the features that you're working on within the platform, how do you see that technology benefiting our customers at Sama?\n\n(09:21) So things like object detection and image segmentation in computer vision should really be an integral part of a data labeling solution. I think that what's really at the top of our team's list, in terms of priority is better quality in terms of annotation, and better efficiency. So that's really what we're looking to achieve. I believe customers can benefit from even better quality of our in-house labeling and also our ability to take on larger sets of data in a matter that is efficient and scalable. It's about being able to scale and also preserving the quality\n\n(10:06) In terms of limitations around AI—I read so much from McKinsey Global Institute, and I think it was last year, they shared a report of the top five limitations to adopt AI. Two of those common challenges are labeling training data and then obtaining datasets. How can Sama help with this? \n\n(10:28) Of course, Sama as a training data provider can of course help with this aspect, more profoundly, as machine learning and AI gain importance in technology, but also generally in our lives, so will the importance of those data sets and more specifically how those data sets are gathered, if you will.\n\nSo I think that beyond the core labeling services, providing expertise on data acquisition and data labeling not only in technical terms, but also from a social and ethical perspective is really essential and will be increasingly important as society evolves, and also as regulation evolves.\n\n(11:13) Well, in your opinion, Frédéric, how is AI shaping the world around us?\n\n(11:20) Oh wow. That's a really good and open question. I have a few ideas about it. Uh, of course, this is only a subset of all of the ways that I think the world will be shaped. But I think it's shaping the world in many ways and whether those ways are positive or negative are really dependent on how we make use of those advances.\n\nSo first from engineering perspective, I think it's really pushing the boundary of many fields that we thought really were more the realm of humans. For example, if we think of speech recognition, machine translation, and also computer vision, advances in AI have rendered those systems very close to human capability.\n\nWhile maybe 10 or 15 years ago that technology really wasn't there yet, and using these systems really felt still cumbersome. So in that sense, I think it's changing the way we interact with technology and the way we interact with information in general. From a social perspective, I think it's pushing a lot of automation and decision making, in ways that can be both good and bad.\n\nI think it can be very good because machines are much better than humans at making very large scale inferences and taking a lot of factors into account when making that decision leading to, you know, high quality decision systems. It can also be very bad, because if we give too much control to these systems without any human oversight, it can be dangerous. And, the nature of the data used to train these models will ultimately determine its performance.\n\nAlso, if you look at a system that's working in a given context and that's been trained in a given context, it may have unpredictable behavior if we slightly changed the context. So that can be also very dangerous.\n\nAnd finally, I think it's of course changing the dynamics of the job market. And the impact that will have on society is I guess in the realm of politics more than technology, but you know, how do we make the greater number of people benefit from the advances that are provided by AI.\n\n(13:40) Frédéric, 2020 is just around the corner and lots of reports are coming out on the state of AI. Are there any trends that you want to call out that we could expect to see in 2020 regarding AI?\n\n(13:58) There are many exciting fields right now that are progressing very, very quickly in AI, but I'd like to call out three things.\n\nI think in the last one or two years there's been a lot of progress in natural language processing. I think what happened in vision, you know, around 2015, 2016 is happening now in NLP because the accuracy of models is increasing quickly. So I think we can expect some more progress in that area in general.\n\nOf course human communication is not only based on words. I think to reach a really human level of understanding of semantics, we'll have to integrate other modalities like nonverbal communication, visual cues, etc. But there's still room for progress, just based on text.\n\nThe second thing I really find very interesting is causality, so the study of cause and effect between different co-variants and explainability, the ability to explain the why it model is making the prediction it's making will keep growing as a topic of interest. It's actually growing very fast right now, and on another level I think that another area that is a really, really interesting is lower resource machine learning.\n\nSo models that can function with energy constraints or size constraints. Because as we grow more conscious about the energetic and environmental impact with machine learning, there's been a couple of articles recently in various newspapers, so models that require less computing will become more popular.\n\nThis is overall a pretty positive trend I think. Not only because it's more environmentally friendly, but also because I think it puts less of an advantage on very large players in the industry that have an unlimited access to computing power.\n\n(16:01) For sure. So, are we talking at all about AI at the edge?\n\n(16:08) Yeah, we're seeing that a lot. On a lot of devices you can see that you have models that you run on the platform, and I think that's something that will only grow because there will also be more regulations pertaining to data exchange.\n\n(16:24) For sure. For sure. Well, is there anything else that stands out to you regarding the state of AI or any challenges with AI adoption?\n\n(16:34) I think there's a couple of things, but just to call out, two of them, you mentioned earlier the data labeling aspect that is difficult for organizations, but I think it goes deeper than that.\n\nI think even the raw data really is an issue because very often in various organizations, depending on their level of technical savviness, the raw data, if available even is often distributed across different departments and various formats, you know—Excel sheets, SQL databases, etc. So it's very difficult to actually take that data and even just do anything with it. I think that's a very big challenge that organizations really need to tackle.\n\nThe second aspect I wanted to call out is also the increasing debate on the use of data in many applications where massive data is collected from users. I think it's a very welcome debate because we're all aware of that regulation has somewhat been lagging behind in that respect, and it has led to a number of problematic situations. But, I think it will be very interesting to see how will the technologies be impacted by all of the new regulations with respect to data ownership and privacy.\n\n(17:50) It's been incredibly eye opening talking with you today, and I think my last question for you is just, what do you love most about working in AI?\n\n(18:02) Most definitely I think it's the ability to constantly work on new problems, and also the ability to really work with really smart and talented people all the time. I also enjoy really shaping a whole new engineering discipline. I think that's very exciting.\n\n\n\nThis interview is the second installment of a new audio blogging series titled, \"Ask Me Anything,\" where Sama interviews subject matter experts working in artificial intelligence.","seo_title":"8 Answers to Your Questions About AI and Machine Learning","slug":{"_type":"slug","current":"8-answers-to-your-questions-about-ai-and-machine-learning"},"tags":[{"_key":"Y7bAcGrE","label":"Machine Learning","value":"Machine Learning"},{"_key":"qhgHNaG5","label":"AI","value":"AI"},{"_key":"0PqR2xB7","label":"Best of","value":"Best of"},{"_key":"NoohHt2W","label":"Sama Engineering","value":"Sama Engineering"}],"title":"8 Answers to Your Questions About AI and Machine Learning"},{"_createdAt":"2019-12-02T19:00:00Z","author":{"_id":"1a59f036-e3fe-4f02-9a34-688ce45de143","avatar":{"_type":"image","asset":{"_ref":"image-7d8f236ba010dd4927d0c5a93368bdce1f712843-390x390-webp","_type":"reference"}},"bio":"Currently a Project Manager at Sama, Taylor Rouleau has a passion for ensuring ethical and sustainable practices in tech. After 5 years leading production teams for our customers, Taylor's expertise is applied internally in our Project Management Office. She heads up efforts to maintain our industry-leading data training processes with a special focus on Security \u0026 Compliance.","name":"Taylor Rouleau","slug":{"_type":"slug","current":"taylor-rouleau"}},"estimatedReadingTime":2,"featured_image":{"_type":"image","asset":{"_ref":"image-980fdc38382a1cc5d9c462b19141b595b5d9ea31-974x460-png","_type":"reference"}},"meta_description":"Samasource was proud to sponsor CodeJam 2019, an annual hackathon at McGill University, from November 15 - 17, 2019.","openGraphImage":null,"plaintextBody":"Sama was proud to sponsor CodeJam 2019, an annual hackathon at McGill University, from November 15 - 17, 2019.\n\nThe event was organized by the McGill Electrical, Computer and Software Engineering Student Society and gathered 300+ students with diverse skill sets to form teams and spend 36 intense hours collaborating to solve cutting-edge problems with code.\n\n\n\nPHOTO: Sama President and COO Wendy Gonzalez (left) and Loic Juillard, VP of Engineering presenting at McGill University CodeJam 2019\n\n\n\nTo kick off the weekend, event sponsors presented on their organization's purpose and shared details of their Sponsor Challenge. Sama’s challenge focused on edge detection, a process which allows the major features and events in images to be automatically cataloged.\n\nSama VP of Engineering, Loic Juillard explained the challenge of extracting structured information from images, as well as the most common approaches and models (usually deep learning) used for this purpose, during his presentation.\n\n\n\nPHOTO: Example of edge detection\n\n\n\nObject detection and tracking is an important and growing field of computer vision, and to give challenge-takers a head start, Frederic Ratle, Head of Artificial Intelligence at Sama, presented a workshop on machine learning techniques for object detection.\n\n\n\nPHOTO: S Head of AI, Frederic Ratle presenting at CodeJam 2019 in Montreal\n\n\n\nRepresentatives from Sama were onsite all weekend to support teams as mentors and witness the University's ecosystem of social innovation and engineering first-hand.\n\nWith the explosive growth of machine learning, and the rise of Montreal as an AI Hub, Canada was a natural choice as the location of our R\u0026D hub. We look forward to continuing to support and connect with the McGill community.","seo_title":"Highlights from McGill CodeJam 2019","slug":{"_type":"slug","current":"highlights-from-mcgill-codejam-2019"},"tags":[{"_key":"FYGgyErZ","label":"Sama Engineering","value":"Sama Engineering"},{"_key":"bymHQxr3","label":"Events","value":"Events"}],"title":"Highlights from McGill CodeJam 2019"},{"_createdAt":"2019-11-25T19:00:00Z","author":{"_id":"97dc2368-fffb-4c41-82aa-5a9cbe2ec670","avatar":{"_type":"image","asset":{"_ref":"image-af58425525bb33d8cffdc1f1b10f02bf1e4faf57-1916x2028-jpg","_type":"reference"}},"bio":"Sharon is the Content Marketing Manager at Sama where she's responsible for telling the story behind the company's impact sourcing mission and human-powered training data solutions. Sharon holds a MS in Integrated Marketing Communications and is passionate about helping social enterprises transform abstract concepts into results-driven marketing.","name":"Sharon L. Hadden","slug":{"_type":"slug","current":"sharon-l-hadden"}},"estimatedReadingTime":7,"featured_image":{"_type":"image","asset":{"_ref":"image-0dda25b386d198579da4a9acc6e67c0ec1dfb06f-1000x667-jpg","_type":"reference"}},"meta_description":"In this interview, we chat with Taylor Rouleau, Project Manager at Sama on how Walmart Labs used AI to improve their retail item coverage.","openGraphImage":null,"plaintextBody":"Walmart Labs recently published an article regarding their work with Sama, where we delivered high-quality training data to further their machine learning initiatives. \n\nThe article highlights how Sama has covered more than 2.5 million items, improving Walmart's retail item coverage from 91 percent to 98 percent.\n\nIn this interview, I chat with Taylor Rouleau, Project Manager at Sama on how she and her team made sure Walmart's machine learning model was set up for success.\n\naudio embed\n\nTranscript: \"Ask Me Anything\" Part 1: How Walmart Labs Used AI to Improve Retail Item Coverage\n\n(00:04) Hello, and welcome to the Ask Me Anything series by Sama, where we interview subject matter experts working in artificial intelligence. I'm your host, Sharon L. Hadden, an AI enthusiast and content marketing manager at Sama.\n\nProject Manager and Social Intrapreneur, Taylor Rouleau has a passion for social responsibility in business. At Sama, Taylor leads production teams in five delivery centers across three countries. When asked, how did Walmart Labs use AI to improve their retail item coverage from 91 percent to 98 percent, here's what Taylor had to say.\n\n(00:45) Hi Taylor and thanks for joining me today. I know in a recent Medium article, it mentioned how Sama has improved Walmart's item coverage from 91% to 98%, and I'm just curious of what needed to happen from a project management perspective to achieve that.\n\n(01:05) We've been working with the Walmart team for the past few years and taking their retail item coverage from 91 up to 98% has been a really great milestone for us.\n\nThe main tasks that we do for Walmart is to tag items, items in their multimillion item catalog. So they'll give us about 6,000 product types and they update that about every month on average. We look at the items and say, okay, this is X product type. It might be a bike pump or a casual t-shirt.\n\nOur data training agents learn and practice remembering a pretty large portion of that taxonomy so they can refer to it really quickly. One person who does really thoroughly know the taxonomy is our team lead in Nairobi, Johnas Wandera.\n\nHe's been really instrumental in training our team of 63 agents to tag everything quickly and accurately. Honestly, as project manager, I just make sure we have everything we need and clear and good communication with the client, to allow Johnas to do his excellent work with the team on the ground.\n\n(02:09) You know, when you were saying, you know one person who's memorized 6,000 product types, I'm thinking Johnas must have some memory.\n\n(02:21) He really does. But honestly he's been working with it for going on 10ish years now. Back from when we first started with them in 2009, so he's had quite a bit of time and practice to get very familiar with it.\n\n(02:35) I would love to know was that 7% increase a targeted goal or just a sweet side effect? \n\n(02:44) Probably a little bit closer to the later. It really wasn't a stated goal, but the Walmart team has been a really great partner working together with us to deliver that 7% increase. It really makes a difference to have a dedicated platform for project setup, workflow and delivery, and the customized training we've done with our agents makes them something like experts on the whole catalog.\n\n(03:06) It even sounds like collaboration really made all the difference.\n\n(03:11) Yeah, definitely. We work really closely with the Walmart team to make sure their expectations were being met, and the excellent production team that we have as well as our great platform made a huge difference.\n\n(03:11) Well, what was the most important thing you would say you needed to do to deliver those results?\n\n(03:32) I think the most important task as a project manager is making sure we really clearly understand the client's expectations and needs and then making sure those are clearly communicated to the data training team on the ground so that they're able to be successful in their work. So I guess just being a good communicator and liaison is the key.\n\n(03:52) What are some common challenges you face to deliver training data?\n\n(03:58) I guess the most challenging thing that will sometimes happen is that we'll be given image annotation work and the images we end up with when we're doing production are busier or more complicated than what the client gave us for our initial testing and training.\n\nThis can also happen if a client changes instructions. I had a client that originally told us that they didn't want us to tag anything smaller than 20 by 20 pixels in images. So we trained around that and had the agents prepared for that. But once we were heading into production, they realized that wasn't really applicable and they wanted us to tag everything.\n\nWe didn't think it was a really big deal at first, but pretty quickly realized that it meant that instead of three annotations per image, we had closer to nine on average. So every image was taking three times as long as we planned for. Thankfully we were able to do a project change request with that client and get them on the same page about the change in complexity, which allowed it to be successful and everyone was happy in the end.\n\n(05:03) Wow. Well that can be a big change if requirements move away from what was originally scoped.\n\n(05:11) Yup, definitely. Which is why it's so important to have a close partnership and good communication with our clients to make sure we're identifying those changes as they happen and continuing to meet the client's needs and expectations.\n\n(05:25) Well, thanks again for joining me today. I think I just have one more question for you and that's, what do you love most about working in AI?\n\n(05:35) I think the AI world is a lot less scary than lay people tend to think. There's so much growth and change happening in the field, so I'm constantly learning and I have the opportunity to work on new and innovative projects pretty frequently.\n\nThere's also a lot of hubbub and controversy around AI, so it's really a privilege to work for an organization that makes sure we're training data ethically and also has a social mission. Working at Sama allows me the expertise to advocate on behalf of both our impact and our data training best practices and those of our clients as well.\n\n\n\nThis interview is the first installment of a new audio blogging series titled, \"Ask Me Anything,\" where Sama interviews subject matter experts working in artificial intelligence.","seo_title":"How Walmart Labs Used AI to Improve Retail Item Coverage","slug":{"_type":"slug","current":"how-walmart-labs-used-ai-to-improve-retail-item-coverage"},"tags":[{"_key":"pujip4Jo","label":"Case Studies","value":"Case Studies"}],"title":"How Walmart Labs Used AI to Improve Retail Item Coverage"},{"_createdAt":"2019-11-19T19:00:00Z","author":{"_id":"a009d418-aa96-47ac-a73a-fd2cd52c79d9","avatar":{"_type":"image","asset":{"_ref":"image-e0d717f753ba4876a6b0dbf9f125cf6c3d27e545-500x500-webp","_type":"reference"}},"bio":"Wendy Gonzalez is an executive passionate about building high-performing, high-functioning teams that develop and scale innovative, impactful technology. With two decades of managerial and technology leadership experience for companies including EY, Capgemini, Cycle30 (acquired by Arrow Electronics) and General Communications Inc, Gonzalez is currently the CEO of Sama, the provider of accurate data for ambitious AI, used by leading technology companies such as Walmart, Google, Nvidia and Getty. Before taking on her role as CEO, Gonzalez spent 5 years at Sama as COO, and is an active Board Member of the Leila Janah Foundation.","name":"Wendy Gonzalez","slug":{"_type":"slug","current":"wendy-gonzalez"}},"estimatedReadingTime":5,"featured_image":{"_type":"image","asset":{"_ref":"image-9cbf25ce2c598676fd52d23ab3fee8700b305c4a-500x333-jpg","_type":"reference"}},"meta_description":"When we work with companies that want to use AI in e-commerce, we notice a few common barriers in AI adoption. Here are 4 reasons e-commerce needs AI to stay ahead.","openGraphImage":null,"plaintextBody":"Serving a niche audience, personalization, cultivating strategic partnerships, or even cutting costs in your supply chain to pass on the savings are well-known ways e-commerce enterprises remain competitive.\n\nHowever, in an era where an agile operation is fast becoming the expectation, here are four reasons e-commerce needs artificial intelligence to stay ahead.\n\nImprove Accuracy and Efficiency in Your Supply Chain\n\nDemand planning already uses machine learning to incorporate data and insights like store traffic, weather forecasts and competitor pricing in supply chain operations, but AI can also help improve accuracy and efficiency in your supply chain in other ways. \n\nSince working together, Sama has improved Walmart’s item coverage from 91 percent to 98 percent.\n\nTraining Walmart’s systems meant manually cataloging and evaluating more than 2.5 million items, and while sharpening the supply chain with AI comes with its share of challenges, having a training data partner with a commitment to quality and security helped further Walmart’s machine learning initiatives. \n\n\n\nStreamline Business Operations\n\nAI can play an integral role in streamlining e-commerce operations. For example, Amazon uses artificial intelligence to make adjustments in delivery routes and arrival times.\n\nOther retailers like Gap are also using a combination of human effort and automation (https://www.theverge.com/2019/5/1/18526092/amazon-warehouse-robotics-automation-ai-10-years-away) to perform dynamic product picking in shipping warehouses. \n\nResearch continues on how to fully automate shipping warehouses, but it’s this level of human-in-the-loop AI (https://www.sama.com/samahub-platform)that shows great promise for e-commerce enterprises.\n\nWorking together, humans and machines can find ways to deliver customer orders with efficiency, providing a stark competitive advantage to companies who adopt AI as part of their operational roadmap.\n\nPut Product Recommendations on Autopilot\n\nA product recommendation engine allows you to suggest products uniquely suited for your buyers. Think of this algorithm as the digital substitute for brick and mortar store clerks who guide shoppers to relevant and similar products.\n\nOur work with Walmart contributes to the accuracy of their product recommendations, and we’ve also helped Getty’s Images catalog stock photos based on key attributes, to support the foundation of its recommendation engine.\n\nRecommendation engines help you up-sell, cross-sell and otherwise inspire customers to bundle offers and create custom solutions best suited for their needs, and according to a McKinsey report, Amazon’s AI-powered recommendation engine was responsible for 35 percent (https://www.mckinsey.com/industries/retail/our-insights/how-retailers-can-keep-up-with-consumers) of the company's revenue. \n\nIn addition to product recommendations, artificial intelligence can automate post-purchase activities like accurately timed repurchasing messages, so your buyers are their most ready to buy.\n\nCreate Immersive Customer Experiences\n\nToday’s always-connected consumer shops from multiple devices, putting seamless customer experiences at the core of driving customer engagement.\n\nA recent study showed that about two-thirds of shoppers (https://www.emarketer.com/content/two-thirds-of-internet-users-check-phones-in-store-for-product-information-skipping-store-associates) check their phone in-store for product information e.g. looking up product reviews, comparing prices, etc. Whether it’s to send targeted and relevant messaging and ads, or to enable real-world product scanning, e-commerce needs AI, in order to create immersive customer experiences at scale. \n\nWhen we work with companies that want to use AI in e-commerce, we notice a few common barriers in AI adoption, such as a lack of training data strategy, lack of talent and/or platform to get started with AI and a lack of high-quality data to properly train AI algorithms.\n\nIf any of these barriers are stopping you from implementing AI across your organization, we can help. Helping e-commerce enterprises keep up with the demand for personalization, while driving the best level of digital classification, i.e., enabling the maximum/optimal experience for users to describe and see what they are shopping for, is what we do.\n\nContact us (https://www.samasource.com/contact-us) to learn more about accelerating your ML pipeline with high-quality training data from Sama.\n\n\n","seo_title":"4 Compelling Reasons to Use AI in E-commerce","slug":{"_type":"slug","current":"4-compelling-reasons-to-use-ai-in-e-commerce"},"tags":[{"_key":"DCGPNa4P","label":"AI","value":"AI"},{"_key":"KwAtlFPO","label":"Best of","value":"Best of"},{"_key":"1uBvx190","label":"Use Cases","value":"Use Cases"},{"_key":"Cryu3t8R","label":"E-Commerce","value":"E-Commerce"}],"title":"4 Compelling Reasons to Use AI in E-commerce"},{"_createdAt":"2019-11-08T19:30:00Z","author":{"_id":"26fb3cc4-608d-40e1-bb4d-955bceda232a","avatar":{"_type":"image","asset":{"_ref":"image-f1fd7fbcc4633299cdbedddba22cb44e24f17317-518x518-svg","_type":"reference"}},"bio":"From self-driving cars to smart hardware, Sama fuels AI. Founded over a decade ago, we’re experts in image, video and sensor data annotation and validation for machine learning algorithms in industries including automotive, navigation, AR/VR, biotech, agriculture, manufacturing, and e-commerce. Our staff are driven by a mission to expand opportunity for low-income people through the digital economy, and our social business model has helped over 50,000 people lift themselves out of poverty.","name":"Sama Team","slug":{"_type":"slug","current":"sama-team"}},"estimatedReadingTime":2,"featured_image":{"_type":"image","asset":{"_ref":"image-c66363bf6c32d33afe6180066409aa883cde1821-1500x1000-jpg","_type":"reference"}},"meta_description":"ICCV 2019 provided a welcoming platform for the distribution and discussion of scholarly and technical work in computer vision.","openGraphImage":null,"plaintextBody":"The workshops and presentations during the International Conference on Computer Vision 2019 (ICCV) were a great preview into what’s next in computer vision.\n\nHere are a few highlights from the premier international computer vision event for students, academics and industry researchers.\n\nPHOTO: Sama Team at ICCV 2019\n\nKey Takeaways from ICCV 2019\n\nICCV attendance increased by 150 percent from 2017 to 2019, going from 3,000 attendees to 7,500. Likewise, ICCV received more than 4,000 papers, up by 50 percent since 2017. Everything from convolutional neural networks to neural architecture search, to robust object detection was published. This article highlights the best paper awards received.\n\nOn the exhibit floor, we found varied applications of computer vision, from a pair of cameras detecting a person's face, age, and emotional state to algorithms helping doctors detect breast cancer more easily. There were also some interesting poster presentations, some of which were highlighted in the ICCV Daily 2019 such as, “Align, Attend and Locate: Chest X-ray Diagnosis via Contrast Induced Attention Network with Limited Supervision.”\n\nPHOTO: Exhibitor booth at ICCV 2019\n\nThroughout the conference, opinions ran high around the use of lidar and radar vs camera sensors only for autonomous driving. There were various approaches on display outlining specific use cases for different sensor packages.\n\nPHOTO: Qualcomm exhibitor display at ICCV 2019\n\n\n\nICCV 2019 provided a welcoming platform for the distribution and discussion of scholarly and technical work, and the Sama team looks forward to continuing conversations on how to securely train and validate computer vision.","seo_title":"3 Takeaways from ICCV 2019","slug":{"_type":"slug","current":"3-takeaways-from-iccv-2019"},"tags":[{"_key":"y8AlUYn8","label":"Best of","value":"Best of"},{"_key":"67uTPs7c","label":"Events","value":"Events"}],"title":"3 Takeaways from ICCV 2019"},{"_createdAt":"2019-10-31T19:30:00Z","author":{"_id":"97dc2368-fffb-4c41-82aa-5a9cbe2ec670","avatar":{"_type":"image","asset":{"_ref":"image-af58425525bb33d8cffdc1f1b10f02bf1e4faf57-1916x2028-jpg","_type":"reference"}},"bio":"Sharon is the Content Marketing Manager at Sama where she's responsible for telling the story behind the company's impact sourcing mission and human-powered training data solutions. Sharon holds a MS in Integrated Marketing Communications and is passionate about helping social enterprises transform abstract concepts into results-driven marketing.","name":"Sharon L. Hadden","slug":{"_type":"slug","current":"sharon-l-hadden"}},"estimatedReadingTime":4,"featured_image":{"_type":"image","asset":{"_ref":"image-7ae53a5241d19127bbf145b4e4827d4d3c611117-1000x667-jpg","_type":"reference"}},"meta_description":"Here's what Nike's use of AR in e-commerce means for the retail industry.","openGraphImage":null,"plaintextBody":"This year, Nike made headlines updating its app with augmented reality to measure your feet before you buy new sneakers. This addition of on-the-go shoe sizing further showcases a pivot across the e-commerce industry toward immersive customer experiences both online and offline.\n\n\n\n3 Benefits of AR in E-commerce\n\nImproved Shopping Experiences\n\nWith AR, e-commerce enterprises have an opportunity to remove online shopping objections like product fit from the equation, resulting in improved shopping experiences for buyers. Online shoe shopping is a prime example, given how shoe sizing can vary from one brand to another.\n\nThe Nike app is said to measure each foot \"within 2 millimeters of accuracy\" and suggest a size based on the style of shoe you’re looking at, which ideally takes the guesswork out of buying shoes online.\n\n\n\nIncreased Buyer Confidence\n\nA general consensus is that Nike shoes run small in most styles, making it difficult for new customers to buy into the brand sight unseen, while making existing advocates think twice about trying new styles from a brand they love. AR provides an avenue for shoppers to find their “perfect fit” whether it’s shoes, clothes, makeup, or even jewelry.\n\nTake Brilliant Earth for example. The company is widely known for its conflict free diamonds, and its mobile user experience for engagement ring shopping includes a virtual try on option powered by AR.\n\n\n\nPHOTO: Brilliant Earth’s AR toolkit in action.\n\n\n\nEven Shopify, a go-to e-commerce platform for established and aspiring brands has made the move to offer AR as a part of their seller toolkit.\n\nThis shows that engaging with shoppers is more than just proactive customer service and two-way conversations—it’s about increasing confidence among buyers by giving them a truer sense of the size, scale and detail of your product.\n\n\n\nHuman-to-Machine Enabled Product Improvements\n\nArguably the most exciting thing about Nike’s AR-enabled app is the potential for human-to-machine collaboration on product improvements.\n\nNike expressed it would be integrating what it learns about customer shoe size as a primary feature for measuring shoes. Over time, this could lead to production improvements based on cumulative measurement data from the app and purchase data from people with similar sized feet.\n\nYes—AR presents an exciting opportunity for the retail industry, but it also marks an era where humans and machines can work together to shape and mold the world around us.\n\nPHOTO: Nike App with AR via The Verge\n\n\n\nIn a time where a 90’s animated series can be turned into an AR-enabled game, and bring people together in droves, artificial intelligence can prove to be a unique competitive advantage.\n\nIn fact, 45 percent of consumers surveyed said they would try AR, and an additional 30 percent reported “they would never go to another clothing store again if AR would allow them to buy the right size clothing with confidence.”\n\nThe market opportunity for AR in e-commerce is promising, however, AI requires a solid foundation of data to be effective, and if that data quality is compromised, so are your algorithms.\n\nAccording to Mckinsey Global Institute, 1 out of 3 use cases for retraining AI systems requires a model refresh at least monthly, and sometimes daily.\n\nNike’s insistence on human-to-machine learning is a signal that continuously refreshed, quality data is needed in order to deliver on the personalized experiences expected of the retail industry.\n\nFor years, Sama has delivered turnkey, high-quality training data and validation to train the world's leading AI technologies. E-commerce is no exception. Download our solution brief to learn more about our secure training data annotation platform, or contact our team here.","seo_title":"What Nike's Use of AR in E-commerce Means for the Retail Industry","slug":{"_type":"slug","current":"what-nikes-use-of-ar-in-ecommerce-means-for-the-retail-industry"},"tags":[{"_key":"CRJq9YCS","label":"AI","value":"AI"},{"_key":"r7tObS22","label":"E-Commerce","value":"E-Commerce"},{"_key":"ERoPGwf6","label":"Retail","value":"Retail"},{"_key":"6vQAFdFK","label":"AR/VR","value":"AR/VR"}],"title":"What Nike's Use of AR in E-commerce Means for the Retail Industry"},{"_createdAt":"2019-10-24T22:00:00Z","author":{"_id":"97dc2368-fffb-4c41-82aa-5a9cbe2ec670","avatar":{"_type":"image","asset":{"_ref":"image-af58425525bb33d8cffdc1f1b10f02bf1e4faf57-1916x2028-jpg","_type":"reference"}},"bio":"Sharon is the Content Marketing Manager at Sama where she's responsible for telling the story behind the company's impact sourcing mission and human-powered training data solutions. Sharon holds a MS in Integrated Marketing Communications and is passionate about helping social enterprises transform abstract concepts into results-driven marketing.","name":"Sharon L. Hadden","slug":{"_type":"slug","current":"sharon-l-hadden"}},"estimatedReadingTime":3,"featured_image":{"_type":"image","asset":{"_ref":"image-31c1da1bdf3c6cd218e26d4f06a30f002ec18a8e-2000x1333-jpg","_type":"reference"}},"meta_description":"This list of computer vision insights shares how artificial intelligence is learning to understand and relate to the intensely visual world around us.","openGraphImage":null,"plaintextBody":"Computer vision is one of the most basic and crucial elements of artificial intelligence, but that doesn’t make it any less interesting.\n\nOur daily experience of the world is human nature and computer vision provides an avenue for artificial intelligence to understand and relate to the intensely visual world we live in.\n\nHere are a few interesting insights about computer vision that we’ve sourced from around the web.\n\n\n\nMost Popular Computer Vision Research Areas\n\nOf all the research papers submitted to this year’s Computer Vision and Pattern Recognition (CVPR) conference, the most popular computer vision research areas were detection, segmentation, 3D and adversarial training, showing an increased interest in unsupervised learning.\n\nIndustries Adopting Computer Vision\n\nWhile automotive continues to hold the largest share of the computer vision market, increasing quality standards across industries has prompted more companies to adopt factory automation fueled by computer vision systems. There’s also a rising demand for vision-guided robotic systems and application-specific computer vision systems, alongside a growing adoption of 3D computer vision.\n\n\n\nA Fundamental Problem with Computer Vision\n\nBhargav Desai, Technical Writer and Instructor at Eduonix shares that deep learning itself is arguably the most fundamental problem with computer vision. The emphasis on its strengths while lacking acknowledgement for apparent weaknesses such as mistaking an overturned school bus for a snowplow is not just a mistake but a revealing mistake.\n\n“It not only shows that deep learning systems can get confused, but that they are challenged in making a fundamental distinction known to all us humans, the distinction between “understanding” the object and “seeing” an object,” Desai explains.\n\nComputer Vision as Part of the Shopping Experience\n\nThere’s a growing demand among consumers for more robust search options including visual and voice search. A recent survey found that over 60% of respondents would be comfortable with new technologies like the ability to search by image as a part of their digital shopping experience.\n\nProjected Market Growth in Computer Vision Solutions\n\nThe market for computer vision solutions is expected to reach over 17 billion U.S. dollars by 2023, which is indicative of the growing pace of research and advancements in computer vision.\n\nThe Growing Application of Computer Vision Systems\n\nEnterprises like DHL are using extended reality experiences like AR to boost accuracy in their business operations. The company is using AR glasses to display picking and placement directions to its operators, and this application of computer vision systems has led to a 15 percent average increase in productivity.\n\n\n\nIf you’re interested in learning more about how Sama quickly trains and validates machine learning, download our solution brief for details on our project set-up and workflows, platform security and annotation tools for computer vision.","seo_title":"Computer Vision Insights From Around the Web","slug":{"_type":"slug","current":"computer-vision-insights-from-around-the-web"},"tags":[{"_key":"gRBnMPo9","label":"Machine Learning","value":"Machine Learning"},{"_key":"rxlxYiKX","label":"AI","value":"AI"},{"_key":"kT5jCNe2","label":"Best of","value":"Best of"},{"_key":"3o90fgFc","label":"Computer Vision","value":"Computer Vision"}],"title":"Computer Vision Insights From Around the Web"},{"_createdAt":"2019-09-26T23:30:00Z","author":{"_id":"26fb3cc4-608d-40e1-bb4d-955bceda232a","avatar":{"_type":"image","asset":{"_ref":"image-f1fd7fbcc4633299cdbedddba22cb44e24f17317-518x518-svg","_type":"reference"}},"bio":"From self-driving cars to smart hardware, Sama fuels AI. Founded over a decade ago, we’re experts in image, video and sensor data annotation and validation for machine learning algorithms in industries including automotive, navigation, AR/VR, biotech, agriculture, manufacturing, and e-commerce. Our staff are driven by a mission to expand opportunity for low-income people through the digital economy, and our social business model has helped over 50,000 people lift themselves out of poverty.","name":"Sama Team","slug":{"_type":"slug","current":"sama-team"}},"estimatedReadingTime":3,"featured_image":{"_type":"image","asset":{"_ref":"image-ea7e9958cf6957b59e16e49dcf1ec785eadbecff-5760x3840-jpg","_type":"reference"}},"meta_description":"Here are a few things to consider, to strengthen your data security practices.","openGraphImage":null,"plaintextBody":"Data security, often referred to as information security (IS), refers to measures taken to prevent unauthorized access to computers, databases and websites. Here are a few things to consider, to strengthen your data security practices.\n\nData Security Best Practices\n\n1. Limit Data Access\n\nLimiting data access based on those who actually need it can help minimize security risks. In addition to things like assigning access based on categories, you can implement infrastructure like a multi-tenant interface to assign roles according to the principle of least privilege.\n\nWe use role based access to ensure client data is only accessed by authorized employees who have a legitimate reason to access the information. For example, data trainers can only perform work, while supervisors can perform work, as well as sampling and QA tasks.\n\n2. Document Your Data Security Policy\n\nA data security policy outlines your organization’s commitment to protect the information you handle, as well as the security measures taken to protect said data.\n\nIf your business operates in or serves residents of the European Union (EU) and European Economic Area (EEA), include details of your data protection responsibilities according to the General Data Protection Regulation (GDPR). Our security and trust webpage features an overview of our data privacy and security policies. You can also download our datasheet to learn more.\n\n\n\n3. Data Encryption In Transit and At Rest\n\nWhen properly encrypted, even compromised data is inaccessible. Our directly managed workforce prepares work in ISO certified delivery centers, using our secure, training data annotation platform. We encrypt client data in transit and at rest, to protect it from unauthorized access from ingestion all the way through to delivery.\n\n4. Conduct Regular Audits\n\nIn addition to limiting data access, encrypting data and following industry-leading data security and privacy practices, consider doing regular audits to help keep your data secure. This could be done on a regular ongoing basis i.e., automated security scanning and pen-testing, in addition to annual or bi-annual checks that take a deeper dive into your infrastructure.\n\n\n\nData privacy and security are fundamental design requirements in our technologies, services, business practices and operations. We use a number of physical and logical security measures, including regular vulnerability testing, to protect client data.\n\nIf you’re interested in learning more about our annotation platform, download the Solution Brief or request a demo.","seo_title":"4 Things That Make a Difference in Data Security","slug":{"_type":"slug","current":"https://www.sama.com/blog/4-things-that-make-a-difference-in-data-security"},"tags":[{"_key":"PJHsWYQd","label":"AI","value":"AI"},{"_key":"n0lW8qhB","label":"Security \u0026 Trust","value":"Security \u0026 Trust"},{"_key":"hDx5GOGL","label":"Training Data","value":"Training Data"}],"title":"4 Things That Make a Difference in Data Security"},{"_createdAt":"2019-08-27T16:55:52Z","author":{"_id":"97dc2368-fffb-4c41-82aa-5a9cbe2ec670","avatar":{"_type":"image","asset":{"_ref":"image-af58425525bb33d8cffdc1f1b10f02bf1e4faf57-1916x2028-jpg","_type":"reference"}},"bio":"Sharon is the Content Marketing Manager at Sama where she's responsible for telling the story behind the company's impact sourcing mission and human-powered training data solutions. Sharon holds a MS in Integrated Marketing Communications and is passionate about helping social enterprises transform abstract concepts into results-driven marketing.","name":"Sharon L. Hadden","slug":{"_type":"slug","current":"sharon-l-hadden"}},"estimatedReadingTime":4,"featured_image":{"_type":"image","asset":{"_ref":"image-732eda1d26d7e742d29a6898b03b98578f328aaf-1920x1280-jpg","_type":"reference"}},"meta_description":"Here are 6 TED talks on AI Ethics anyone working in artificial intelligence should watch.","openGraphImage":null,"plaintextBody":"The impact of AI on our daily lives prompts us to take a hard look at the positive and negative effects of the technology.\n\nSix thought leaders share what's needed to govern these unsupervised algorithms and ensure ethical values, in addition to human intelligence make their way into artificial intelligence.\n\n\n\nTED Talks on AI Ethics\n\n1. The Era of Blind Faith in Big Data Must End\n\nAlgorithms are used in everything from recruiting to banking, and in this TED talk, Cathy O’Neil, mathematician and data scientist, discusses how algorithms could in fact reinforce human bias.\n\n\u003cdiv style=\"max-width:854px\"\u003e\u003cdiv style=\"position:relative;height:0;padding-bottom:56.25%\"\u003e\u003ciframe src=\"https://embed.ted.com/talks/cathy_o_neil_the_era_of_blind_faith_in_big_data_must_end\" width=\"854\" height=\"480\" style=\"position:absolute;left:0;top:0;width:100%;height:100%\" frameborder=\"0\" scrolling=\"no\" allowfullscreen\u003e\u003c/iframe\u003e\u003c/div\u003e\u003c/div\u003e\n\n2. Can We Protect AI from Our Biases?\n\nHumans are inherently biased, but AI doesn’t have to be. Documentary filmmaker, Robin Hauser discusses the need to establish ethical standards for the governance of AI in this TED talk.\n\n\u003cdiv style=\"max-width:854px\"\u003e\u003cdiv style=\"position:relative;height:0;padding-bottom:56.25%\"\u003e\u003ciframe src=\"https://embed.ted.com/talks/robin_hauser_can_we_protect_ai_from_our_biases\" width=\"854\" height=\"480\" style=\"position:absolute;left:0;top:0;width:100%;height:100%\" frameborder=\"0\" scrolling=\"no\" allowfullscreen\u003e\u003c/iframe\u003e\u003c/div\u003e\u003c/div\u003e\n\n3. How to Get Empowered Not Overpowered \n\nIn the next decade, humans expect AI to outsmart human intelligence. MIT Physicist and AI Researcher, Max Tegmark helps us see the opportunities in AI by sharing the steps needed so AI can be used to help humanity flourish.\n\n\u003cdiv style=\"max-width:854px\"\u003e\u003cdiv style=\"position:relative;height:0;padding-bottom:56.25%\"\u003e\u003ciframe src=\"https://embed.ted.com/talks/max_tegmark_how_to_get_empowered_not_overpowered_by_ai\" width=\"854\" height=\"480\" style=\"position:absolute;left:0;top:0;width:100%;height:100%\" frameborder=\"0\" scrolling=\"no\" allowfullscreen\u003e\u003c/iframe\u003e\u003c/div\u003e\u003c/div\u003e\n\n4. How to Keep Human Bias Out of AI\n\nWhile there’s no way to completely rule out bias in your training data, Kriti Sharma, AI technologist shares three ways we can start making more ethical algorithms in her TEDx talk on how to keep human bias out of AI.\n\n\u003cdiv style=\"max-width:854px\"\u003e\u003cdiv style=\"position:relative;height:0;padding-bottom:56.25%\"\u003e\u003ciframe src=\"https://embed.ted.com/talks/kriti_sharma_how_to_keep_human_bias_out_of_ai\" width=\"854\" height=\"480\" style=\"position:absolute;left:0;top:0;width:100%;height:100%\" frameborder=\"0\" scrolling=\"no\" allowfullscreen\u003e\u003c/iframe\u003e\u003c/div\u003e\u003c/div\u003e\n\n5. Machine Intelligence Makes Human Morals More Important\n\nTechno-sociologist, Zeynep Tufekci discusses how human values and human ethics become even more critical as algorithms and digital connectivity spreads.\n\n\u003cdiv style=\"max-width:854px\"\u003e\u003cdiv style=\"position:relative;height:0;padding-bottom:56.25%\"\u003e\u003ciframe src=\"https://embed.ted.com/talks/lang/en/zeynep_tufekci_machine_intelligence_makes_human_morals_more_important\" width=\"854\" height=\"480\" style=\"position:absolute;left:0;top:0;width:100%;height:100%\" frameborder=\"0\" scrolling=\"no\" allowfullscreen\u003e\u003c/iframe\u003e\u003c/div\u003e\u003c/div\u003e\n\n6. The Rise of Human Computer Cooperation\n\nComputers can’t solve major world issues alone. Data Intelligence Agent, Shyam Sankar shares how developing the relationship between computation and human creativity is even more important than finding the right algorithm.\n\n\u003cdiv style=\"max-width:854px\"\u003e\u003cdiv style=\"position:relative;height:0;padding-bottom:56.25%\"\u003e\u003ciframe src=\"https://embed.ted.com/talks/shyam_sankar_the_rise_of_human_computer_cooperation\" width=\"854\" height=\"480\" style=\"position:absolute;left:0;top:0;width:100%;height:100%\" frameborder=\"0\" scrolling=\"no\" allowfullscreen\u003e\u003c/iframe\u003e\u003c/div\u003e\u003c/div\u003e","seo_title":"6 TED Talks to Watch on AI Ethics","slug":{"_type":"slug","current":"6-ted-talks-to-watch-on-ai-ethics"},"tags":[{"_key":"W3DkGS5h","label":"Ethical AI","value":"Ethical AI"},{"_key":"ltDe5GIk","label":"Best of","value":"Best of"},{"_key":"TKGFOsI9","label":"Training Data","value":"Training Data"},{"_key":"L9htGs5b","label":"AI Bias","value":"AI Bias"}],"title":"6 TED Talks to Watch on AI Ethics"},{"_createdAt":"2019-08-20T21:30:00Z","author":{"_id":"97dc2368-fffb-4c41-82aa-5a9cbe2ec670","avatar":{"_type":"image","asset":{"_ref":"image-af58425525bb33d8cffdc1f1b10f02bf1e4faf57-1916x2028-jpg","_type":"reference"}},"bio":"Sharon is the Content Marketing Manager at Sama where she's responsible for telling the story behind the company's impact sourcing mission and human-powered training data solutions. Sharon holds a MS in Integrated Marketing Communications and is passionate about helping social enterprises transform abstract concepts into results-driven marketing.","name":"Sharon L. Hadden","slug":{"_type":"slug","current":"sharon-l-hadden"}},"estimatedReadingTime":3,"featured_image":{"_type":"image","asset":{"_ref":"image-f0d61bc7762955f6affe83c5358b216c9fdc77e3-4836x3372-jpg","_type":"reference"}},"meta_description":"Data isn’t the only thing holding back artificial intelligence. Read more about some of the challenges and trends in AI.","openGraphImage":null,"plaintextBody":"Data isn’t the only thing holding back artificial intelligence. Factors like deciding where and how to deploy AI in business and public perception also play a role in the future of AI.\n\n\nChallenges in Artificial Intelligence\n\nThere’s still work to be done before achieving true intelligence. The gap between artificial intelligence and artificial general intelligence is wide. AI must first “learn to learn” in order to understand and perform intellectual tasks that can be done by humans. (MIT Technology Review)\n\nAI doesn’t always live up to its expectations. With over 50% of people getting their information on AI from movies and TV or social media, the high expectations of AI don’t live up to the hype surrounding it. (McKinsey Global Institute)\n\nPublic opinion of AI is tentative. Public opinion regarding the trustworthiness of AI is tentative. A recent survey found that 50% of consumers feel “optimistic and informed” about AI, while the other half feel “fearful and uninformed”. (Blumberg Capital)\n\nFalse positives and bias in data cause uncertainty. Datasets are curated based on human logic and values, making it difficult to completely rule out bias in the resulting ML model. (Sama)\n\nShortage of data and lack of infrastructure impede the AI pipeline. More companies are considering AI projects, but few have a process to bring projects to production. Enterprises lack the data and infrastructure to support smooth data flow from ingestion to algorithm, making it difficult to operationalize ML models and intelligence. (IDC)\n\nLabeling datasets is arguably the hardest part of building AI. Among other AI challenges like having a clear strategy to source the data that AI requires, organizations argue that the hardest part of building AI products is data preparation and labeling. (Inside Big Data)\n\nDifficulty explaining why a complex decision was reached. AI is programmed to learn by itself, and sometimes, it reaches decisions that no human can explain. We’ve seen this with Google DeepMind’s AlphaZero algorithm, and initiatives like Explainable AI (https://bdtechtalks.com/2019/01/10/darpa-xai-explainable-artificial-intelligence/)have developed to find answers to this problem. (Intercepting Horizons)\n\nFear that AI will negate the necessity of humans in the workforce. There’s a prevailing fear that AI will take over jobs previously performed by humans. However, AI has its limitations. The Future of Jobs report predicts how the division tasks between human and machine will shift between now and 2022. (World Economic Forum)\n\nGood talent is hard to find. Finding professionals that have the right skillset and experience for AI work is among the top challenges for enterprises who have piloted or embedded AI in their organization. (McKinsey Global Institute)\n\nAs new developments in AI occur, the societal and data challenges surrounding the technology will continue to shift and evolve. These challenges should not be overlooked, but rather carefully observed as we pursue human-level intelligence for AI.\n\n","seo_title":"What's Holding Back Artificial Intelligence?","slug":{"_type":"slug","current":"whats-holding-back-artificial-intelligence"},"tags":[{"_key":"opg67GGB","label":"Machine Learning","value":"Machine Learning"},{"_key":"zbe0rrVL","label":"AI","value":"AI"},{"_key":"dV7VmzZT","label":"Best of","value":"Best of"}],"title":"What's Holding Back Artificial Intelligence?"},{"_createdAt":"2019-07-11T22:00:00Z","author":{"_id":"6dd25391-26af-42ff-a2a2-f0778ce1b002","avatar":{"_type":"image","asset":{"_ref":"image-4e1c47984bb16cb0370855b6272663d6744216d3-450x450-jpg","_type":"reference"}},"bio":"Currently a Senior Product Manager at Sama, Audrey guides cross-functional teams to create thoughtful product solutions. She has guided teams of designers and engineers at HUGE Inc. and NBCUniversal, and monitored user analytics at the Wall Street Journal. With a BA in history from Harvard, an MA in anthropology from Columbia and an MBA from UNC Chapel Hill KFBS, Audrey is passionate a using technology and data analytics facilitate social impact and environmental solutions through technology.","name":"Audrey Boguchwal","slug":{"_type":"slug","current":"audrey-boguchwal"}},"estimatedReadingTime":2,"featured_image":{"_type":"image","asset":{"_ref":"image-31269a4f01b8e495d1ec30a473f063a4957254c6-5760x3840-jpg","_type":"reference"}},"meta_description":"Samasource's revamped toolset for 2D image vector segmentation is ideal for computer vision projects using vector shapes to structure training data.","openGraphImage":null,"plaintextBody":"Sama is pleased to announce production availability of our revamped toolset for 2D image vector segmentation using bounding boxes, polygons and lines.\n\nThe vector toolset is ideal for computer vision projects using vector shapes to structure training data for detection, classification, labeling and object tracking projects.\n\nThe new vector toolset is an update to our previous offering for annotating 2D images. The new tools contain UX enhancements and new features designed to further improve annotation efficiency and accuracy for our expert workforce.\n\nHoused in Sama’s proprietary platform, the toolset leverages existing Hub capabilities for work prioritization and quality management, to deliver the highest quality training data every time.\n\nQUALITY\n\nThe new vector toolset is optimized for quality. The annotation workspace has a larger image display and more precise drawing tools enabling Sama’s workforce to annotate with an even greater degree of accuracy.\n\nThe toolset gives individual annotators direct access to image and shape adjustment options (such as shape opacity), instead of restricting these options to the admin control panel.\n\nNow, individual annotators can adjust settings to get the best view of every unique image while annotating.\n\nEFFICIENCY\n\nThe toolset is also optimized for efficiency. The larger image, new keyboard hotkeys and more intelligent zoom features help reduce time spent panning and adjusting an image.\n\nWhat’s more, the toolset is built on a modern codebase that helps streamline development for new features built by Sama’s dedicated engineering team.\n\n\n\nCONSIDERING A 2D VECTOR SEGMENTATION PROJECT?\n\n\nFrom autonomous vehicle applications to high tech e-commerce initiatives, Sama has the annotation technology to put your project on track for success. Drop us a line, and request a demo of the new toolset in action.","seo_title":"Revamped 2D Vector Segmentation","slug":{"_type":"slug","current":"revamped-2d-vector-segmentation-on-samahub"},"tags":[{"_key":"oc26gThR","label":"Product","value":"Product"},{"_key":"8Uhzi08N","label":"Vector Annotation","value":"Vector Annotation"},{"_key":"W46d0x8z","label":"Training Data","value":"Training Data"},{"_key":"NwhOkizM","label":"Data Annotation","value":"Data Annotation"},{"_key":"46Go0q8m","label":"Data Quality","value":"Data Quality"}],"title":"Revamped 2D Vector Segmentation"},{"_createdAt":"2019-07-01T21:45:00Z","author":{"_id":"80db3356-b402-44cb-a717-2e0f9c9fa3e4","avatar":{"_type":"image","asset":{"_ref":"image-399295e959574e371fe2ba7bfd55ba4aee8a589d-320x320-jpg","_type":"reference"}},"bio":"Heather is passionate about bringing world-changing technologies to market and using supply chain purchasing power for good. She is a data-driven strategist experienced in developing and leading go-to-market, communications, and sustainability initiatives at start-ups and multi-national organizations. Heather is most happy when she’s growing companies that make a positive impact, enjoying the outdoors, and spending time with her family.","name":"Heather Gadonniex","slug":{"_type":"slug","current":"heather-gadonniex"}},"estimatedReadingTime":2,"featured_image":{"_type":"image","asset":{"_ref":"image-9da8ed5f9e247d160615efa55bcb38366df86d5a-3000x3000-jpg","_type":"reference"}},"meta_description":"Most media coverage discuss bias, fairness, and ethical use of AI, but the humanitarian aspect of the AI supply chain is often overlooked.","openGraphImage":null,"plaintextBody":"A few months back, I had the opportunity to visit our offices in Nairobi and Uganda, as the final step in my on-boarding at Sama.\n\nMy colleagues had warned me of the vibrancy and grit to be expected, but in the 8 days I spent in East Africa, the “human” in human-powered AI began to come alive.\n\n\n\nPHOTO: Liliosa Mbirimi (left), Heather Gadonniex and Renuka Kaimal (right), pictured with Julius. Julius is a former Sama employee that now employs over 25 people in the business he started with Sama GiveWork Challenge funds.\n\n\n\nIf you’re not familiar, Sama provides training data and algorithm validation to 25 percent of the Fortune 50. While our headquarters is in San Francisco, we’re one of the largest AI providers, and one of the largest digital employers, in East Africa.\n\nBefore joining Sama, I knew AI had a human element - that the algorithms we train couldn’t exist without humans.\n\nAfter all, every marketing slick touts how the combination of human and machine intelligence will make us smarter, more efficient -- superhuman.\n\nWhat we’re not told is that behind every machine learning algorithm, there’s a band of people tasked with labeling the data used to train AI technologies.\n\nFor example, a self-driving car gets its initial smarts from a human reviewing thousands of images and/or video and labeling the cars, trucks, sidewalks, pedestrians, etc., that are present. This is how the car learns to be autonomous.\n\nThese humans creating training data for algorithms are the soul of AI.\n\nEarlier in my career, I had a deep focus in measuring and managing environmental and social impacts of supply chains and implementing corporate impact programs. Carbon footprint and circular economy were part of my everyday lexicon.\n\nNow, staring into the eyes of our human workforce -- the real-life AI supply chain -- I started to think of the implications for supply chain ethics and sustainability. And, I realized how serendipitous it was to be working at an organization that embraced paying a living wage and equally distributing opportunity from the start, thanks to the vision of our founder Leila Janah.","seo_title":"The Ethical AI Supply Chain: Protecting the Soul of AI","slug":{"_type":"slug","current":"the-ai-supply-chain-protecting-the-soul-of-ai"},"tags":[{"_key":"5qTQ6zwl","label":"Ethical AI","value":"Ethical AI"},{"_key":"iRBewe3m","label":"AI","value":"AI"},{"_key":"29LDhGI9","label":"Impact","value":"Impact"},{"_key":"TkUkSBfs","label":"Leila Janah","value":"Leila Janah"}],"title":"The Ethical AI Supply Chain: Protecting the Soul of AI"},{"_createdAt":"2019-06-28T21:59:52Z","author":{"_id":"80db3356-b402-44cb-a717-2e0f9c9fa3e4","avatar":{"_type":"image","asset":{"_ref":"image-399295e959574e371fe2ba7bfd55ba4aee8a589d-320x320-jpg","_type":"reference"}},"bio":"Heather is passionate about bringing world-changing technologies to market and using supply chain purchasing power for good. She is a data-driven strategist experienced in developing and leading go-to-market, communications, and sustainability initiatives at start-ups and multi-national organizations. Heather is most happy when she’s growing companies that make a positive impact, enjoying the outdoors, and spending time with her family.","name":"Heather Gadonniex","slug":{"_type":"slug","current":"heather-gadonniex"}},"estimatedReadingTime":3,"featured_image":{"_type":"image","asset":{"_ref":"image-d37fc7fdfe75a66d5f87a4ab2829f5f7c4030be8-4032x3024-jpg","_type":"reference"}},"meta_description":"From facial recognition to AR/VR, computer vision is changing the way we interact with the world around us. Here are some highlights from CVPR 2019.","openGraphImage":null,"plaintextBody":"Last week, the Sama team attended the Computer Vision and Pattern Recognition (CVPR) conference in Long Beach, one of the largest computer vision conferences in the world.\n\nWith a heavy focus on research presentations, the conference was well attended by students, academics, industry researchers and professionals working in the field of computer vision and machine learning.\n\nWe’ve seen explosive growth in the conference since first attending - a strong indication that the field of computer vision is continuing to grow faster than analysts predict. This year, a record 5,160 papers were submitted, 2,451 more than last year. 1,294 papers were accepted, and there were 9,227 registered attendees, a 32.6% increase from 2018.\n\nPHOTO: Heather Gadonniex and Abha Laddha, exploring Sense Photonics (https://sensephotonics.com/) latest solid-state LiDAR technology. The company recently launched and announced a $26M Series A.\n\n\n\nFor us, this was also a breakthrough conference.\n\nWe released our first open source data set for research and development, in collaboration with Cornell Tech. It was used as part of the Sixth Workshop on Fine-Grained Visual Categorization. You can download it for free on Git Hub.\n\nWe constructed our first ever 20x20 booth, built from scratch to ensure our brand values - trust, quality, scale, and impact - were accurately represented. \n\nWe talked with some of the brightest minds working to address long-standing development challenges in areas like health, agriculture and education at the Computer Vision for Global Challenges dinner hosted by Facebook and Partnership on AI.\n\nWe had countless meetings with prospective employees, customers, and those that want to work with Sama to obtain high-quality training data. They were equally impressed by our technology, training data expertise, and our impact mission.\n\n\n\nPHOTO: Douglas Welcome manning the Sama booth, built from recycled and recyclable materials with East African inspired swag that was impact sourced.\n\n\n\nFrom facial recognition to augmented reality, computer vision is changing the way we interact with the world around us. CVPR 2019 was a reflection of that, and the Sama team looks forward to collaborating with the many organizations leading the way.","seo_title":"Highlights from CVPR 2019","slug":{"_type":"slug","current":"highlights-from-cvpr-2019"},"tags":[{"_key":"9fySqjzW","label":"AI","value":"AI"},{"_key":"8IsWZFwF","label":"Events","value":"Events"},{"_key":"AlpYkFWc","label":"Computer Vision","value":"Computer Vision"}],"title":"Highlights from CVPR 2019"},{"_createdAt":"2019-06-18T17:23:05Z","author":{"_id":"97dc2368-fffb-4c41-82aa-5a9cbe2ec670","avatar":{"_type":"image","asset":{"_ref":"image-af58425525bb33d8cffdc1f1b10f02bf1e4faf57-1916x2028-jpg","_type":"reference"}},"bio":"Sharon is the Content Marketing Manager at Sama where she's responsible for telling the story behind the company's impact sourcing mission and human-powered training data solutions. Sharon holds a MS in Integrated Marketing Communications and is passionate about helping social enterprises transform abstract concepts into results-driven marketing.","name":"Sharon L. Hadden","slug":{"_type":"slug","current":"sharon-l-hadden"}},"estimatedReadingTime":1,"featured_image":{"_type":"image","asset":{"_ref":"image-6590ae389c492e7971eb49032f9d91cf34325811-2247x1500-jpg","_type":"reference"}},"meta_description":"We've collected 6 computer vision infographics on everything from visual search to the history of computer vision from around the web.","openGraphImage":null,"plaintextBody":"There are a number of trends, tips and statistics floating around the web about computer vision.\n\nWe've collected six infographics on everything from visual search to the history of computer vision below, so you can use them in your next slide deck or round-table talk.\n\nSama - Training Data Maturity Infographic\n\n\nSAS - Advantages of Computer Vision Infographic\n\n\n\nSOURCE: Infographic sourced from the SAS website\n\nTech Republic - Enterprise Adoption of AR/VR\n\n\n\nSOURCE: Infographic sourced from the Tech Republic website\n\nComputer Science Zone - History of Computer Science\n\nSOURCE: Infographic sourced from the Computer Science Zone website\n\nMedium - Visual Search Statistics\n\nSOURCE: Infographic sourced from the Medium website\n\nOSA - Sensors for Self-Driving Cars\n\nSOURCE: Infographic sourced from The Optical Society of America website","seo_title":"From Around the Web: 6 Computer Vision Infographics","slug":{"_type":"slug","current":"from-around-the-web-6-infographics-for-computer-vision"},"tags":[{"_key":"bM4ZQ3e6","label":"Best of","value":"Best of"},{"_key":"lg4g7ftM","label":"Computer Vision","value":"Computer Vision"},{"_key":"8tkHMZla","label":"Infographic","value":"Infographic"}],"title":"From Around the Web: 6 Computer Vision Infographics"},{"_createdAt":"2019-06-12T20:00:00Z","author":{"_id":"97dc2368-fffb-4c41-82aa-5a9cbe2ec670","avatar":{"_type":"image","asset":{"_ref":"image-af58425525bb33d8cffdc1f1b10f02bf1e4faf57-1916x2028-jpg","_type":"reference"}},"bio":"Sharon is the Content Marketing Manager at Sama where she's responsible for telling the story behind the company's impact sourcing mission and human-powered training data solutions. Sharon holds a MS in Integrated Marketing Communications and is passionate about helping social enterprises transform abstract concepts into results-driven marketing.","name":"Sharon L. Hadden","slug":{"_type":"slug","current":"sharon-l-hadden"}},"estimatedReadingTime":4,"featured_image":{"_type":"image","asset":{"_ref":"image-a49d2248810c7d2141277314bfa9b78a95c2a9a3-2250x1500-jpg","_type":"reference"}},"meta_description":"Here are 7 organizations attending CVPR 2019 who are leading the way in computer vision, plus 3 noteworthy companies from around the web.","openGraphImage":null,"plaintextBody":"Since 1983, the Computer Vision and Pattern Recognition (CVPR) conference has brought together industry-leading academics, researchers and companies, and CVPR 2019 is no exception.\n\nHere are seven organizations attending CVPR who are leading the way in computer vision, plus three noteworthy companies from around the web.\n\n\n\n\nAdobe Systems Inc. Last year, Adobe Research contributed 30 papers to CVPR 2018. Among the papers contributed to CVPR 2019 are Photmetrix Mesh Optimization for Video-Aligned 3D Object Reconstruction and Texture Mixer: A Network for Controllable Synthesis and Interpolation of Texture.\n\nAurora Innovation Founded by three scientists who are no stranger to autonomous vehicles, Chris Urmson, Sterling Anderson and Drew Bagnell of Aurora Innovation recently announced a deal with Fiat Chrysler to deploy self-driving car technology in its commercial vehicles.\n\nBaidu A Google search for “Baidu computer vision” yields countless scholarly articles. Baidu’s visual tech team took home first place in the ICME 2019 Grand Challenge of 106-p Facial Landmark Localization and continues to explore topics like temporal modeling approaches and sensor fusion.\n\nHyperSense HyprSense’s vision intelligence and human sensing technology for live 3D animation captures and conveys facial expressions in real-time. They’ll be exhibiting their motion capture software at CVPR 2019 near the Gaming Zone.\n\nOpenAI Nonprofit lab, OpenAI’s mission is to ensure artificial intelligence benefits all of humanity. They’ve partnered with companies like Google to visually map how AI systems understand the world around us, and their website features a resource section with free software for experimenting with AI.\n\nSama Sama delivers high quality training data at scale using a combination of our cloud annotation platform and a managed workforce for ethical human-powered training data and validation.\n\nWe’ll be at CVPR 2019 at booth #1561 to answer all your training data questions. Also, don’t miss the Fine-Grained Visual Categorization (FGVC) workshop on June 17. We collaborated with Cornell Tech to produce the fashion dataset being used in the competition.\n\nTwentyBN Featured on VentureBeat’s list of top AI companies in the world, TwentyBN develops technologies that “allow machines to interact with humans the way humans do.” The avatars and AI assistants made possible by TwentyBN put an intelligent spin on automating the retail experience.\n\nValeo Valeo’s innovative technologies help automakers reduce CO2 emissions. The company has built a strong portfolio of technologies and products, including driving assistance systems that use computer vision to fully automate parking, detect objects around the vehicle, etc.\n\nWayfair This may come as a surprise, but Wayfair’s technology blog includes anecdotes and explanations from their data science team. It gives insight to new projects at Wayfair like training image synthesis and reminds us that the use case for computer vision in retail and e-commerce will only continue to grow.\n\nXnor Xnor is helping companies adopt AI at the edge with hardware and software equipped to enable deep learning models on less than power than your phone.\n\nComputer vision often brings about daydreams of driverless cars and facial recognition, but as you can see, there’s more to it than only these applications.\n\nWhat companies are you keeping a close watch on? Leave a comment and let us know.","seo_title":"10 Organizations Leading the Way in Computer Vision","slug":{"_type":"slug","current":"10-organizations-leading-the-way-in-computer-vision"},"tags":[{"_key":"pm9yH2PJ","label":"Best of","value":"Best of"},{"_key":"YwMfUUBk","label":"Data Annotation","value":"Data Annotation"},{"_key":"NsG61NKf","label":"Computer Vision","value":"Computer Vision"}],"title":"10 Organizations Leading the Way in Computer Vision"},{"_createdAt":"2019-06-10T20:30:00Z","author":{"_id":"26fb3cc4-608d-40e1-bb4d-955bceda232a","avatar":{"_type":"image","asset":{"_ref":"image-f1fd7fbcc4633299cdbedddba22cb44e24f17317-518x518-svg","_type":"reference"}},"bio":"From self-driving cars to smart hardware, Sama fuels AI. Founded over a decade ago, we’re experts in image, video and sensor data annotation and validation for machine learning algorithms in industries including automotive, navigation, AR/VR, biotech, agriculture, manufacturing, and e-commerce. Our staff are driven by a mission to expand opportunity for low-income people through the digital economy, and our social business model has helped over 50,000 people lift themselves out of poverty.","name":"Sama Team","slug":{"_type":"slug","current":"sama-team"}},"estimatedReadingTime":3,"featured_image":{"_type":"image","asset":{"_ref":"image-b8d4bb8d54ed036603c03aa7803647dcd5b0709c-1500x1125-jpg","_type":"reference"}},"meta_description":"The Sama Hackathon in Costa Rica encouraged 56 students to look for ways to use data science to help reduce poverty and income inequality.","openGraphImage":null,"plaintextBody":"Can we create platforms to help those in need overcome their current situation?\n\nThis is the question 53 Computer Engineering students set out to answer during the Sama Hackathon on May 31 - June 1, 2019.\n\nThe Hackathon was held at the Jose Figueres Ferrer Library, Costa Rica Institute of Technology, Main Campus - Cartago and encouraged students to look for ways to use data science to help reduce poverty and inequality.\n\nPHOTO: Computer Engineering students at the Sama Hackathon\n\nCosta Rica has multiple sources of data available for analysis, ranging from census data gathered by the National Institute of Statistics and Census, to the Ministry of Economics.\n\nDuring the Hackathon, participants explored whether we can use those data sources to find ideas and promote initiatives to improve employment rates, development of skills, etc.\n\n13 teams set out to look for historical trends on why regions are more/less developed based on market trends, and the three winning teams were awarded based on their ability to come up with public policy ideas based on that data.\n\n\n\nPHOTO: First place winners of the Sama Hackathon\n\nThe Winning Team\n\nThe first place winners organized exports data from Costa Rica to the largest economies in the world and evaluated the potential for new small businesses to be created, depending on their region.\n\nFor instance, if Germany is importing a lot of flowers (in general) and the fraction coming from Costa Rica is small, then could we identify the regions where there is flower growing potential and promote policies to incentivize farmers to move from their current products to flowers.\n\nPHOTO: Second place winners of the Sama Hackathon\n\nThe Runner-Up\n\nThe second place winners combined multiple data sources; probably the most out of any group. They established multiple relationships and synthesized them into their own data sources.\n\nThis team used the most technology out of all the teams, attempting to implement several of the key tools and frameworks suggested, in order to find overarching patterns in data related to economic development from Costa Rican institutions.\n\nPHOTO: Third place winners of the Sama Hackathon\n\nThird Place Winners\n\nThe team characterized small businesses in Costa Rica using factors from multiple databases, such as vertical, economic geo region, and others. In Costa Rica, about 50% of small businesses don't make it to year three. They analyzed if there were companies that were more or less likely to survive the three year anniversary, as a proxy for success.\n\nThey tried to cluster those businesses in order to determine the characteristics of successful ones, in hopes of finding patterns for new entrepreneurs to follow.\n\n\n\nCongratulations to the winning teams and many thanks to all who participated.\n\nThe Samae Hackathon is just one of the many programs and initiatives Sama facilitates. Among others is our scholarship program and GiveWork challenge. Learn more about our impact programs here.","seo_title":"53 Students Use Data Science to Reduce Poverty and Income Inequality","slug":{"_type":"slug","current":"56-students-use-machine-learning-to-reduce-poverty-and-inequality"},"tags":[{"_key":"3Lg7mrMf","label":"Ethical AI","value":"Ethical AI"},{"_key":"WFav2zTI","label":"Events","value":"Events"},{"_key":"A0q55Xjf","label":"Impact","value":"Impact"}],"title":"53 Students Use Data Science to Reduce Poverty and Income Inequality"},{"_createdAt":"2019-06-07T20:00:00Z","author":{"_id":"97dc2368-fffb-4c41-82aa-5a9cbe2ec670","avatar":{"_type":"image","asset":{"_ref":"image-af58425525bb33d8cffdc1f1b10f02bf1e4faf57-1916x2028-jpg","_type":"reference"}},"bio":"Sharon is the Content Marketing Manager at Sama where she's responsible for telling the story behind the company's impact sourcing mission and human-powered training data solutions. Sharon holds a MS in Integrated Marketing Communications and is passionate about helping social enterprises transform abstract concepts into results-driven marketing.","name":"Sharon L. Hadden","slug":{"_type":"slug","current":"sharon-l-hadden"}},"estimatedReadingTime":3,"featured_image":{"_type":"image","asset":{"_ref":"image-bbc3a22b57173b2dd2c6020fe7af40171efed6a9-2250x1500-jpg","_type":"reference"}},"meta_description":"13 open source datasets for machine learning, including one dataset featured in the Fine-Grained Visual Categorization (FGVC) workshop at CVPR 2019.","openGraphImage":null,"plaintextBody":"tion (FGVC) workshop at CVPR 2019 on June 17.\n\nMachine Learning Datasets\n\nCOVID-19 Open Research Dataset Allen Institute for AI partnered with leading research groups to prepare this research dataset of over 45,000 scholarly articles about COVID-19 and the coronavirus family of viruses.\n\nGoogle Open Images Google AI introduced over 9 million images spanning 6,000 categories--”enough to train a deep neural network from scratch.”\n\nWaymo Open Dataset Waymo released one of the largest, most diverse autonomous driving datasets to date. All you need is a Gmail account, and you can access the dataset.\n\nImageNet If you’re looking for an image database organized according to the WordNet hierarchy, give ImageNet a try.\n\niMaterialist-Fashion Sama and Cornell Tech announced the iMaterialist-Fashion dataset in May 2019, with over 50K clothing images labeled for fine-grained segmentation. The dataset was used in the FGVC workshop at CVPR, co-sponsored by Google AI.\n\nFishnet.AI Working together with Sama, The Nature Conservancy released Fishnet.AI, an AI training dataset for fisheries. This dataset of approximately 35,000 images with an average of 5 bounding boxes per image was collected from on-board monitoring cameras for long line tuna fishing activity in the Western and Central Pacific.\n\nVisual Genome Visual Genome is the product of 9 technology professionals with a goal of connecting structured image concepts to language. \n\nUCI Machine Learning Repository The University of California - Irvine (UCI) maintains 474 datasets as a service to the machine learning community. \n\nPew Research Center Gain access to raw data from survey research via Pew Research Center. An account is required to access their datasets, but registration is easy.\n\nLabelme Use the Labelme Matlab toolbox to access a large dataset of annotated images. \n\nLabelled Faces in the Wild (LFW) Develop your facial recognition application using LFW, a collection of over 13,000 face photographs collected from around the web.\n\nDataset Finders\n\nKaggle Data scientists and machine learners can find and publish datasets on Kaggle, an online community that was acquired by Google in 2017. Kaggle’s master list of datasets boasts a wide range of niche data sources.\n\nAmazon Web Services (AWS) With over 110 datasets and counting, you’ll find a web crawl of billions of web pages, NASA satellite imagery and more on the Registry of Open Data for AWS. If you want to add to the registry, of course there’s an AWS Labs GitHub repository (https://github.com/awslabs/open-data-registry/) for that.\n\nGoogle Dataset Search Google Dataset Search indexes datasets from digital libraries, personal websites and publisher pages, so you can find them when you need them. It’s currently in beta, but the predictive interface makes it easy to see what datasets are available on your selected topic at a glance.\n\nThis is just a small sample of the free, open source datasets that are available for machine learning use cases. If you have a dataset or dataset finder you’d like to add, hit us up and let us know.","seo_title":"13 Open Source Datasets for Machine Learning","slug":{"_type":"slug","current":"11-open-source-datasets-for-machine-learning"},"tags":[{"_key":"h28cARez","label":"Machine Learning","value":"Machine Learning"},{"_key":"dDZvCu9M","label":"Best of","value":"Best of"},{"_key":"WAc2iHgu","label":"Events","value":"Events"},{"_key":"hPEDVzwV","label":"Open Datasets","value":"Open Datasets"}],"title":"13 Open Source Datasets for Machine Learning"},{"_createdAt":"2019-06-06T23:01:00Z","author":{"_id":"26fb3cc4-608d-40e1-bb4d-955bceda232a","avatar":{"_type":"image","asset":{"_ref":"image-f1fd7fbcc4633299cdbedddba22cb44e24f17317-518x518-svg","_type":"reference"}},"bio":"From self-driving cars to smart hardware, Sama fuels AI. Founded over a decade ago, we’re experts in image, video and sensor data annotation and validation for machine learning algorithms in industries including automotive, navigation, AR/VR, biotech, agriculture, manufacturing, and e-commerce. Our staff are driven by a mission to expand opportunity for low-income people through the digital economy, and our social business model has helped over 50,000 people lift themselves out of poverty.","name":"Sama Team","slug":{"_type":"slug","current":"sama-team"}},"estimatedReadingTime":4,"featured_image":{"_type":"image","asset":{"_ref":"image-ea7c2cbe6b2ae714f214ce5750d2f485e58d429e-2441x1500-jpg","_type":"reference"}},"meta_description":"Kirk Boydston, Training Data Specialist at Sama shares five considerations to move your machine learning model toward level 4 autonomous driving.","openGraphImage":null,"plaintextBody":"Last week at Autonomous Vehicle Technology Expo, Kirk Boydston, Training Data Specialist, Sama asked attendees if more data was required to get their machine learning model even close to 100% accuracy.\n\nThe answer was an unanimous yes, and the best practices Kirk shared emphasized the importance of a solid training data strategy to move toward level 4 autonomous driving.\n\nHere are five considerations to move your machine learning model down the training data continuum.\n\n1. Train a Proof-of-Concept ASAP to Determine Data Needs\n\nThe feedback gained from a proof-of-concept (POC) model provides the necessary insight to estimate how much data is needed to achieve level 4 maturity. Don’t wait until you have the “right” amount of data, or an all-encompassing data content scope.\n\nAllow the results of your POC to drive the need for more data and refine, expand and improve as unknown use cases and edge cases surface.\n\n2. Set Clear Training Data Rules to Cover Edge Cases\n\nEdge cases should not be an afterthought, but rather a key component of your training data strategy. Objects may have many different names e.g., Is it a scooter? Or a small moped? Or, perhaps a mini motorbike?\n\nDefine clear classification rules for subjective and specific objects, and set objective rules to determine their class. These training data rules will help you cover edge cases effectively.\n\n3. Choose Your Training Data Partner Wisely\n\nImpact and sustainability have become increasingly more important corporate values. Consider partnering with a training data supplier that utilizes an ethical supply chain like Sama.\n\nOur directly managed workforce has annotated over one million images on Sama, and we’ve helped partners like General Motors and Volkswagen achieve high-quality data at scale with our training data expertise.\n\nBottom line is your actions have impact. Not only will an ethically minded training data strategy lead to higher quality results, you’ll be making a difference in communities near and far.\n\n4. Test Your Model for Bias\n\nIn addition to lower precision, models trained on biased data can have ethical, legal and safety problems. For example, data biased toward pedestrians with lighter skin may cause the model to identify pedestrians with darker skin less accurately. \n\nYou will almost always have overrepresentation of some elements and underrepresentation of others, and thoughtfully testing your model for bias before, after and throughout production will help move your model to maturity.\n\n\n\n5. Continuously Evolve Your Model\n\nModel training is never done. Every day, the world is changing around you. New cars, new fashions, etc., require newly sourced and labeled training data. According to McKinsey Global Institute 1 out of 3 AI systems require model refresh at least monthly and sometimes daily. In cases where your model is weak, treat the occurrence like a bug that needs to be fixed and continuously evolve your model.\n\nAchieving 100% accuracy can feel a lot like approaching the speed of light, and while 100% accuracy isn’t necessary for all algorithms (like chatbots), level 4 maturity is the goal automotive OEMs are striving for.\n\nKirk’s presentation, “Warp “Driving” Approaching AI’s Speed of Light,” urged attendees to be relentless in seeking out weak scenarios to train with new data.\n\nHis key takeaway was training and validation for machine learning may require different precision thresholds and volumes depending on the stage of maturity. If you want to get your AV machine learning model toward 100% accuracy, success will come from being quality-focused and iterative.\n\n","seo_title":"Moving Toward Level 4 Autonomous Driving","slug":{"_type":"slug","current":"https://www.sama.com/blog/moving-toward-level-4-autonomous-driving"},"tags":[{"_key":"3KIYoFV1","label":"Machine Learning","value":"Machine Learning"},{"_key":"bPng3xAl","label":"Training Data","value":"Training Data"},{"_key":"TRiIb7nw","label":"Data Quality","value":"Data Quality"},{"_key":"KO7eeuZJ","label":"Autonomous Transportation","value":"Autonomous Transportation"}],"title":"Moving Toward Level 4 Autonomous Driving"},{"_createdAt":"2019-06-06T02:48:23Z","author":{"_id":"97dc2368-fffb-4c41-82aa-5a9cbe2ec670","avatar":{"_type":"image","asset":{"_ref":"image-af58425525bb33d8cffdc1f1b10f02bf1e4faf57-1916x2028-jpg","_type":"reference"}},"bio":"Sharon is the Content Marketing Manager at Sama where she's responsible for telling the story behind the company's impact sourcing mission and human-powered training data solutions. Sharon holds a MS in Integrated Marketing Communications and is passionate about helping social enterprises transform abstract concepts into results-driven marketing.","name":"Sharon L. Hadden","slug":{"_type":"slug","current":"sharon-l-hadden"}},"estimatedReadingTime":7,"featured_image":{"_type":"image","asset":{"_ref":"image-72a27932f674f6ebfb6d90744a43bead1069a121-2250x1500-jpg","_type":"reference"}},"meta_description":"AI-enabled products come with their share of challenges. Here's how Vulcan partnered with Sama to use artificial intelligence for wildlife conservation.","openGraphImage":null,"plaintextBody":"Vulcan, the Seattle-based organization built by Microsoft co-founder Paul Allen, has a long history of supporting research and initiatives that make a global impact.\n\nNow the Vulcan Impact team is continuing its commitment to better protect wild plant and animal species and their habitat by using artificial intelligence for wildlife conservation.\n\nDownload Vulcan Case Study.\n\n\n\nThe Challenge\n\n\nAI-enabled products that can record and monitor African wildlife come with their share of challenges. In addition to requiring massive amounts of training data, the diversity of the data must account for species, landscape, cultural relevance and human influence.\n\n\n\n“We ran into a problem when we were trying to detect cows in the imagery. We had a ton of pictures of cows from Washington, where we are, but cows look different in Africa,” Gracie Ermi, Research Software Engineer at Vulcan, pointed out. “Diversity in the dataset has been super challenging.”\n\n\n\nUnmanned aerial vehicles (UAVs) have proven to be a viable way to capture large amounts of data, however, these aerial surveys result in countless hours of video footage that can make finding value in the data collected challenging.\n\nIf processed by humans alone, the work can prove to be mundane when there’s nothing of interest on the screen for hours on end. This is where machine learning proves useful, and the accuracy of the model depends on the accuracy of the data used to train the algorithm.\n\nTo ensure the highest quality training data, Vulcan partnered with Sama, hiring a dedicated team of data annotators to put bounding boxes around key areas of interest in videos and images, and then pass the data back to Vulcan’s machine learning team to build various ML models.\n\nBen Suidman, a Senior Program Manager at Vulcan, shared, “There was good clear communication about expectations and ease of use of the platform. Often, what looks like a hot stone sitting out in the sun may later turn out to be a termite mound once it’s cooled down.\"\n\n\"We had to provide information about the dataset to Sama, letting them know a set of 2,000 - 10,000 images were collected with certain objects in mind. That’s how we directed them, and ultimately that’s how the system was successfully trained.”\n\nThe Solution\n\nIt was imperative that Vulcan partner with an expert on training data annotation given that any mistakes could lead to inaccuracies in the ML model. The Sama team went through a training period aimed at delivering quality annotation at scale and developing subject matter expertise for Vulcan’s specific use case.\n\n“Once you have a trained team, it pays off because they know what to look for and how to spot certain objects in the imagery. Some objects just blend into the background in images that have been taken from a certain angle from the sky, so you need a trained eye to spot those particular objects,” Suidman said.\n\nTo date, Sama has labeled over 600,000 images for Vulcan, having achieved a quality SLA of 95% in support of their efforts to use AI for wildlife conservation. With Sama’s help, Vulcan is able to expedite the processing of data collected from UAVs, without compromising on quality.\n\nThe Impact\n\nThe Vulcan Impact team’s work to survey wildlife in sub-Saharan Africa is just a small fraction of the work being done for wildlife conservation.\n\nOn any given day, there are multiple projects in flight, all working toward the ultimate goal to protect endangered species and ensure stable and thriving generations of wild animals.\n\nVulcan’s effort to enhance remote identification of animals has the potential to make a huge impact on wildlife conservation, allowing monitoring to be done over a larger area and faster than can be done on foot, or even in vehicles.\n\nDownload Vulcan Case Study.\n\nAdditionally, by automatically detecting visual anomalies with artificial intelligence, Vulcan hopes to enable rapid-response human wildlife conflict and loss of habitat, and potentially use this technology to monitor ecosystems or update censuses of animal species.\n\nVulcan’s efforts could also provide even greater situational awareness to rangers in the parks of Africa and other parts of the world, alerting them in advance of pernicious activities, so they’re better prepared to respond.\n\n“We want to enable and empower agencies to have more confidence in the data that they’re using. If we can lower the uncertainty in the data, we can provide these agencies with higher confidence in the numbers so they can act on them,” Pooja Mathur, Senior Product Manager at Vulcan explained.\n\n\n\n\nVulcan sees the technology they are developing eventually being used for additional use cases, including predictive analytics to forecast how changes in our environment and ecosystem will affect the animals and wildlife around us.\n\nA Word From the Vulcan Impact Team\n\nThe Vulcan impact team credits its success in developing technology products that make a global impact to its collaborative mindset.\n\n\nPawan Nrisimha, Director of Product Management at Vulcan shared, “It’s not about competitive analysis, it’s about partnership analysis. In philanthropy, in tech for good, you are always looking for partnerships.\"\n\n\"It’s about figuring out the puzzle and thinking about, ‘How can I fill in a hole, while somebody else fills in another hole.’ The most important thing is to build key partnerships to work together and not step on each other.”\n\n\nIn working with Sama, Vulcan has significantly improved its turnaround times to process training data, allowing its algorithms to thrive.\n\n\n“Having good data is crucial to any machine learning problem. Working with Sama has been great because not only are we able to get the data in the correct format to train our machine learning algorithms. There’s the added bonus of the impact Sama is making in people’s lives. It creates a win-win all around,” Mathur expressed.","seo_title":"How Vulcan is Using AI for Wildlife Conservation","slug":{"_type":"slug","current":"how-vulcan-is-using-ai-for-wildlife-conservation"},"tags":[{"_key":"GIh9aQR1","label":"Case Studies","value":"Case Studies"}],"title":"How Vulcan is Using AI for Wildlife Conservation"},{"_createdAt":"2019-06-05T14:21:10Z","author":{"_id":"6dd25391-26af-42ff-a2a2-f0778ce1b002","avatar":{"_type":"image","asset":{"_ref":"image-4e1c47984bb16cb0370855b6272663d6744216d3-450x450-jpg","_type":"reference"}},"bio":"Currently a Senior Product Manager at Sama, Audrey guides cross-functional teams to create thoughtful product solutions. She has guided teams of designers and engineers at HUGE Inc. and NBCUniversal, and monitored user analytics at the Wall Street Journal. With a BA in history from Harvard, an MA in anthropology from Columbia and an MBA from UNC Chapel Hill KFBS, Audrey is passionate a using technology and data analytics facilitate social impact and environmental solutions through technology.","name":"Audrey Boguchwal","slug":{"_type":"slug","current":"audrey-boguchwal"}},"estimatedReadingTime":5,"featured_image":{"_type":"image","asset":{"_ref":"image-1184784eeedb6464bcb4703dda418de157e6b21c-2250x1500-jpg","_type":"reference"}},"meta_description":"Samasource exhibited and presented at the venerable 2019 Embedded Vision Summit in Santa Clara, California.","openGraphImage":null,"plaintextBody":"Last week, Sama exhibited and presented at the venerable Embedded Vision Summit in Santa Clara, California.\n\nEmbedded Vision Alliance has hosted EBS since 2012, bringing together technology providers across industries who are enabling innovative and practical applications for computer vision.\n\nThis year for the first time, I represented Sama at the talks and presented a technical talk about Training Data Strategy: Avoiding Bias and Legal and Ethical Sourcing Considerations. Take a look at my related blog post for more details. \n\nAI at the Edge\n\nOn the exhibition floor, the hottest topic was AI at the edge and edge computing. AI at the edge brings AI into small, powerful computer chips located directly in devices, such as phones.\n\nWith the computing happening in the device itself, AI devices no longer need to send data back to the cloud. AI at the edge enables remote and distributed applications, such as using a cell phone to diagnose crop diseases on rural farms.\n\nFor Sama, AI at the edge means new, flexible applications for computer vision that might require new training data strategies and solutions.\n\nHigh quality training data annotation continues to be a high priority need for companies developing AI and computer vision. The challenge remains: high quality data at a reasonable cost and timeline.\n\nBlending highly trained workforces with annotation automation and auto-labeling can make significant gains in efficiency. Sama’s expert, experienced workforce combined with our technology continues to be an offering that is well-positioned to help address these problems with real-world data.\n\nPHOTO: Computer vision in action on the showcase floor recognizes Audrey's face\n\nData Capture\n\nAt the Sama booth, we had many great conversations about data capture. Attendees who stopped by our booth also asked questions about semantic segmentation and smarter video interpolation.\n\nThough computer vision has advanced in many ways, data capture remains a challenge - particularly as algorithms become more sophisticated and require bespoke data sourcing.\n\nThat’s one of the many reasons why Sama plans to expand our data capture offering.\n\n\n\nPHOTO: Wendy Gonzales (left), Audrey Boguchwal and Heather Gadonniew at the Sama booth\n\nData Bias\n\nTowards the end of the two-day event, I presented on training data strategy in the Technical Insights II Track.\n\nIn my presentation, I discussed approaches to training data strategy for avoiding data bias and considering legal and ethical sourcing issues. Data can be biased if it does not represent reality accurately: it is missing examples of use cases, or doesn’t have enough examples of use cases (even if a few are present).\n\nBiased data can result in poor algorithm performance if an algorithm simply doesn’t work as designed. Even worse, biased data can cause problems if say, a facial recognition algorithm identifies the wrong person as a criminal, or an autonomous vehicle algorithm fails to detect the presence of a cyclist.\n\nI also discussed legal and ethical sourcing considerations: knowing regional laws and considering privacy and other gray areas when sourcing data. We’ll update this blog post to link to the video of my presentation when EBS posts it. Also be sure to check out my related blog post for more details.\n\nPHOTO: Audrey mid-sentence while presenting on training data strategy\n\n2019 Women in Vision Reception\n\nWe were invited to attend the first Women in Vision Reception ever held at EBS. It was inspiring to hear the exciting work in which other women are involved. We hope to attend future receptions!\n\n\n\nPHOTO: Women in Vision Reception group photo\n\n\nAs always, Sama continuously seeks to be ahead of the training data market. We are always researching the latest technologies, applications and business cases to be able to offer the most comprehensive training data solutions and strategies.\n\nOur time at the Embedded Vision Summit 2019 gave us an inside look into the computer vision industries’ current needs and challenges. We can’t wait to see what’s next!","seo_title":"Highlights from the 2019 Embedded Vision Summit","slug":{"_type":"slug","current":"highlights-from-the-2019-embedded-vision-summit"},"tags":[{"_key":"2Ur4O0La","label":"Best of","value":"Best of"},{"_key":"8FMbOtTY","label":"Events","value":"Events"}],"title":"Highlights from the 2019 Embedded Vision Summit"},{"_createdAt":"2019-05-29T20:30:00Z","author":{"_id":"a0737369-6ea9-40c1-a9cf-eb99927afadf","avatar":{"_type":"image","asset":{"_ref":"image-e7eabfb4112ca13a4f63e989039243ccde7396ff-3834x3447-jpg","_type":"reference"}},"bio":"Twisha is the Senior Impact Manager at Sama and oversees the end-to-end implementation of the global measurement and evaluation (M\u0026E) system. A large part of her cross-functional work involves utilizing data to analyze the social impact of the business model and invest in innovative worker-centric initiatives.","name":"Twisha Mehta","slug":{"_type":"slug","current":"twisha-mehta"}},"estimatedReadingTime":8,"featured_image":{"_type":"image","asset":{"_ref":"image-4135d710bc08353f4e6ef67905ad0f2dbcb73a54-5012x3432-jpg","_type":"reference"}},"meta_description":"As a social business, we often face the dual challenge of generating social impact while balancing business needs. Learn how we're measuring impact right.","openGraphImage":null,"plaintextBody":"No one promised measuring impact would be easy, but thanks to thought partners like Innovations for Poverty Action and researchers at MIT, we're conducting an experimental study to evaluate the efficacy of our impact sourcing model for the first time.\n\nEditor's Note: This is a cross-posting with Innovations for Poverty Action by Twisha Mehta and Brennan Shearer.\n\nWhen we talk about randomized controlled trials (RCT) in development, we’re often talking about the findings of an academic paper. But another key part is how partners think through the role that rigorous impact evaluations can play in achieving their learning goals.\n\nAn understated part of a successful RCT is the internal organizational buy-in that our implementation partners make to deliver their services in a way that complies with the design of a rigorous evaluation. I was able to catch up with Twisha Mehta from Sama, currently in the middle of an evaluation of their program in Kenya, to hear a partner’s perspective of the goals and challenges they met along the way.\n\nSama provides secure, high quality training data and validation for AI. They count the most prestigious technology firms as their clients and serve technology teams driving humanity forward. As an early adopter of Impact Sourcing, the core of the Sama business model is rooted in social impact. Their staff is driven by a mission to expand opportunity for low-income people through the digital economy.\n\nWhat is Sama, and what’s the program being evaluated?\n\nSama is a social business that provides training data strategy, annotation, and validation services to technology companies. We have global delivery centers in Uganda, Kenya, and India that provide language and computer vision data services for use in machine learning algorithms, such as those used in self-driving cars and virtual reality applications.\n\nOur business model is predicated on the idea that, when combined with the appropriate technology platform, we can effectively train a workforce comprised exclusively of workers from low income, low education backgrounds to deliver high quality digital services at market competitive quality and prices. This is the driving idea behind our Learning and Development (L\u0026D) training program. We recruit students to join 10 day bootcamps from low-income communities throughout Nairobi, Kamapla, and Gulu.\n\nUpon completing the training course, they are eligible to apply for jobs at our global delivery centers. For most trainees, this is the first formal job they’ve had. The curriculum covers basic digital skills, occupational skills relevant to AI work.\n\nThis RCT aims to understand the value of training as a stand-alone service along with the value of training in combination with the ability to apply for a job at our global delivery centers.\n\n(See IPA’s project summary for an easily digestible explanation of the impact evaluation)\n\nWhy was it the right time for a randomized evaluation? What do you hope to learn?\n\nAs a social business, we often face the dual challenge of generating social impact, while also balancing business needs of growth and scale. We’ve learned a lot along the way on how to do that effectively.\n\nMeasuring impact is important to us for a few reasons. First, we want to create more transparency about what we’ve learned. We want to change the mindset of private sector corporations that believe that you have to choose between social impact and profit. You can have both!\n\nWe’ve seen firsthand how transformative the power of a paycheck has been on the lives of the people who we hire. From that perspective we want to use this impact evaluation as a proof point.\n\nSecond, we’ve reached a stage of organizational maturity where we have unprecedented insights into our entire supply chain. Data is a core part of our DNA as we grow and scale- and we are committed to using it for continual learning. Operationally, this positions us to not only ask questions about our impact, but to make changes to our targeting and delivery model based on what we learn.\n\nCan you describe the process of running an RCT within a profitable business?\n\nInternally, we had support from the senior management team which recognized the business case for the RCT – we needed to undertake this study in order to prove the efficacy of our business model and prove that social businesses can thrive in the world (and back it up with rigorous impact data!).\n\nOperationally, the challenge was to deliver within the constraints of our demand driven business without compromising service quality. This meant getting buy-in from teams at the local level and working very closely with the local teams.\n\nWhat were the challenges?\n\nOur foremost concern was about our reputation and maintaining the trust we’ve built in the communities we operate in. It was very important to us to be forthright and transparent about the study process with the people we invited for our Learning and Development training boot camps.\n\nStill, it was a challenging shift for our trainers, who work closely with the communities we recruit from, to inform people that they were placed in the control group- and will not receive any training or opportunity to apply for jobs with Sama.\n\nTo mitigate any perceptions of unfairness, the team decided to run the randomization process with a physical lottery during the open enrollment periods. This helped interested students that were placed in the control group understand that it was simply the luck of the draw.\n\nThe second major challenge was to operationalize the research design. After all, our services are dependent on client demand and we can’t hire for new positions without securing work first. We worked closely with our various business units to forecast hiring based on client demand, then worked with the researchers to plan the research within those constraints.\n\nWhat’s next for this evaluation?\n\nAt Sama, we have a strong data driven culture across all of our business units. Data provides us with concrete evidence to pilot initiatives, understand their efficacy, and invest in ideas that work.\n\nThe RCT is a significant investment for our company. Good, bad, or ugly, we're going to share these results and learn from them. We know that we can use the results of this RCT to improve our training program to deepen our positive impact by creating more digital jobs.\n\nWhen we started, Sama was a seemingly far-fetched idea: Training and hiring marginalized East African youth to do digital work for some of the largest technology companies.\n\nHowever, several philanthropic funders including Rockefeller Foundation and Mastercard Foundation took a chance on us and helped us test and scale our idea until we reached financial sustainability. Seven years after we started, we hit financial sustainability- an elusive north star for many start ups!\n\nNow, as we complete our tenth year as a company, we look forward to the RCT results to validate our impact, create new academic literature on our model, and most importantly, provide evidence to our early supporters that well thought-out out-of-the box ideas are worth investing in! (With impact data of course!)","seo_title":"Measuring Impact for a Social Enterprise","slug":{"_type":"slug","current":"measuring-impact-for-a-social-business"},"tags":[{"_key":"fAZImpYg","label":"Ethical AI","value":"Ethical AI"},{"_key":"i7FgIhg1","label":"Impact","value":"Impact"}],"title":"Measuring Impact for a Social Enterprise"},{"_createdAt":"2019-05-20T18:30:00Z","author":{"_id":"6dd25391-26af-42ff-a2a2-f0778ce1b002","avatar":{"_type":"image","asset":{"_ref":"image-4e1c47984bb16cb0370855b6272663d6744216d3-450x450-jpg","_type":"reference"}},"bio":"Currently a Senior Product Manager at Sama, Audrey guides cross-functional teams to create thoughtful product solutions. She has guided teams of designers and engineers at HUGE Inc. and NBCUniversal, and monitored user analytics at the Wall Street Journal. With a BA in history from Harvard, an MA in anthropology from Columbia and an MBA from UNC Chapel Hill KFBS, Audrey is passionate a using technology and data analytics facilitate social impact and environmental solutions through technology.","name":"Audrey Boguchwal","slug":{"_type":"slug","current":"audrey-boguchwal"}},"estimatedReadingTime":4,"featured_image":{"_type":"image","asset":{"_ref":"image-f9a4a7493982c6bf1be96e8260df8a98e4467b28-2250x1500-jpg","_type":"reference"}},"meta_description":"During Embedded Vision Summit 2019, Audrey Boguchwal will share four training data strategies that help AI teams avoid training data bias.","openGraphImage":null,"plaintextBody":"According to a new McKinsey Global Survey, adoption of artificial intelligence continues to advance, but the same foundational barriers still remain when trying to create value from AI at scale.\n\nAmong the challenges to successfully adopt AI is bias in data and algorithms. Audrey Boguchwal, Senior Product Manager at Sama shares four practical approaches to training data strategy that can help AI teams avoid the effects of training data bias. \n\n\n\nThe Impact of Biased Data\n\n\nModels trained on biased data can be less accurate, resulting in insufficient training for your algorithm. Recent studies have shown that biased data can result in problems with facial recognition used in identification, surveillance, and law enforcement. Biased data can also perpetuate historical, negative stereotypes across race and gender. \n\nEnsuring that reality is always represented in your data is a constructive way to minimize the impact of data bias, however, a clear training data strategy to legally and ethically source the data AI requires is fundamental for developing smarter models.\n\n\n\nAvoid Data Bias with a Practical Training Data Strategy\n\nClearly articulate your end training goal and know what data is needed to get to it. When you start with the end goal in mind, you're primed to think through the skill set, tool set and milestones needed to achieve your training goal. For example, for object classifiers, your training data strategy might include preprocessing data to capture bias or offset dataset bias. You recognize that images may look similar before you begin data collection and make a plan to consider transformations i.e. flip or automatically crop images so they vary.\n\nMap out ways bias can enter data and proactively source data to avoid it. Keep in mind that humans are inherently biased, so eliminating all forms of bias is near impossible. Rigorously examine your own biases and the biases of those providing data/information to you.\n\nAvoid selection bias by varying search terms and data sources, and avoid negative set bias by varying data i.e. collect data that contains background scenes in addition to objects of interest. Test before and after training, on a wide range of data. If in the end, you find your model to be of low variance and high bias, use cross-validation to influence the degree of flexibility of your model. Methods like cross dataset generalization can also help determine how reliant your model is on the \"native\" dataset, when compared to other representative datasets.\n\nEnsure data represents reality for your training goal in quantity and diversity, and replenish data often. High-tech, automotive, retail-- there isn't a single industry adopting AI that shows signs of stagnating growth. Refresh data often to stay ahead of trends. Use more than one training set, especially if it's a stock set. Success comes from being iterative, source and label new data as the world changes.\n\nAn effective training data strategy can help you determine ways to mitigate unwanted bias. Audrey Boguchwal, Senior Product Manager at Sama will present \"Practical Approaches to Training Data Strategy: Bias, Legal and Ethical Considerations this year at the 2019 Embedded Vision Summit conference.\n\nAudrey's presentation will expand on the four strategies to avoid training data bias in this post by exploring use-cases that show how unintended bias can creep into datasets, sharing tests to detect dataset bias, and outlining legal and ethical data sourcing considerations.\n\nIf you'll be attending Embedded Vision Summit, stop by booth #621 to discuss your training data needs with the Sama team, or click below to request a demo of our cloud-based data annotation platform.","seo_title":"4 Training Data Strategies to Avoid Bias","slug":{"_type":"slug","current":"4-training-data-strategies-to-avoid-bias"},"tags":[{"_key":"79xbZuT2","label":"Events","value":"Events"},{"_key":"hX284VAD","label":"Training Data","value":"Training Data"},{"_key":"VrBfZ9C5","label":"AI Bias","value":"AI Bias"}],"title":"4 Training Data Strategies to Avoid Bias"},{"_createdAt":"2018-12-14T19:54:00Z","author":{"_id":"71091c91-664a-44a6-9474-acc40eb12457","avatar":{"_type":"image","asset":{"_ref":"image-bc776336801adf71e2599337e8d6f02186b109d0-500x500-jpg","_type":"reference"}},"bio":"Matthew leads the product team at Sama, responsible for the platform that enables Sama's AI/ML data enrichment teams, internal enterprise operations tools to ensure quality and scalability, and all new product initiatives for the evolution of algorithm development and human-powered automation.","name":"Matthew Landry","slug":{"_type":"slug","current":"matthew-landry"}},"estimatedReadingTime":8,"featured_image":{"_type":"image","asset":{"_ref":"image-9a175242c5b6d9d34ab8d001420280c4279ffe11-1125x1500-jpg","_type":"reference"}},"meta_description":"Training your AI in 3D","openGraphImage":null,"plaintextBody":"The world has three dimensions. Why shouldn't your AI?\n\nToday, we're announcing the production availability of our new 3D annotation engine for the Sama. This new offering allows our expert annotation team to nimbly explore your lidar point clouds, searching for objects of interest and producing the highest quality 3D labels.\n\nAs much as we love a good \"3D\" joke, we can't leave out the important 4th dimension -- time. Building on the object tracking expertise from our video annotation toolkit, we support lengthy sequences of point cloud data, tracking moving objects through time and space.\n\nWAIT, WHAT ARE POINT CLOUDS?\n\nBefore we get too deep into the weeds on 3D annotation, let's take a step back and talk about 3D point clouds and why they are important.\n\nThroughout the history of computer vision technology, two-dimensional camera images have dominated. Techniques to detect edges and extract objects from the background, approaches to estimate relative distances in order to construct implied dimensions, and carefully-calibrated dual-camera captures that triangulate all incrementally nudged forward a machine's ability to \"see.\"\n\nAdvances in deep neural nets dramatically improved computer vision with a greater degree of robustness -- and kicked off the current race to bring self-driving cars to market.\n\nCars are heavy and filled with combustible material. They travel at high velocities. They share the same space as other hurtling vehicles, ambling humans, and wandering animals. This makes the perception of distance absolutely critical. Recognizing an obstruction in the road is step 1; step 2 is estimating distance and making the appropriate response (slam on the brakes? maneuver safely around?).\n\nHence, lidar technology has been rapidly adopted as the key enabler for self-driving cars. Kyle Vogt, CEO of GM Cruise, says, “sensors are a critical enabler for deploying self-driving cars at scale, and LIDARs are currently the bottleneck.” Rapid innovation to make lidar sensors faster, smaller, and cheaper with higher density, higher accuracy data make adoption near inevitable.\n\nWhat does the data look like? Rather than a 2D image displayed on a flat screen, a lidar sensor generates a 3D snapshot of the surrounding environment. It scans the area with pinpricks of light, measuring the time-of-flight of reflections, and thus precisely capturing a collection of (X, Y, Z) points. We can reconstruct this to display on our screens, much like an Xbox game.\n\nThe richness and fidelity of this type of data feeds a new generation of AI algorithms (e.g., see this paper about deep neural nets with direct voxel inputs by Apple), making depth perception much easier.\n\nMUCH MORE THAN SELF-DRIVING CARS\n\nIt's not just self-driving cars. 3D point clouds power a variety of machine vision use cases.\n\nWhile an autonomous vehicle may use a technique like SLAM to dynamically build a 3D map of the environment, providers of high fidelity mapping data collect their own point clouds so that they can provide the ground-truth reference map. For example, providers like HERE could build 3D into their HD Localization maps.\n\nDelivery robots are another exciting use case for lidar sensor based perception. The navigation concepts are very similar to automotive, but many delivery robots can more easily traverse sidewalks and coexist with pedestrians.\n\nIn a warehouse context, robots with 3D lidar sensing are able to shift materials around, rapidly picking up and transporting pallets to reconfigure the warehouse or optimize manufacturing workflows. Collision avoidance in the presence of human operators as well as indoor navigation and mapping round out the common ways that 3D point cloud data power cognitive robots.\n\nFinally, unmanned aerial vehicles -- otherwise known as drones -- can use lidar to collect high resolution 3D data, such as when \"corridor mapping\" a pipeline, surveying a construction site, or performing visual structural inspections.\n\nEmpowering the Human Element\n\nSama has extensive project experience annotating lidar sensor data from cutting-edge instruments. While we quickly ramped projects to produce those annotations on our clients’ own in-house apps, we wanted to create a tool flexible enough for the broader market.\n\nThat's why we're launching our 3D annotation tool that supports a wide range of 3D data types from essentially all lidar sensors. We have likely worked with sensor data from your lidar vendor of choice.\n\nThere's more to it beyond the raw data.\n\nThe Sama mission is to use human ingenuity to accelerate AI development, and one of our favorite tricks is to use algorithms to assist and interact with our expert human annotators. Our new 3D tool has some powerful time-saving features such as:\n\nautomatic cuboid estimation to quickly capture an entire object after clicking only a single point;\n\nautomatic ground-plan extraction to make objects and environs easier to discriminate;\n\nsynchronized camera images for 2D confirmation of the 3D points; and\n\nobject tracking estimation to accelerate annotation and predict movement even when objects are occluded or only partially visible\n\nThis approach lets us combine the skills of expert annotators, who can cleverly correlate what a camera shows with even sparse point clouds, with algorithms to accelerate the manipulation of 3D annotations in space and time. Together, we rapidly produce ground-truth annotations of the highest quality to power 3D perception.\n\nThe bottom line: our clients get to market faster with their latest 3D perception algorithms. Quality and throughput make a difference.\n\nExperience Matters\n\nWe have run many projects through our teams, and consistently produce the quality results made possible only by long-term, dedicated annotation experts. The Sama model allows our agents to concentrate on a single client's data, understanding the nuances and unique requirements of their machine learning team (e.g., how precisely to enclose an object, whether to include side-view mirrors or bicycle racks, how to treat objects that pass out of view, what constitutes sufficient visual contact, and so on).\n\nOne project included side-mounted lidar sensors that produced data rotated by 90 degrees. Rather than expecting our team to annotate with their necks tilted at an angle, our engineering team whipped up a quick \"rotate camera\" tool to quickly reorient the workspace. Between continuous improvements of our Hub annotation studio, iterative refinement of the project objectives and annotation goals, and experience with point cloud data from nearly every lidar sensor out there, there's a bright future in 3D annotation here at Sama.\n\nPartnering with Sama, you can get the most from your 3D lidar projects. We’re incredibly excited about this production-ready 3D annotation tool and the future of high-performance lidar. If you have 3D annotation on your mind and would like to see a demo of our annotation platform in action: Drop us a line! ","seo_title":"Training Your AI in 3D","slug":{"_type":"slug","current":"training-your-ai-in-3d"},"tags":[{"_key":"C3ND2OEf","label":"Product","value":"Product"},{"_key":"dJfUvcIY","label":"Training Data","value":"Training Data"},{"_key":"Se91A2Xp","label":"Data Annotation","value":"Data Annotation"}],"title":"Training Your AI in 3D"},{"_createdAt":"2018-09-08T19:40:00Z","author":{"_id":"26fb3cc4-608d-40e1-bb4d-955bceda232a","avatar":{"_type":"image","asset":{"_ref":"image-f1fd7fbcc4633299cdbedddba22cb44e24f17317-518x518-svg","_type":"reference"}},"bio":"From self-driving cars to smart hardware, Sama fuels AI. Founded over a decade ago, we’re experts in image, video and sensor data annotation and validation for machine learning algorithms in industries including automotive, navigation, AR/VR, biotech, agriculture, manufacturing, and e-commerce. Our staff are driven by a mission to expand opportunity for low-income people through the digital economy, and our social business model has helped over 50,000 people lift themselves out of poverty.","name":"Sama Team","slug":{"_type":"slug","current":"sama-team"}},"estimatedReadingTime":8,"featured_image":{"_type":"image","asset":{"_ref":"image-ed0c756ff646aae4c0602af5a501a53f5046d398-1500x1000-jpg","_type":"reference"}},"meta_description":"Coming AI events to attend September - October 2018","openGraphImage":null,"plaintextBody":"With AI increasingly growing, so are the number of events. To strike the balance between the latest insights from respected industry leaders and get accessible, practical machine learning tactics to apply to real use cases today, here’s our list of events to attend.\n\n\n\n1. ECCV 2018 - EUROPEAN CONFERENCE ON COMPUTER VISION\n\n\nThe 15th European Conference on Computer Vision (ECCV) is happening in München, September 8 – September 14th. It’s one of the top conferences in computer vision with premier academic and technical presentations, co-locating a company exhibition. Many professors, researchers, entrepreneurs and technologists share their ideas and research through papers and poster sessions at focused workshops and tutorials. The expo at the Gasteig Cultural Center showcases companies that are pushing the boundaries of what’s possible with computer vision and machine learning technologies today.\n\nNotable sponsors include many of our clients, amongst others, Microsoft, Facebook and Google AI, as well as other automotive, high tech and ecommerce companies highly adopting AI. We’re proud to be sponsor of ECCV, meet us at the exhibition floor, stand no. 30.\n\n\n2. GTC JAPAN\n\n\nGTC Japan is the largest GPU Technology Event for AI development in Japan, hosted by NVIDIA, September 13-14 in Tokyo. They showcase the latest breakthroughs in autonomous vehicles as well as other AI applications, including smart cities, healthcare and virtual reality. As home base of many of the world’s largest car manufacturers, Japan showcases world-leading technology to accelerate the development of self-driving cars.\n\nThere’s a great line up of over 160 interesting speakers. Some of the speeches we look forward to attend, include Osama Itoh (Honda) on predicting performance of pedestrian protection and Hiroaki Mikami (Sony) on Distributed DNN Training with Adaptive Batch-size Control. Next to that, it will be great to visit Ryuji Hamamoto on Analysing medical data applying AI to diagnosis and treatment. And without a doubt, with the advancements in our proprietary technology platform, we look forward to hear from Yukiko Yanagawa (Omron) speaking about Object type recognition by 3D point cloud LiDAR data.\n\nMeet our VP Sales Houk Nolten or Business Development Director Dennis van Herk, or register to set a time upfront to meet our training data experts during the conference.\n\n\n\n3. AUTOSENS BRUSSELS\n\n\nCalled the leading technical summit for ADAS and autonomous vehicle perception technology, AutoSens is the place to be for engineers that go beyond the hype. From 17-20 September, Brussels will be visited by over 500 senior technical experts from the automotive OEMs and Tier 1 suppliers, offering insightful technical presentations focused around the vehicle perception process chain.\n\nLike Robert Stead, Conference Director of Autosens said, ‘I decided to start this event, because I saw a need to through the hype and create a meeting that is truly for the engineers - the guys and girls who are applying their knowledge and solving engineering problems on a daily basis. Autosens is a technical conference, but set in the business context of a complex supply chain. By assembling a representative advisory board, we can get to the heart of the challenges facing the army of engineers crunching the numbers and running the trials that move ideas from the drawing board into production vehicles.’\n\nMeet our Business Development Director Dennis van Herk or Celie Jenkins or get in touch with our Director Advisory Services Mélodie Kinet, or register to set a time upfront to meet our training data experts during the conference.\n\n\n\n4. AI SUMMIT SAN FRANCISCO\n\n\nNow in its 3rd year, The AI Summit is the world’s first and largest conference \u0026 exhibition to look at the practical implications of AI for enterprise organizations, the actual solutions that are transforming business productivity. The AI Summit aims to help the business leader, data scientist and engineer successfully implement their AI projects.\n\nWe previously attended the AI Summit in London earlier this year, and will be attending again in San Francisco, September 19 - 20. It will be held at the beautiful Palace of Fine Arts, with over 200 speakers and exhibitors. Make sure to catch our team there at booth 908 to talk training data and all of its applications!\n\n\n\n5. AUTO AI BERLIN\n\n\nWe.Connect hosts Europe’s number 1 event on deep driving \u0026 Level +5 automation in Berlin, September 23 – September 25. The event brings together the ADAS scene with all stakeholders who play an active role in the deep driving, imaging, computer vision, sensor fusion and perception and Level +5 automation. The 2-day program provides extensive learning and networking at the Maritim proArte Hotel Berlin.\n\nThe event’s mission is to be an active part of the full automation and self-driving cars reality. You’ll explore how AI, ML \u0026 CV will change technology concepts, architectures, software platforms, ADAS, and further technical requirements. Discuss the challenges of AI supported, cognitive cars, discover latest trends in sensor and perception technology and cover the full scope of deep driving in Level 4 \u0026 Level 5 from market leaders.\n\nMake sure not to miss the icebreaker on Sunday, September 23, and Auto.AI Evening Dinner to network with peers. Let’s meet at one of the events or come to visit us at our booth no.2.\n\n\n\n6. GTC EUROPE\n\n\n‘Over three thousand attendees, more than 200 speaking sessions and over 80 exhibitors with brilliant demos’, those are among the top reasons you can’t afford to miss GTC Europe 2018 according to NVIDIA. The premier AI Conference, October 9 – 11 in Munich, is the place to discover how deep learning is taking part of NVIDIA’s offerings, meet great people from all over the world and learn from breakthrough work in the fields of artificial intelligence, deep learning, virtual reality, high performance computing, autonomous machines and much more.\n\nThe event host speakers from many of our clients, including NVIDIA, Google and Facebook, as well as many others from the automotive car manufacturing and mapping companies, hightech and ecommerce scene. Like to learn more from 10 years of best practices in training the AI models of companies alike? Meet us at our booth E13 near the Nvidia Square.\n\n\n\n7. WORLD AI\n\n\nAfter a great success last year, Amsterdam will host another World Summit AI at October 10 - 11 for the entire AI ecosystem from Enterprise to Big Tech, Startups, Investors and Science. Doubling the size of the summit expecting 6.000 attendees and over 140 of the brightest brains on stage, the event will tackle burning AI issues for 2018 and beyond. Particularly interesting are the applied solutions for enterprises as well as discussions on AI ethics and AI4good.\n\nWith the Netherlands being Sama stepping stone to Europe last year to train the eyes of self-driving cars, robotics and e-commerce platforms to enhance visual search, we didn’t want to close the list without mentioning the great Dutch AI events. Interested to learn from our best practices to train AI? We’d like to welcome you at our European Head Office in the WTC in the Hague!","seo_title":"AI Events to Attend in Fall of 2018","slug":{"_type":"slug","current":"coming-ai-events-to-attend-september-october-2018"},"tags":[{"_key":"t34DnXsa","label":"AI","value":"AI"},{"_key":"m7M7ftb0","label":"Events","value":"Events"}],"title":"AI Events to Attend in Fall of 2018"},{"_createdAt":"2018-07-17T00:00:00Z","author":{"_id":"71091c91-664a-44a6-9474-acc40eb12457","avatar":{"_type":"image","asset":{"_ref":"image-bc776336801adf71e2599337e8d6f02186b109d0-500x500-jpg","_type":"reference"}},"bio":"Matthew leads the product team at Sama, responsible for the platform that enables Sama's AI/ML data enrichment teams, internal enterprise operations tools to ensure quality and scalability, and all new product initiatives for the evolution of algorithm development and human-powered automation.","name":"Matthew Landry","slug":{"_type":"slug","current":"matthew-landry"}},"estimatedReadingTime":5,"featured_image":{"_type":"image","asset":{"_ref":"image-af58edf394d9676203b9bc44284f08ccf52125a6-1000x1500-jpg","_type":"reference"}},"meta_description":"Announcing object tracking with video annotation","openGraphImage":null,"plaintextBody":"Today, Sama announces the availability of our latest image annotation toolset for advanced video object tracking. These new tools, in the hands of our expert annotation workforce, level up Sama’s object tracking performance while maintaining the same extreme quality results of our ground truth training data services.\n\nWhat this means for our customers is an even more scalable approach to annotating the growing stream of video object tracking data. Faster training data production speeds your algorithm development and gets you to market faster.\n\nWhy focus on video object tracking?\n\nTesla, as an example, has over 250,000 cars on the road, each packed with high quality cameras to capture the world around them. Video footage collected from a fleet of this size can feed an extremely sophisticated autonomous driving deep learning system. And it's not just Tesla. In the Bay Area, we've become accustomed to seeing data capture vehicles from just about every autonomous driving company out there -- and all of them are collecting video.\n\nAs the computer vision industry progresses from simple object identification (can the algorithm tell what an object is?) to object tracking (can the algorithm follow a specific object over time?), we need tools that can effortlessly annotate this video stream. Sama delivers.\n\nWhat difference does a tool make?\n\nThe traditional approach to an object tracking project is to split the video into individual images and then annotate each image separately, paying careful attention to ensure consistent identifiers for each unique object in sequential images. It's very challenging work, as any Sama agent or quality analyst will tell you. It takes careful attention to detail and often exceeds the capabilities of most annotation services. (We had to build some supporting tools in our platform to make it tractable.)\n\nSama’s introduction of video annotation for object tracking completely changes the game. Now, an entire video sequence can be assessed as a whole, whether the clip contains 2 frames or 2,000 frames. This feature makes it much easier and faster to follow a single object -- even if it's moving -- from beginning to end of a video. If the object disappears from the camera view and reenters later (think: overtaking a cyclist in traffic, only to have them blow past you at the next intersection), we can easily, accurately accommodate it. The whole process is more efficient while maintaining the highest annotation quality, especially as the density of objects increases. And believe me, image complexity at the cutting edge of computer vision is getting up there.\n\nNo, really, why are you so excited?\n\nOne of the coolest aspects of the new tool is how it semi-automatically annotates frames, which makes for a more efficient workflow. If a user starts by drawing a bounding box around an object, the tool automatically estimates the object's location in subsequent or previous frames. Our expert annotation workforce carefully scrutinizes those estimates, and manually tweaks them as needed to get the tracking fully dialed in.\n\nWhen we think about where to focus our platform development, we're always looking for ways to augment the capabilities of our human workforce. We think about how we can make our data services better by using technology to make our team more efficient and more accurate -- with ever more complicated annotation projects. Video annotation is a very visceral demonstration of this approach.\n\n\n\n(By the way, the process couldn't be easier for customers. Hand over camera footage -- color, b\u0026w, high frame rate, low frame, SD, UHD, whatever -- to our project team, and we manage the entire project from start to finish, delivering annotation results that you can immediately route into your training pipeline.)\n\n\nThat’s a wrap!\n\nIt's the leveling up in the speed -- with the highest accuracy -- of our ground truth training data annotation service that really matters. We work with a many clients developing sophisticated vision algorithms, with very aggressive targets for annotation completeness and correctness. Ground truth training data is precious and object tracking video sequences particularly so. Data scientists need confidence in the quality of that training data so that they squeeze the maximum performance out of their deep learning models, focusing on the architecture and hyper-parameter tuning instead of grooming erroneous data.\n\nPartnering with Sama, you can get the most from your object tracking projects. We’re proud of this production-ready video annotation tool, and have big plans for evolving it. If you have object tracking on your mind and would like to see a demo of our annotation platform in action: Drop us a line!","seo_title":"Introducing Object Tracking with Video Annotation","slug":{"_type":"slug","current":"announcing-object-tracking-with-video-annotation"},"tags":[{"_key":"lFK49VoI","label":"Machine Learning","value":"Machine Learning"},{"_key":"uWZdY0Qc","label":"Product","value":"Product"},{"_key":"vbSgnXan","label":"Video Annotation","value":"Video Annotation"},{"_key":"NfB0x55k","label":"Data Annotation","value":"Data Annotation"}],"title":"Introducing Object Tracking with Video Annotation"},{"_createdAt":"2018-06-21T18:31:10Z","author":{"_id":"26fb3cc4-608d-40e1-bb4d-955bceda232a","avatar":{"_type":"image","asset":{"_ref":"image-f1fd7fbcc4633299cdbedddba22cb44e24f17317-518x518-svg","_type":"reference"}},"bio":"From self-driving cars to smart hardware, Sama fuels AI. Founded over a decade ago, we’re experts in image, video and sensor data annotation and validation for machine learning algorithms in industries including automotive, navigation, AR/VR, biotech, agriculture, manufacturing, and e-commerce. Our staff are driven by a mission to expand opportunity for low-income people through the digital economy, and our social business model has helped over 50,000 people lift themselves out of poverty.","name":"Sama Team","slug":{"_type":"slug","current":"sama-team"}},"estimatedReadingTime":7,"featured_image":{"_type":"image","asset":{"_ref":"image-c664f546c6c09a66a670e1248bbb5b2b9055ae81-1500x1072-jpg","_type":"reference"}},"meta_description":"Machine Learning 101: in this post, we'll present a simple overview of machine learning and how it helps computers solve complex problems.","openGraphImage":null,"plaintextBody":"If you’ve kept up with today’s tech news, then you’ve probably read some pieces about machine learning. Unfortunately, many of those articles target expert audiences who already know how to code and design algorithms. What is machine learning, anyway, and where can you turn to get up to speed on the basics?\n\nIn this post, we’ll present a simple overview of machine learning and how it helps computers solve complex problems. Even if you're a complete novice, you'll learn something new from the information below.\n\n\nWhat is Machine Learning?\n\nInsider blog TechEmergence compiled a definition of machine learning that aggregates definitions from several leading experts in industry and academia:\n\n“Machine Learning is the science of getting computers to learn and act like humans do, and improve their learning over time in autonomous fashion, by feeding them data and information in the form of observations and real-world interactions.” - TechEmergence\n\nThe key difference between machine learning and traditional programming is that a machine learning algorithm does not have to be told formally how to get from the input data to the output data. The machine learning algorithm is given examples of input and the expected output and it learns the rules itself. As the algorithm is presented with more input examples and their associated expected output, it can improve decision making performance over time.\n\n\n\nFor example, if you played chess against software that was built using machine learning, the software powering your computer opponent would study the results of your moves, its moves and its strategies to become a better player. Eventually, it would learn so much that it would defeat you in every game - even when presented with unfamiliar scenarios it had not seen and analyzed. Even if you're a chess master, the computer will almost certainly learn to play better than you.\n\nYou can apply the chess example to any type of information. For instance, machine learning could help software identify people trespassing on property, predict stock market trends, navigate autonomous vehicles, identify farming pests and more. As long as the software has access to useful data and a reliable algorithm, it can learn.\n\n\n\nThe Three Types of Machine Learning\n\nNot surprisingly, a cutting-edge computer science topic like machine learning can get very complicated. Most machine learning work can be grouped into three categories: supervised learning, unsupervised learning, and reinforcement learning.\n\n\nSUPERVISED LEARNING\n\nSupervised learning means that software is trained on data that has been labeled. For instance, you might input a 500 images labeled \"cow\" and another 500 images labeled \"human” into the algorithm. After analyzing the images, the program could differentiate between a picture of a human or a cow based on an analysis of the pixels arrangement color and shape. Reasonably accurate computer vision programs require large quantities training images and can make amusing mistakes if inadvertently trained to notice something else -- like grass. \n\n\n\nLabeling data makes it considerably easier for computers to learn. This principle isn't surprising when you think about how you learn. Imagine if someone handed you a page full of numbers with no explanation. You probably wouldn't know what the numbers mean and thus you wouldn’t know how to process that data right away. However, if you were then handed the same page of data with the label “phone numbers,” the numbers suddenly make more sense.\n\nUNSUPERVISED LEARNING\n\nIn unsupervised machine learning, the data used do not have any labels. Without labels, successful machine learning usually requires more data before it can generate useful outputs. The algorithm can try to detect similarities and differences in the input data and start to group them based on those characteristics. With enough examples, the groupings can become very meaningful.\n\nReferring to the scenario above, the phone numbers (assuming they have area codes) would have three digits that vary far less than the following seven. The unsupervised learning algorithm could start to group the phone numbers based on their similar area codes, and correctly assign a newly discovered phone number into the appropriate area code group.\n\nOf course, the algorithm doesn’t even know what an area code is, but it has learned something important about patterns that it can apply to sorting future samples. (And now you perhaps have an inkling of how Netflix can recommend movies based on your previous viewing choices.)\n\nREINFORCEMENT LEARNING\n\nReinforced learning conceptually splits the difference between supervised and unsupervised approaches with a trial-and-error approach. In the chess-playing example, you might have an algorithm that can make any move, and a grader that tells the algorithm whether the player’s move is legal (that is, if it tries to move a pawn six spaces forward, the grader says, “nope!”). Through trial-and-error, the algorithm would eventually learn how each chess piece should move. Similarly, as it plays through more games, it would learn what it means to win or lose, and how to better achieve the wins.\n\nIn fact, just these kinds of techniques allowed the AlphaGo and AlphaGo Zero programs to very rapidly become world-class Go players.\n\n\nClean Data is Necessary (But Hard to Get)\n\n\nMachine learning relies on clean data. Without reliable data, software can't learn the right lessons or become better at usefully automating tasks. It might learn from the noise instead of the signal.\n\nUnfortunately, it's difficult for data scientists to provide the most advanced learning algorithms with good, clean data. Some of the reasons include:\n\nInsufficient people to label a mountain of raw data;\n\nIrrelevant data that gets mixed in with desired data;\n\nIncomplete or partially labeled data; and\n\nHuman error in labeling data\n\nThese challenges could mean that your machine learning algorithm uses corrupted training data, which could lead to poor learning results that get repeated and amplified. The software, in other words, doesn't learn the right lessons to do its job well.\n\nWorking with a partner that understands the most effective ways to source and identify clean data will give you an advantage over competitors. You can learn more about data enrichment by reaching out to Sama. Our training data work is trusted by the world’s leading technology teams working on AI and Machine Learning across industries, from self driving cars to robotics for advanced surgery.\n\n\n\n\n\n\n\n\n","seo_title":"Machine Learning 101","slug":{"_type":"slug","current":"machine-learning-101"},"tags":[{"_key":"sD519oq6","label":"Machine Learning","value":"Machine Learning"},{"_key":"y63rBA8Q","label":"Best of","value":"Best of"},{"_key":"NDx7I7j8","label":"Training Data","value":"Training Data"}],"title":"Machine Learning 101"},{"_createdAt":"2018-05-18T17:00:00Z","author":{"_id":"73009228-1a1b-400d-b745-5fd85486dff0","avatar":{"_type":"image","asset":{"_ref":"image-f1fd7fbcc4633299cdbedddba22cb44e24f17317-518x518-svg","_type":"reference"}},"bio":null,"name":"David Munene Gakuru","slug":{"_type":"slug","current":"david-munene-gakuru"}},"estimatedReadingTime":null,"featured_image":{"_type":"image","asset":{"_ref":"image-aae8da0891ca0f4d1128a15f20089c0e731872d0-1500x1000-jpg","_type":"reference"}},"meta_description":"Why ISO certification matters: Picking the right training data partner for your algorithm.","openGraphImage":null,"plaintextBody":null,"seo_title":"Why ISO Certification Matters: Choosing the Right Training Data Partner","slug":null,"tags":[{"_key":"olaOVXMP","label":"Security \u0026 Trust","value":"Security \u0026 Trust"},{"_key":"IDs9S3Ja","label":"Training Data","value":"Training Data"}],"title":"Why ISO Certification Matters: Choosing the Right Training Data Partner"},{"_createdAt":"2018-05-11T16:00:00Z","author":{"_id":"6dd25391-26af-42ff-a2a2-f0778ce1b002","avatar":{"_type":"image","asset":{"_ref":"image-4e1c47984bb16cb0370855b6272663d6744216d3-450x450-jpg","_type":"reference"}},"bio":"Currently a Senior Product Manager at Sama, Audrey guides cross-functional teams to create thoughtful product solutions. She has guided teams of designers and engineers at HUGE Inc. and NBCUniversal, and monitored user analytics at the Wall Street Journal. With a BA in history from Harvard, an MA in anthropology from Columbia and an MBA from UNC Chapel Hill KFBS, Audrey is passionate a using technology and data analytics facilitate social impact and environmental solutions through technology.","name":"Audrey Boguchwal","slug":{"_type":"slug","current":"audrey-boguchwal"}},"estimatedReadingTime":1,"featured_image":{"_type":"image","asset":{"_ref":"image-df12378ee9ac92007db6ab4b43e58310688aeb2a-1125x1500-jpg","_type":"reference"}},"meta_description":"We'll showcase how Sama's web research and data cleaning services help create training data for Quid to build their NLP-powered data platform.","openGraphImage":null,"plaintextBody":"We’ve written before about Sama’s proprietary online software that we use to connect low-income individuals to digital work. We’ve also shared how our image annotation service creates training data for a computer vision client.\n\nIn this post, I’ll showcase how Sama’s web research and data cleaning services help create training data for longtime client Quid to build their natural language processing-powered data platform.\n\n\nHow does Quid create business intelligence?\n\n\nQuid is working to ingest the world's collective intelligence. Their goal? Help business decision makers quickly gain the insights they need to make more informed decisions. Quid’s product is a platform that searches, analyzes and visualizes data to deliver key insights. Customers bring questions such as, “What does the current landscape look like for financial technology?” For each question, Quid creates a custom visual interactive map from raw business data: documents and articles.\n\nThis visual map was generated in response to a question about the current global fintech landscape.\n\nTo read the entire case study, download the PDF here!","seo_title":"How Quid Creates Reliable Business Intelligence","slug":{"_type":"slug","current":"how-does-quid-create-reliable-business-intelligence"},"tags":[{"_key":"1EAOekEr","label":"Case Studies","value":"Case Studies"}],"title":"How Quid Creates Reliable Business Intelligence"},{"_createdAt":"2018-03-30T22:32:27Z","author":{"_id":"26fb3cc4-608d-40e1-bb4d-955bceda232a","avatar":{"_type":"image","asset":{"_ref":"image-f1fd7fbcc4633299cdbedddba22cb44e24f17317-518x518-svg","_type":"reference"}},"bio":"From self-driving cars to smart hardware, Sama fuels AI. Founded over a decade ago, we’re experts in image, video and sensor data annotation and validation for machine learning algorithms in industries including automotive, navigation, AR/VR, biotech, agriculture, manufacturing, and e-commerce. Our staff are driven by a mission to expand opportunity for low-income people through the digital economy, and our social business model has helped over 50,000 people lift themselves out of poverty.","name":"Sama Team","slug":{"_type":"slug","current":"sama-team"}},"estimatedReadingTime":7,"featured_image":{"_type":"image","asset":{"_ref":"image-25260ecbc486f8d6c5ce3cbf310355c6b9c90532-1500x1000-jpg","_type":"reference"}},"meta_description":"Data collection is a systematic strategy for gathering and measuring information from a variety of sources to get an accurate picture about a specific area of interest.","openGraphImage":null,"plaintextBody":"Here at Sama, we live, breathe and eat data. While we do assist in data collection, most of the time clients come to us with data in need of enhancement, annotation, and more.\n\nWhere does the data come from? Well, data collection varies widely, so in this post we’ll look into the most common questions around this topic.\n\nData collection is a systematic strategy for gathering and measuring information from a variety of sources to get an accurate picture about a specific area of interest. The collection of various data points allows your company to answer specific questions, evaluate potential outcomes, and make predictions about future possibilities and trends.\n\nData is powerful. However, it’s not only about having the data, but also about knowing how to capture the right information — and then understanding how to use it effectively.\n\nA decision based on data rather than intuition brings success rates up by as much as 79 percent. But what is data collection, what can you do with it, and what is the potential impact to your business?\n\n\nData Collection: Understanding Why It Matters\n\nSuccessful businesses have always relied on some type of data. Yet the difference between businesses of the past and those of today is that the sheer volume of data available has rapidly expanded. Having more data empowers these leaders to tap into insights that were difficult to harness in the past.\n\nBut why is data collection so important? Here are a few of the major reasons.\n\nData is the backbone of solid decisions. Data arms you with the details required to make results-driven decisions. For example, let’s say that you need to restock inventory. One approach is to restock based on general insights and anecdotal details from sales staff. An even better strategy, however, is to tap into the data, gather insights and truly understand what is in short supply and any future trends that may make an impact.\n\nData assists with solving complex challenges. For example, let’s say that you’re planning an upcoming sales promotion and want to feature a specific product. An educated guess will tell you which product might be a good fit for a specific market segment, but data provides an even better option, one that is more accurate. Using analytics, you could view previous purchasing patterns and determine that preferences are leaning toward an entirely different product; performing this analysis saves you time and money.\n\nData assists with improving processes. Perhaps you want to understand why one department performs better than another. Data can help you dig deeper to get a more accurate picture of what’s going on with each department and the factors at play, and then come up with an effective solution so that other departments perform just as well.\n\n\nWhat Type of Data is Most Important to Collect?\n\nWhen you're trying to understand data collection, it helps to better define the pieces of information that must be gathered. The problem isn’t having enough data; there are vast amounts of data for your company to collect, harvest and use to gain valuable insights to run a more profitable business. So the question is actually what type of data do you want?\n\nFor example, let’s say that you own a retail store and you want to better understand how customers are moving through the store. Viewing the paths that customers take to purchase various products would allow you to more accurately anticipate customers’ needs and place the correct merchandising along those paths. Some companies even leverage data collected via GPS through apps in which customers consent to the collection, and the data is available to learn more about customers’ behaviors and preferences.\n\nOr maybe you want to learn more about online shopping patterns for specific segments. For example, how many products and what types of products do customers look at prior to their final purchases? If so, you might collect data points along the buying journey to glean relevant insights and ultimately improve the customer experience.\n\nUltimately, the first step in deciding what data to collect is determining what you want to learn. Because there is a vast amount of data to collect, you must know which “bread crumbs” to follow; otherwise, you’ll be lost.\n\n\nWhat are the Potential Benefits of Effective Data Collection?\n\nThe results of collecting, analyzing and gaining insights from big data are significant. For companies that use big data, 92 percent of executives report being satisfied with the results and 89 percent rate big data as very or extremely important. In fact, 89 percent of those surveyed in a report conducted by General Electric said that lack of big data adoption creates risk of losing market share.\n\nThe effects of accurately collecting the right data apply to a variety of business areas, such as customer service, which has the potential to create much stronger bonds between the company and the customer. For example, Delta Airlines uses big data to tackle one of the biggest fears of travelers: lost baggage.\n\nThe company checks over 130 million bags each year, and it also became the first airline that allowed customers to track luggage via their mobile devices. The tracking application has been downloaded over 11 million times and uses data to help customers relieve the anxiety that comes with the worry of losing important baggage.\n\nThis is just one example of the positive impact that data collection can have, but there are many more, all of which have the potential to create changes in a business that make customers feel truly known and improve your bottom line. In many cases, the data is already there, so all you must do is seek it out, glean insights from it, and start reaping the rewards.\n\nIf your company needs help with data collection, Sama is here for you.\n\nOur services help our many clients achieve their business goals while also changing the lives of our workers, who earn a living wage and lift themselves out of poverty.\n\nContact Sama today to learn more about our data services.","seo_title":"What is Data Collection and Why Do You Care?","slug":{"_type":"slug","current":"what-is-data-collection-and-why-do-you-care"},"tags":[{"_key":"E5C8tuMS","label":"Training Data","value":"Training Data"},{"_key":"nEjDa8yW","label":"Data Quality","value":"Data Quality"}],"title":"What is Data Collection and Why Do You Care?"},{"_createdAt":"2018-03-20T21:00:00Z","author":{"_id":"54800006-1861-42a5-a4bb-0aa2441aef30","avatar":{"_type":"image","asset":{"_ref":"image-ffd42fb1fe492d2a4a5421d537e379ee2ef7850b-299x299-jpg","_type":"reference"}},"bio":"Steve is a Senior Account Executive for Sama focusing on AI applications for the automotive industry.","name":"Steve Allen","slug":{"_type":"slug","current":"steve-allen"}},"estimatedReadingTime":6,"featured_image":{"_type":"image","asset":{"_ref":"image-200df2138bcc00249eacdf9d29fa4c78fc756b0a-844x1500-jpg","_type":"reference"}},"meta_description":"Steve Allen of Sama share's the key questions he gets asked about LiDAR and Point Cloud Annotation.","openGraphImage":null,"plaintextBody":"Sama was recently at the Auto.AI show in San Francisco and over half of the questions asked there were about LiDAR, point cloud data and 3D imagery. Below, we give you the view from our window.\n\n\nWhat is LiDAR and Why is it Such a Hot Topic?\n\nTo answer the first: LiDAR (Light Detection and Ranging) is a laser-based surveying method that builds up a depth-based image of the world by shining out laser lights and then measuring how long it takes for the reflected pulse to bounce back to the sensor. (Read more about LiDAR technology and its uses in this Techcrunch article.)\n\nTo answer the second question; LiDAR is one of the critical technologies in the development and deployment of autonomous vehicles. It is also a part of the development mix in other hot topics like border security, drone mapping and much more.\n\nBecause of the ability to collect three-dimensional measurements, laser scanning systems are used for surveying the built environment (such as buildings, road networks, and railways) as well as creating digital terrain (DTM) and elevation models (DEMs) of specific landscapes.\n\n\n\nEnvironmental applications also benefit from LiDAR – laser scanning is a popular method of map design, including mapping flood risk, carbon stocks in forestry and monitoring coastal erosion.\n\nIn the field of autonomous driving, LiDAR's strength is being able to visualize a 360 view around the vehicle. The only significant company currently not on board is Tesla, who is using camera and radar only at this point. Their abstention from the technology is also widely talked about. And LiDAR is the technology that Waymo/Google and Uber had gone to court over.\n\n\nKey Challenge and Aspirations Around LiDAR and Point Cloud Annotation\n\nMany companies - Sama included - can annotate 2D images, and recognize that LiDAR and 3D (also called Point Cloud) labeling is rapidly growing need. However, labeling 3D data presents some unique challenges.\n\nPoint Cloud Data labeling challenge 1 - navigating/labeling in a 3D space requires a carefully designed UI. Many companies develop proprietary tools - but then need to find a workforce that can be trained to use it. (Sama can help with that.) We are also in the process of developing our own Point Cloud Annotation tool, including a Point Cloud Viewer, that we can make available to clients.\n\nPoint Cloud Data labeling challenge 2 - depending on the sensor you are using, the resolution and the clarity can be miserable - making it hard to differentiate between objects. To get higher resolution, one solution is to use a more sensitive sensor. The top of the line model of these in existence today can cost between $60,000 to $80,000. These systems are primarily used in test and data collection vehicles and are too expensive for production cars.\n\nThe most common solution to this last challenge is that many companies use multiple, but less expensive LiDAR sensors. (We often see test cars from the major brands driving around our neighborhood in San Francisco. They’re usually set up with multiple sensors to build datasets for testing.)\n\nFrom the conversations we had at Auto.AI, we learned that there is a growing need for a broadly available 3D annotation platform that has:\n\nLabeling UI that is intuitive enough for a non-data scientist to use.\n\nFlexibility to be used for more than one customer/industry\n\nAccess to skilled workers who can be trained to provide high quality and consistent annotation (did we mention we help with that?)\n\nThe ability to work on and transfer large files. Large 3D file sizes can make it challenging to share files over the cloud.\n\nThe ability to work on and transfer sensitive files. There is a growing need to ensure the security and privacy of data gathered.\n\nThe ability to produce a top quality data set - the basis of a top quality algorithm.\n\n\nTrends in LiDAR Adoption\n\nAs we have come to expect with technology, investments lead to scale, which leads to price drops. In the past year, GM's Cruise project and Google spinoff Waymo, among many others, made significant investments in LiDAR technology, which is predicted to lead to an order of magnitude cost reduction.\n\nAlong with price drops, we also see considerable gains in capabilities. Tech leader Velodyne's new VLS-128 sensor set a new record by doubling the number of laser beams on its previous top-of-the-line LiDAR system to a massive 128 while shrinking the overall size of the sensor by 70 percent. Velodyne has announced price cuts on other products as well.\n\nOne trend that Sama can speak to personally is LiDAR equipment manufacturers partnering with annotation and labeling partners to develop a full, affordable solution to companies wishing to put this technology to use on their projects. (We help with that.)\n\nWe learned a lot talking to people at the AutoAI show and we look forward to being part of another lively conversation at the Nvidia GPU show later this month. (If you're going, can we buy you a drink?)\n\nPlease drop us a line if you would like to schedule some time to talk about your plans and aspirations in LiDAR or other data-related areas.\n\n","seo_title":"What's the Latest with Lidar and Point Cloud Annotation?","slug":{"_type":"slug","current":"whats-the-latest-with-lidar-and-point-cloud-annotation"},"tags":[{"_key":"DAOlTUFZ","label":"Events","value":"Events"},{"_key":"8LUsgcYe","label":"Data Annotation","value":"Data Annotation"},{"_key":"CvHVKSNz","label":"LiDAR","value":"LiDAR"}],"title":"What's the Latest with Lidar and Point Cloud Annotation?"},{"_createdAt":"2018-03-15T18:31:04Z","author":{"_id":"71091c91-664a-44a6-9474-acc40eb12457","avatar":{"_type":"image","asset":{"_ref":"image-bc776336801adf71e2599337e8d6f02186b109d0-500x500-jpg","_type":"reference"}},"bio":"Matthew leads the product team at Sama, responsible for the platform that enables Sama's AI/ML data enrichment teams, internal enterprise operations tools to ensure quality and scalability, and all new product initiatives for the evolution of algorithm development and human-powered automation.","name":"Matthew Landry","slug":{"_type":"slug","current":"matthew-landry"}},"estimatedReadingTime":7,"featured_image":{"_type":"image","asset":{"_ref":"image-9296abfc6dc641ff3c4469e7235289ee1aa70b58-1500x1000-jpg","_type":"reference"}},"meta_description":"Last week, Sama visited the Auto.AI event, which bills itself as the platform bringing together the stakeholders who play an active role in the deep driving, computer vision, and sensor fusion.","openGraphImage":null,"plaintextBody":"This week, Sama visited the Auto.AI event, which bills itself as the platform bringing together the stakeholders who play an active role in the deep driving, computer vision, sensor fusion, perception and cognitive vehicles scene.\n\nOr to put it another way: anyone who is anyone in the greater ecosystem for self-driving cars.\n\nIn attendance were leading OEMs (Original Equipment Manufacturers) like GM, Toyota, Hyundai, Ford, Mercedes-Benz, and so on. Also, there were many hardware technology companies developing sensors and cameras, as well as myriad providers of technology to make useful the vast data output from these sensors. (ICYMI - We are in the latter category).\n\n\n\nSee our Account Executive Steve Allen answer questions on camera… no pressure!\n\n\n\nA Keen Interest in Data Annotation\n\nWe spoke to many attendees, from both OEMs and sensor tech companies who were very curious about our offerings in the annotation space. They asked how we could annotate data sets, what kinds of data we could work with, and what makes Sama stand out from other options.\n\nOur differentiators in brief: there is great appreciation for the value of our skilled agents, as we can train them directly during pilot projects, leading to immediate and consistent quality output during the project; they are on staff and work in Sama-operated offices with structured supervision and project management; and our ISO 9001 certification was very appreciated.\n\nAnd the fact that we have a measurable social impact mission adds value to the company’s CSR departments. (Stay tuned for our upcoming blogs where we go into a little more detail on these aspects.)\n\nAutonomous Driving (AD) Tech Trends\n\nIt was notable that there were several speakers talking about the need for car/passenger interaction design. Some examples:\n\nAcknowledgement of the importance of the vehicle's ability to communicate to the passenger what objects the car is noticing and that influence self-driving decisions.\n\nExposing these decision hints visually will help people to gain trust in the AI and understand how it works. There is an intrinsic problem where it's really hard to explain why a neural network makes a given decision -- but this could help.\n\nThe awareness that autonomous vehicles need to take suggestions from the passenger on Spotify stations or travel route preferences, keep track of who is in the car, and have an internal sense of the state of the car (spilled coffee, forgotten phones).\n\nThere were many examples of the research around reading facial cues or what human commands or questions really mean.\n\nWhat this tells me is that the technology is getting to a more mature state where we can put effort into these secondary considerations are nonetheless necessary to making this technology viable and successful in the mainstream.\n\nThe conference ended with a demo from GM of a L2 autonomous system - currently available at a Caddy dealership near you. While not fully autonomous, the vehicle can take control on the highway and proactively vibrate seats and flash lights to keep drivers alert and engaged.\n\nThey also showed demo videos of their L4 autonomous Chevy Bolts fielded by the Cruise Automation subsidiary operating in San Francisco. Though there were passengers in the car who could take control if necessary, it was a compelling proof of concept in a denser, more dangerous environment with bicyclists and pedestrian traffic. GM/Cruise hasn’t officially announced when their automated ride hailing service will launch to the public, but it’s already operating in beta for San Francisco employees! In short, GM is demonstrating pretty rapid rapid development and growth.\n\n\n\nAn overview of some of the players that was shared at the conference. GM is leader of the pack. Navigant Research\n\nA company called Renovo Motors shared their vision of an OS for self-driving taxis - tying together a coalition of technologies around the projected $1 trillion “robotaxi” marketplace. They make bold, exciting predictions that by 2030, 95% of all urban miles will be driven by autonomous taxis.\n\nA trend worth watching - “multitasking networks.” These autonomous driving algorithms are able to run different perception routines in parallel and switch between them as desired for changing environments or scenarios. For example, you might have one algorithm that's controlling the car while you're in clear daylight conditions. A little helper algorithm detects that you’ve driven into a fog bank (as I do every morning when crossing the Golden Gate Bridge!), and the car may switch to a different control algorithm optimized for fog navigation.\n\nCombining this multitasking with interest in “sensor fusion” definitely shows that there's a need for multimodal sensor data and more sophisticated algorithms to operate in transient situations.\n\nSama Commitment to Best Annotation Technology\n\nWe continuously look for the data types that are of interest to our customers. There is so much data needed for 2D and Video annotation technology that we can support. We also heard a lot of interest around Lidar 3D technology. Look for future reports from the field on this blog.\n\nAs head of product for Sama, I can tell you we are always looking to how we can incorporate emerging data sources into our platform to provide rapid, high quality annotation. If there’s one thing the Sama journey has shown, it’s that the benefit of a skilled workforce is their flexibility and adaptability to varied tasks with uncompromising quality.\n\nI think that puts us in a very good position as new sensor data becomes more important for more sophisticated algorithms. I feel very confident we're going to be able to build those workflows into our platform and train up our workforce to deliver.\n\nIf you’d like to learn any more about our capabilities and approaches to supporting your data needs in this area, we would urge you to download our solution brief about our Annotated Data Sets. We look forward to joining you in this adventure unfolding on the the roads of the world.","seo_title":"Takeaways from AutoAI Conference 2018","slug":{"_type":"slug","current":"takeaways-from-autoai-conference"},"tags":[{"_key":"2fZXZfik","label":"AI","value":"AI"},{"_key":"bS0jYOSG","label":"Events","value":"Events"},{"_key":"hKUrqBWv","label":"Autonomous Transportation","value":"Autonomous Transportation"}],"title":"Takeaways from AutoAI Conference 2018"},{"_createdAt":"2018-01-24T20:52:26Z","author":{"_id":"cc6094b6-cdfd-4509-b972-c1539888dcdf","avatar":{"_type":"image","asset":{"_ref":"image-f1fd7fbcc4633299cdbedddba22cb44e24f17317-518x518-svg","_type":"reference"}},"bio":"Currently a Senior Account Executive at Sama, Marcelo works with enterprise clients in the high-tech space to help develop their AI strategies and get the most out of their training data. He has managed relationships with Fortune 500-level clients at numerous companies in various technology spaces - hardware, SaaS and services. Marcelo is driven by the goal of reducing global poverty through Sama’s social impact mission.","name":"Marcelo Benedetti","slug":{"_type":"slug","current":"marcelo-benedetti"}},"estimatedReadingTime":6,"featured_image":{"_type":"image","asset":{"_ref":"image-e6e7bab73442aa5bcd307816c3419505a7895093-1500x1500-jpg","_type":"reference"}},"meta_description":"With increased buzz around synthetic data, it is important to understand the advantages and limitations of this solution, and the overall affect on the application.","openGraphImage":null,"plaintextBody":"With increased buzz around synthetic data, it is important to understand the advantages and limitations of this solution, and the overall affect on the application.\n\nIs it really possible to use generated synthetic data as training data? Let's first take a step back and define synthetic data. In short, synthetic data is system-generated data that mimics real data in terms of essential parameters set by the user. Synthetic data is any production data not obtained by direct measurement, and is considered anonymized. Conceptually, synthetic data may seem like a compilation of “made up” data, but there are specific algorithms designed to create realistic data. Synthetic data can assist in teaching a system how to react to certain situations or criteria.\n\nHow is synthetic data generated? Synthetic data can be created by stripping any personal information (names, license plates, etc.) from a real dataset so it is completely anonymized. Another method is to create a generative model from the original dataset that produces synthetic data that closely resembles the authentic data. A generative model is a model that can learn from large, real datasets to ensure the data it produces accurately resembles real-world data. There are three types of generative models: Generative Adversarial Networks or GAN's, Variational Auto Encoders or VAE's, and Autoregressive models. GAN models utilize a generative and discriminative network in a zero-sum game framework. VAE's attempt to recreate output from input using encoding and decoding methods. Autoregressive models train the network to create individual pixels based on previous pixels above and to the left of them.\n\nNow that we've discussed what synthetic data is and how it's created, we can look into why synthetic data sets are used and how it stacks up against real data for system training.\n\nSynthetic data has a wide variety of applications such as image processing, IoT, AI, machine learning, defense, and natural language processing. Synthetic data has also been used for machine learning applications. The main reasons why synthetic data is used instead of real data are cost, privacy, and testing. Producing synthetic data through a generation model is significantly more cost-effective and efficient than collecting real-world data. This especially applies to the autonomous vehicle space where real-world data can be both time-consuming and costly to collect. With synthetic data, it becomes cheaper, and fast to produce new data once the generative model is set up.\n\nAnother major advantage of synthetic data is anonymity as all personal information has been removed and the data cannot be traced back to the original owner, avoiding any possible copyright infringements. This is critical when attempting to recreate realistic user behaviors as synthetic data protects the authentic data privacy and confidentiality. For example, the U.S. Census Bureau utilized synthetic data without personal information that mirrored real data collected via household surveys for income and program participation.\n\nSynthetic data can be used to test existing system performance as well as train new systems on scenarios that are not represented in the authentic data. Rather than utilizing costly real-world data to test if the system is providing the desired output, you can plug in synthetic data and analyze the results. For an instance where authentic data does not represent every possible situation, synthetic data can play a vital part in system training. This is notably relevant in the defense space where it is necessary to ensure the system can handle a variety of intrusion and attack types. Using artificial data, you can train a system on a multitude of scenarios not covered in the authentic data, thus improving its defensive capabilities.\n\nSynthetic data does not come without its limitations. While synthetic data can mimic many properties of authentic data, it does not copy the original content exactly. Models look for common trends in the original data when creating synthetic data and in turn, may not cover the corner cases that the authentic data did. In some instances, this may not be a critical issue. However, in most system training scenarios, this will severely limit its capabilities and negatively impact the output accuracy.\n\nAlso, the quality of synthetic data is highly dependent on the quality of the model that created it. These generative models can be excellent at recognizing statistical regularities in datasets but can also be susceptible to statistical noise, such as adversarial perturbations. Adversarial perturbations can cause the model or network to completely misclassify data and in turn, create highly inaccurate outputs. The way to resolve this issue is to leverage real-world human annotated data, input into the model, and test the outputs for accuracy.\n\nAnother challenge presented by using synthetic data is the need for a verification server, an intermediary computer that performs identical analysis on the initial data. This system is put in place to test and compare the authentic and synthetic data outputs. This is to ensure the system has been properly trained and is not generating the desired outputs due to any assumptions that were built into the synthetic data.\n\nWhile synthetic data can be easy to create, cost-effective, and highly useful in some circumstances, there is still a heavy reliance on human annotated and real-world data. The only way to guarantee a model is generating accurate, realistic outputs is to test its performance on well-understood, human annotated validation data. While generating realistic synthetic data has become easier over time, real-world human annotated data remains a necessary part of machine learning training data.","seo_title":"The Advantages and Limitations of Synthetic Data","slug":{"_type":"slug","current":"the-advantages-and-limitations-of-synthetic-data"},"tags":[{"_key":"VJ46fjrk","label":"Training Data","value":"Training Data"}],"title":"The Advantages and Limitations of Synthetic Data"},{"_createdAt":"2018-01-18T18:08:16Z","author":{"_id":"cc6094b6-cdfd-4509-b972-c1539888dcdf","avatar":{"_type":"image","asset":{"_ref":"image-f1fd7fbcc4633299cdbedddba22cb44e24f17317-518x518-svg","_type":"reference"}},"bio":"Currently a Senior Account Executive at Sama, Marcelo works with enterprise clients in the high-tech space to help develop their AI strategies and get the most out of their training data. He has managed relationships with Fortune 500-level clients at numerous companies in various technology spaces - hardware, SaaS and services. Marcelo is driven by the goal of reducing global poverty through Sama’s social impact mission.","name":"Marcelo Benedetti","slug":{"_type":"slug","current":"marcelo-benedetti"}},"estimatedReadingTime":6,"featured_image":{"_type":"image","asset":{"_ref":"image-52e0da07c653ab82df020267a98b83d1893bf5f3-1500x1500-jpg","_type":"reference"}},"meta_description":"Synthetic data is system-generated data that mimics real data in terms of essential parameters set by the user.","openGraphImage":null,"plaintextBody":"As artificial intelligence remains at the forefront of current and future technological advancements, the breadth of applications for machine learning and computer vision algorithms continues to grow as well. With the digital world producing data at an exponential rate and \"big data\" being a hot topic for enterprise corporations, it is vital for these businesses to find competitive advantages in this space.\n\n\n\nAs the applications of these algorithms continue to expand, so does the need for training data (https://www.samasource.org/training-data) (dataset examples used to teach and train algorithms). In many cases, training data can be both costly and challenging to obtain. Hence, utilizing model-generated synthetic data just might be this next major competitive advantage in the artificial intelligence space.\n\nWhat Is Synthetic Data?\n\nWhat exactly is synthetic data? Synthetic data is system-generated data that mimics real data in terms of essential parameters set by the user. Synthetic data is any production data not obtained by direct measurement and thus is considered anonymized. Conceptually, synthetic data may seem like a compilation of “made up” data, but there are specific algorithms that are designed to create realistic data. This synthetic data will then assist in teaching a system how to react to certain situations or criteria, replacing real-world captured training data.\n\nHow To Use Synthetic Data\n\nSo why use synthetic data? Synthetic data sets have a wide variety of applications such as image processing, IoT, AI, machine learning, defense, and natural language processing. The main reasons why synthetic data is used instead of real data are cost, privacy, and testing. Producing synthetic data through a generation model is vastly more cost-effective and efficient than collecting real-world data. This especially applies to the autonomous vehicle space where real-world data can be both time-consuming and costly to collect.\n\nWith synthetic data, it becomes cheap and fast to produce new data once the generative model is set up. The other benefit of synthetic data is anonymity. With personal information being removed, the data cannot be traced back to the original owner so copyright and privacy infringements can be avoided. This is critical in synthetic data machine learning applications where realistic user behaviors are being simulated and private information must be protected.\n\nSynthetic data can also be used to examine existing system performances as well as train new systems on scenarios that are not represented in the authentic data. Rather than utilizing costly real-world data to test if the system is providing the desired output, you can plug in synthetic data and analyze the results. For an instance where authentic data does not represent every possible situation, synthetic data can play a vital part in system training.\n\nCreating Synthetic Data\n\nSo how is synthetic data created? Synthetic data is typically created via a generative model from the original dataset that produces synthetic copies which closely resemble the authentic data. A generative model is a workload model that can learn from real datasets to ensure that the output produced accurately resembles the original, authentic data.\n\nThere are three types of generative models: Generative Adversarial Networks (or GANs), Variational Autoencoders (VAEs), and Autoregressive models. GAN models utilize a generative and discriminative network in a zero-sum game framework. VAEs attempt to recreate output from input using encoding and decoding methods. Autoregressive models train the network to create individual pixels based on previous pixels above and to the left of them.\n\nWhat is the competitive advantage with synthetic data? Besides the aforementioned cost-effectiveness and anonymity of synthetic data, the major competitive advantage lies with its various testing applications. With organizations having more data than ever at their disposal, the key challenge becomes how to extrapolate impactful insights from these large datasets and effectively translate the learnings into action. This is where big data tools and advanced analytics applications come into play.\n\nOrganizations use these tools and application to generate value from their massive datasets. Synthetic data can play a huge part in the development and improvement of these business-critical applications. For example, it can be used for visualization purposes and to test the robustness and scalability of new algorithms. This is vital for any organization working with big data applications.\n\nThough synthetic data's effectiveness in research is limited due as it only replicates certain properties of the original data, it is a highly useful tool to safely share data for testing the performance of a new software or scalability of an algorithm. With the widespread use of big data tools and analytics apps, an investment in synthetic data generation is critical and companies will have to factor this into their future strategic planning.","seo_title":"What is Synthetic Data?","slug":{"_type":"slug","current":"what-is-synthetic-data"},"tags":[{"_key":"ON1ZQ2UR","label":"Training Data","value":"Training Data"}],"title":"What is Synthetic Data?"},{"_createdAt":"2017-12-18T20:37:13Z","author":{"_id":"ba12f1d5-b083-4eb7-929d-7e1639ef64c5","avatar":{"_type":"image","asset":{"_ref":"image-f1fd7fbcc4633299cdbedddba22cb44e24f17317-518x518-svg","_type":"reference"}},"bio":"Daniele is a Sales Operations and Engineering Senior Manager for Sama working with teams in EMEA and APAC.","name":"Daniele Packard","slug":{"_type":"slug","current":"daniele-packard"}},"estimatedReadingTime":4,"featured_image":{"_type":"image","asset":{"_ref":"image-34e0689c14c9330dffeed72f7c6cbef59e2f9ad5-1500x1504-jpg","_type":"reference"}},"meta_description":"The best way for a computer to gain knowledge is to start by showing it exactly what it is you want it to do. For this, we use training data.","openGraphImage":null,"plaintextBody":"Historically, to get a computer to do something, you had to explicitly program it to execute a series of steps to accomplish your desired task. This would include everything from the most simple of arithmetic skills to manipulating objects in a complex digital world like what you see in video games.\n\nMachine learning is a fast-developing field of computer science that allows computers to complete exercises they have not been explicitly programmed to do such as, react to new or unforeseen situations and to then learn from this new input dataset. Not only does this create an entirely novel and potentially more efficient process to have computers engage with the world, but it has also opened the door to applications once thought to be too complex to be achieved with traditional computer science (hello, autonomous vehicles!).\n\nHow does it work? Training data.\n\nThe best way for a computer to gain knowledge is to start by showing it exactly what it is you want it to do. To do this, we use training data, the input the machine learning algorithm references and learns from. The computer will use this to look for patterns, extrapolate connections, create rules, and ultimately learn how to accomplish what it is you are trying to achieve.\n\nThe level of complexity and nuance needed in your training dataset depends on the desired goal. For example, a model that gives a binary yes/no to whether a dog is in an image will need input training images that are categorized as having a dog or not. Whereas, a model that needs to tell you not only whether a dog is in an image, but also show you where the animal is, will need training images where the location of the dog is specified.\n\nDifferent industries and applications have very different training data needs. A financial company trying to automate fraud detection will need appropriately categorized examples of where fraud did and did not happen. Meanwhile, a biomedical company looking to automate medical image analysis will need a doctor to label images (although it has been shown that for certain applications, pigeons and machine learning can help medical image analysis!).\n\nAs a user of digital products you are creating training data all the time, often without even realizing it. When you tell your fitness app your preferences you are contributing to a machine learning model. When, in your inbox, you flag certain messages as spam, you are creating training data for your spam filter algorithm.\n\nThe need for high quality data training sets is ubiquitously recognized and the counterproductive effects of bad data introduced to your model can be succinctly summarized with “garbage in, garbage out”. Essentially meaning that less than stellar training data, which inaccurately represents what you want your model to achieve, will yield an equally poorly performing model.\n\nYour training data also needs to be diverse enough to meet all the potential scenarios your model will or may encounter in order to avoid creating biasses. For example, a model created to process the age of a person using training data composed of only images of adults and their respective ages will be clueless when presented with the image of a child. Biases can cause a benign oversight or cause errors that are dangerous or even socially controversial.\n\nUltimately, the need for training data is growing in parallel with the applications of machine learning. While some are researching ways to minimize the need for training data or generate it digitally, there is growing evidence and research validating how important high volumes of training data can be. And though the degree to which training data will be essential to the future of AI is up for debate, it will surely continue to play an important role in machine learning.\n\n*There are various types of machine learning. For the purpose of this blog article, we are discussing supervised machine learning.","seo_title":"What is Training Data?","slug":{"_type":"slug","current":"what-is-training-data"},"tags":[{"_key":"4Phssued","label":"Training Data","value":"Training Data"}],"title":"What is Training Data?"},{"_createdAt":"2017-05-24T21:33:56Z","author":{"_id":"6dd25391-26af-42ff-a2a2-f0778ce1b002","avatar":{"_type":"image","asset":{"_ref":"image-4e1c47984bb16cb0370855b6272663d6744216d3-450x450-jpg","_type":"reference"}},"bio":"Currently a Senior Product Manager at Sama, Audrey guides cross-functional teams to create thoughtful product solutions. She has guided teams of designers and engineers at HUGE Inc. and NBCUniversal, and monitored user analytics at the Wall Street Journal. With a BA in history from Harvard, an MA in anthropology from Columbia and an MBA from UNC Chapel Hill KFBS, Audrey is passionate a using technology and data analytics facilitate social impact and environmental solutions through technology.","name":"Audrey Boguchwal","slug":{"_type":"slug","current":"audrey-boguchwal"}},"estimatedReadingTime":4,"featured_image":{"_type":"image","asset":{"_ref":"image-73eb0ced8bf91c8bd71f48061438e8d61ce0f8b6-597x398-png","_type":"reference"}},"meta_description":"Better Algorithms, Better Lives: Reducing Poverty Through Training Data","openGraphImage":null,"plaintextBody":"Last time, we explained how Sama our online software, to connect low-income people to digital work. Now we’ll spotlight one of our clients to show how our image annotation work can be used in machine learning to train image recognition algorithms.\n\nMarkable came to Sama for help training their state-of-the-art image recognition technology that identifies fashion products in photos and videos. With Markable’s tech in place, viewers can click on clothing they see while watching TV shows and Markable will generate matching product results. Check out Markable’s demo to see how it works.\n\nTo create image recognition technology, engineers develop a machine learning program that learns to identify objects of interest from training data. For Markable, the training data is photographs of fashion products, clearly labeled so when the algorithm encounters an unknown product, it can infer what it is based on trained examples. This process is called “supervised learning” because the algorithm is given examples structured specifically for training. A high-quality training data set for a vision algorithm consists of tens of thousands of images in which objects are outlined and labeled according to the desired classification. The accuracy of the training data is important as an algorithm trained with inconsistently labeled data won’t learn patterns and won’t be able to identify objects.\n\nHow do all those training data images get labeled accurately and quickly enough to get Markable’s tech to customers? That’s where Sama comes in. As part of our image annotation service, we have a trained, scalable workforce and the right tools to label thousands of images for training data, efficiently and accurately. We balance stringent quality requirements with on-time deliveries to ensure clients like Markable can stick to their project timelines.\n\nSama worked closely with Markable to understand the project’s annotation requirements and refine them for ambiguous images. Markable provided Sama with source images and then we trained a dedicated team of workers how to draw boxes around the objects of interest and label them. For Markable, Sama workers learned to identify every minor visible detail, such as heel-length of a shoe, to ensure exceptional data quality. Sama’s on-site quality analysts inspect a sample of annotated images daily to ensure quality, asking workers to redo any images that don’t meet client standards.\n\nHere’s what image annotation work looks like in the Hub. Agents draw bounding boxes (or tight outlines) around each object and then use a menu to add labels. In the for-presentation-only image below, a worker has labeled the shirt, pants and is labeling boots with a multi-level menu. Sama can set up label data entry to meet a variety of project needs: text entry, selection from a dropdown, or search for a label from a list, to name a few.\n\nWith Sama’s workers, Markable has been able to improve their algorithm and better serve their customers. They wrote to us: “With the help of Sama's annotation, we were able to surpass previous state-of-the-art accuracy on the largest open-source fashion e-commerce dataset. We had a great experience in working with Sama and we will continue working with them in future.”\n\nSama’s client relationships are a win-win: clients provide our workers with life-changing job opportunities and workers help clients achieve their business goals, even on the trickiest projects.\n\nIn our third and final post, we’ll show you how Sama’s workers take on a web research project.","seo_title":"Better Algorithms, Better Lives: Reducing Poverty Through Training Data","slug":{"_type":"slug","current":"better-algorithms-better-lives-reducing-poverty-through-training-data"},"tags":[{"_key":"Bx4Do9xK","label":"Ethical AI","value":"Ethical AI"},{"_key":"Rx6gXgmn","label":"Impact","value":"Impact"},{"_key":"iujrQUVj","label":"Use Cases","value":"Use Cases"},{"_key":"v5mZqxPX","label":"Data Annotation","value":"Data Annotation"}],"title":"Better Algorithms, Better Lives: Reducing Poverty Through Training Data"},{"_createdAt":"2017-05-15T23:51:58Z","author":{"_id":"6dd25391-26af-42ff-a2a2-f0778ce1b002","avatar":{"_type":"image","asset":{"_ref":"image-4e1c47984bb16cb0370855b6272663d6744216d3-450x450-jpg","_type":"reference"}},"bio":"Currently a Senior Product Manager at Sama, Audrey guides cross-functional teams to create thoughtful product solutions. She has guided teams of designers and engineers at HUGE Inc. and NBCUniversal, and monitored user analytics at the Wall Street Journal. With a BA in history from Harvard, an MA in anthropology from Columbia and an MBA from UNC Chapel Hill KFBS, Audrey is passionate a using technology and data analytics facilitate social impact and environmental solutions through technology.","name":"Audrey Boguchwal","slug":{"_type":"slug","current":"audrey-boguchwal"}},"estimatedReadingTime":3,"featured_image":{"_type":"image","asset":{"_ref":"image-bde2f64018e50b7615f188245cc00fe516e84936-693x462-jpg","_type":"reference"}},"meta_description":"How exactly does Sama move people out of poverty?","openGraphImage":null,"plaintextBody":"If you’re reading this, you probably already know that Sama’s mission is to connect low-income people to digital work. And, if you’re like the majority of people reading this, the details of how exactly we do that are probably a little fuzzy. This post will dispel the mystery!\n\nSama developed an in-house SaaS (software as a service) platform that enables us to send work from our clients to workers at our delivery centers in Kenya, Uganda and India. Sama is the linchpin in our model, as it allows our project managers, agents, and clients to access the work being done, while also allowing us to closely monitor quality and provide feedback for continuous improvements.\n\nAs a SaaS platform, the Hub can be accessed securely from anywhere - our San Francisco headquarters to our Sama Center in Nairobi, Kenya. To support the Hub, we have a full time, dedicated engineering team that help the Hub evolve to meet changing client needs and continuously improve worker efficiency.\n\nDuring our sales cycle, Sama meets with clients to better understand their data services needs. When a contract is in place, a project manager will partner with the client to design a Hub project workflow that satisfies project requirements, a quality strategy to check work, a training plan and a delivery schedule.\n\nOn Hub, project managers then upload data from clients that our workers will clean, use as the basis for research, annotate or supplement. Next, they custom-design the task layout and workflow that workers will see when they log in to complete tasks. From question types and dependency logic to an easy-to-read layout, the Hub can accommodate a range of project needs.\n\nOnce the project is set up, workers complete specialized training to learn the new workflow. Throughout training, workers are coached by on-site team leaders and evaluated by quality analysts until their work is at or above the client’s desired SLA for quality. The SLA is the service level agreement, that is, the level of quality agreed upon in the client’s contract with Sama. When training is complete, workers begin production tasks on the Hub. During production, coaching continues to maintain quality. QAs use the Hub to check task quality and send back tasks that are below quality standards for rework. Once work is completed, it can be downloaded from the Hub or delivered directly to the client via API.\n\nIn future posts, we’ll go in-depth in two case studies to see how Sama completes work for different verticals: web research and image annotation for machine learning and computer vision applications, and how the Hub is an integral part of our process.\n\nStay tuned!","seo_title":"How Samasource Moves People Out of Poverty with Digital Work","slug":{"_type":"slug","current":"how-exactly-does-samasource-move-people-out-of-poverty"},"tags":[{"_key":"Z2i2Xm9p","label":"Ethical AI","value":"Ethical AI"},{"_key":"NE08Dk4r","label":"Impact","value":"Impact"},{"_key":"mwOWz5qA","label":"Data Annotation","value":"Data Annotation"}],"title":"How Samasource Moves People Out of Poverty with Digital Work"},{"_createdAt":"2017-03-21T20:31:18Z","author":{"_id":"ba12f1d5-b083-4eb7-929d-7e1639ef64c5","avatar":{"_type":"image","asset":{"_ref":"image-f1fd7fbcc4633299cdbedddba22cb44e24f17317-518x518-svg","_type":"reference"}},"bio":"Daniele is a Sales Operations and Engineering Senior Manager for Sama working with teams in EMEA and APAC.","name":"Daniele Packard","slug":{"_type":"slug","current":"daniele-packard"}},"estimatedReadingTime":4,"featured_image":{"_type":"image","asset":{"_ref":"image-e7af72154b4cdac45f8526a3934f9ac612e23d37-513x321-jpg","_type":"reference"}},"meta_description":"3 Computer Vision Applications You Haven't Heard Of... Yet","openGraphImage":null,"plaintextBody":"From the looming reality of fully autonomous vehicles to a farmer in Japan using TensorFlow to sort cucumbers, computer vision machine learning is becoming an increasingly accessible and ubiquitous phenomenon. As a service provider in this space, Sama has a unique view into the breadth of computer vision applications. While certain applications of machine learning are prevalent in mainstream media (e.g. autonomous vehicles), there are equally interesting and lesser-known applications that are similarly revolutionary:\n\n\n\n1. Environmental\n\nSatellites orbiting the earth have been collecting comprehensive image data sets of the globe for decades. With recent advancement in computer vision, these images can be efficiently and automatically combed for valuable information. By applying deep learning models to sets of satellite imagery, companies like Orbital Insight can extract data on global surface water levels, informing communities, planners, and policymakers in making critical decisions about water resources.\n\n\n\nFigure 1 San Luis Reservoir. Left: Raw image. Middle: Landsat 8 water mask. Right: Orbital insight water mask. Orbital Insights, 2015\n\n\n2. Biomedical\n\nImage analysis in the medical field is a time-consuming process; using computer vision machine learning to automate portions of this analysis could help make the process more efficient and less costly for the general public. A bottleneck to progress in this field has been the lack of training data, which requires specialists to create (though there’s evidence that specialists can be substituted by pigeons!) and the data is typically confidential. The availability of large datasets with diagnostic labels, such as the Alzheimer’s Disease Neuroimaging Initiative (ADNI), has paved the way for new applications, using computer vision machine learning to provide early diagnosis of Alzheimer’s disease.\n\n\n3.Agricultural\n\nThe advancement of drone and computer vision technology has both lowered the cost of gathering huge sets of aerial imagery, and also increased our ability to intelligently extract information from that imagery. These technologies together can help farmers identify crop diseases or predict crop yields, automating a process where the alternative was manual inspection. In collaboration with Paul Allen towards his Great Elephant Census project, Sama facilitated the creation of a computer vision machine learning model used to track and count elephants as a part of anti-poaching efforts.\n\n\n\nElephant Sanctuary in Kenya, Sama, 2016\n\nUltimately applications of computer vision machine learning are limited only by human imagination. We look forward to seeing what applications this evolving technology will create, and are thrilled to count ourselves as part of this thriving community.\n\nIf you want to learn more about the future of practical computer vision, please join us at the Embedded Vision Summit, being held in Santa Clara from May 1-3. It’s the preeminent event for anyone adding computer vision capabilities to their products. We’ll be participating in the Vision Technology Showcase and invite you to stop by our booth.\n\nThe event is designed to inspire you to use vision technology in new ways and to empower you with the practical know-how you need to integrate vision capabilities into your products. Over three days and four tracks, you’ll meet innovators, luminaries, and colleagues in this fast-growing field.\n\nSama Sales team at Embedded Vision Summit, 2016","seo_title":"3 Computer Vision Applications You Haven't Heard Of...Yet","slug":{"_type":"slug","current":"3-computer-vision-applications-you-havent-heard-of-yet"},"tags":[{"_key":"zGqHljfi","label":"AI","value":"AI"},{"_key":"QcKBgjbl","label":"Events","value":"Events"},{"_key":"mpmDlNYK","label":"Computer Vision","value":"Computer Vision"}],"title":"3 Computer Vision Applications You Haven't Heard Of...Yet"},{"_createdAt":"2016-09-21T18:56:59Z","author":{"_id":"88af0504-c0e4-4479-b961-0d74424c8aff","avatar":{"_type":"image","asset":{"_ref":"image-f1fd7fbcc4633299cdbedddba22cb44e24f17317-518x518-svg","_type":"reference"}},"bio":null,"name":"Andrew Ho","slug":{"_type":"slug","current":"andrew-ho"}},"estimatedReadingTime":5,"featured_image":{"_type":"image","asset":{"_ref":"image-c5d3596e3d13a8462000fb2d564532cb99198bff-290x210-jpg","_type":"reference"}},"meta_description":"Winning Customers with Algorithms: How Teams in Nairobi Help Shape Your Shopping Experience","openGraphImage":null,"plaintextBody":"As a kid, I marveled at the strategy involved in retail layouts. From inviting department store entryways stocked with deals, to the tactical placement of milk \u0026 eggs in the backmost aisle of grocery stores, there’s always a reason why an item is placed where it is. It was a never-ending personal challenge to try and decode these invisible maps.\n\nTermed “planogramming,” visual merchandising is key in retail stores. The best stores find a balance between exciting customers without overwhelming them with deals shouting from every corner.\n\nWalmart is the reigning king of retail. With an average of 120,000 products sold in each store, visual merchandising is critical in maximizing both sales and profits. Yet, as with all traditionally brick-and-mortar markets, technology is re-shaping the consumer experience. Forget 120,000 products — eCommerce sites now place tens (and sometimes hundreds) of millions of products at consumer fingertips.\n\nA new visual experience is emerging, one that starts and ends at a computer screen. Think of Walmart.com as the equivalent 150 Walmart retail stores combined into one super-megastore. Nearly 1 square mile of product (think 2/3 the size of Golden Gate Park!). You land on the homepage and are surrounded by mile-long aisles, each with thousands of virtual shelves — where will you wander today?\n\nWith these massive assortments, the key to success in emerging eCommerce marketplaces is ensuring that the consumer finds what they need. Whoever brings the most searchable shelf to the consumer’s eye wins the virtual planogramming competition.\n\nKeys to navigating this marketplace can be basic — when you search for a white t-shirt, you don’t want results to include a red dress. They can also be more complex and predictive — when you search for firewood, schematic search may also recommend you also buy lighter fluid. As a result, each product needs a significant chunk of metadata.\n\nTake this stroller as an example. Beyond knowing that it’s a baby stroller, what color is it? Fabric material? How much weight can it bear? Is is appropriate for a newborn? Does is require assembly? Is it a multifunction stroller?\n\nWith a rapidly growing product assortment, data scientists at WalmartLabs partnered with Sama to create an easy, user-friendly way for consumers to navigate millions of unique items.\n\nThese data scientists create predictive algorithms that automatically assign attributes to products, so that computers can predict with confidence that this is a pink and black stroller made with polyester fabric that can support up to 50 pounds.\n\nAt its core, this data is inherently human. Behind each prediction, each “1” and “0”, is a judgment that was originally done by humans. Many of Walmart.com’s predictive models “learned” from training data supplied by our Sama team in Nairobi, Kenya.\n\n\n\nIn one instance, Walmart created an algorithm to automatically predict the “gender” associated with products listed on their website. The goal was to sort products that had gender specific words in the title or in the description, such as “Nike Air Women’s tennis shoes” so that these items could be identified as products for women. Walmart partnered with Sama to create data sets for creating the ground truth, and training the algorithm. Sama manually QAed 115,522 products — this included checking to see if the algorithm had correctly predicted the gender, and if not, assigning the correct gender.\n\nAs is typically the case prior to the creation of a training data set that provides sufficient ground truth, at the start, the algorithm’s accuracy was around 59%. After using Sama data sets to train the algorithm, WalmartLabs engineers were able to increase the algorithm accuracy to 93%.\n\nWalmart’s partnership with Sama has resulted in visible enhancements in product descriptions, therefore improving the shopping and purchasing experience of its extensive portfolio of products. Moreover, this project provides work for 12 Sama agents — talented women and youth in Kenya for whom access to dignified data work represents a path out of poverty.\n\nNext time you shop online and over-buy because you keep finding exactly what you want, blame our team in Nairobi! Or blame the internet for creating virtual shopping carts that never get full.","seo_title":"Winning Customers with Algorithms:Ãƒâ€šÃ‚Â How Teams in Nairobi Help Shape Your Shopping Experience","slug":{"_type":"slug","current":"winning-customers-with-algorithms"},"tags":[{"_key":"a60hER5C","label":"Training Data","value":"Training Data"},{"_key":"GJU9hmyY","label":"Data Annotation","value":"Data Annotation"},{"_key":"mMxaxwhQ","label":"Retail","value":"Retail"}],"title":"Winning Customers with Algorithms:Ãƒâ€šÃ‚Â How Teams in Nairobi Help Shape Your Shopping Experience"},{"_createdAt":"2016-01-26T21:48:00Z","author":{"_id":"037257b7-27fe-47e4-8b0b-7fc05f55fa6d","avatar":{"_type":"image","asset":{"_ref":"image-d3fb21c2ca35139da39b453c1981b5d04545ab51-800x800-webp","_type":"reference"}},"bio":null,"name":"Leila Janah","slug":{"_type":"slug","current":"leila-janah"}},"estimatedReadingTime":6,"featured_image":{"_type":"image","asset":{"_ref":"image-37701649c15c883643f12a6b0a10ebb11dfdd4c3-5472x3648-jpg","_type":"reference"}},"meta_description":"Davos: It's Not Enough to do Less Bad (5 Tech and Impact Trends)","openGraphImage":null,"plaintextBody":"I spent last week at the World Economic Forum in Davos raising funds for Sama and building Laxmi’s global community of retailers and partners. Along the way I came across some especially powerful ideas, and reinforced one of our own (the impact bond).\n\nHere are five key tech and social impact trends from this year’s Forum:\n\n1. It’s not enough to do less bad. We say this a lot in the office — new businesses can’t define themselves by avoiding the bad (not polluting, not hiring slaves, not using toxic ingredients). The best emerging companies, and the savviest traditional/old brands, understand that creating positive social and environmental impact in the world is important to Millennial consumers. Corporate social responsibility is dead — it feels too weak. We want to see corporate activism on issues like climate change, human rights, and equal pay. It’s already happening. Richard Branson’s B Team hosted a lunch on this and entertained several ideas put forth by social entrepreneurs (including my friends Jeremy Heimans and Nancy Lublin). Chanel’s chief legal counsel said during a brilliant sustainable fashion session organized by Zara Ingilizian that in an era of rising inequality, “the only way luxury can exist is through total integrity.”\n\n\n\n2. The Fourth Industrial Revolution is the way leaders will talk about tech. Professor Klaus Schwab, WEF’s Founder, wrote a new book and named the conference after this theme. The Fourth Industrial revolution refers to digital integration across every industry, from agriculture (think smart crop monitoring via drones) to mindfulness (Paul Davidson’s Mindfulness-based Stress Reduction program is now online) and non-profit work (Nancy Lublin’s Crisis Text Line is a great example). Schwab’s idea is that technology is a revolution as profound as the Enlightenment — what came before will not predict what comes afterwards. There are pros and cons to this shift — we will see rising global prosperity and access to basic goods and services, but we’ll also see economic shocks as more jobs are automated.\n\n\n\n3. Impact bonds and pay-for-outcomes models will dominate global social sector funding. It’s crazy that we spend billions on global development programs that may or may not lead to outcomes — instead, agencies fund programs and activities. We need a new way to fund the most important social sector work that gives organizations the freedom to innovate and spend as they see fit, so long as they deliver. The social impact bond has been tried in dozens of settings — to reduce recidivism, for example, in the UK and the US. How it works: non-profits with demonstrated outcomes commit to a certain target for a given sum — for example, 2,500 people moved out of poverty for $2.5M dollars over three years. Private investors put up the capital (a “security”) via an intermediary. If the non-profit hits the target, philanthropists pay back the investors with a return (usually 4–5%). Donors don’t pay unless the outcome is achieved, investors bear the risk, and non-profits have access to unrestricted capital. Sama is raising growth capital though an impact security — learn more here.\n\n\n\n4. “Stakeholder capitalism” is no longer niche; it’s mainstream. Salesforce CEO Marc Benioff described Schwab’s early idea that stakeholders matter more than simply shareholders as formative. Salesforce’s 1–1–1 model (1% donation of product, profit, and employee volunteer time) was directly inspired by this thinking, as is much of the benefit corporation movement. “Stakeholder capitalism” shifts the focus of business from the narrow lens of short-term shareholder profit to a broad definition of profit that includes workers, customers, and the environment — a very important factor in how Millennials work, shop, and spend time. I think in time we will expand this idea to include “giving work” rather than donating capital — addressing poverty, recidivism, and a host of other social problems by incentivizing companies to hire from marginalized populations.\n\n\n\n5. Safe harbor provisions would allow tech platforms to innovate around labor. What does a job mean today, when 40% of US workers are “contingent labor” (read: contract workers)? 50% of Uber drivers work fewer than 10 hours a week. And what about Airbnb? The founders of these companies aren’t evil — they’re probably much more socially progressive than their Wall Street counterparts. So why not allow them to test things like contributions to retirement plans, better training, and the like without forcing these companies to risk massive lawsuits for being considered employers of record? This issue came up in discussions with US Senator Mark Warner and the Markle Initiative’s Zoe Baird, who are keen to propose legislation along these lines.\n\n\nMy personal favorite moment: running into Dr. Muhammad Yunus three times — twice in the cloakroom — and talking to him about #GiveWork, the core idea behind Sama. Yunus is a legend in the Sama office for his decades-long struggle to build microfinance into a movement (and the subject of my favorite Halloween costume). I had to pinch myself.","seo_title":"Davos: It's Not Enough to Do Less Bad (5 Tech and Impact Trends)","slug":{"_type":"slug","current":"davos-its-not-enough-to-do-less-bad-5-tech-and-impact-trends"},"tags":[{"_key":"XcKmjphM","label":"Ethical AI","value":"Ethical AI"},{"_key":"1tC6Pi0u","label":"Impact","value":"Impact"},{"_key":"X3YfCf7h","label":"Leila Janah","value":"Leila Janah"}],"title":"Davos: It's Not Enough to Do Less Bad (5 Tech and Impact Trends)"}],"pageConfig":{"title":"Sama Blog | Training Data, AI and Impact Sourcing Insights","description":"From machine learning to training data strategy, the Sama blog covers research, news and other AI trends from thought leaders across the globe."}},"__N_SSG":true},"page":"/blog","query":{},"buildId":"202899f50319881aba98bacba98f348defc1a075","isFallback":false,"dynamicIds":[4941,425,3551],"gsp":true,"appGip":true,"scriptLoader":[]}</script></body></html>